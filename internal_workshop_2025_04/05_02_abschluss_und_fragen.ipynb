{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop-Abschluss: Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem letzten Abschnitt des Workshops fassen wir die wichtigsten Konzepte zusammen, diskutieren Anwendungsmöglichkeiten und geben einen Ausblick auf weiterführende Ressourcen und zukünftige Entwicklungen im Bereich der LLM-Entwicklung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zusammenfassung der Workshop-Inhalte\n",
    "\n",
    "Wir haben in diesem Workshop fünf Hauptthemenbereiche behandelt:\n",
    "\n",
    "### Teil 1: Grundlagen\n",
    "- **Structured Output**: Methoden zur Strukturierung von LLM-Ausgaben (JSON, XML)\n",
    "- **Tool Calls**: Integration von externen Tools in LLM-Workflows mit dem `@tool` Decorator\n",
    "- **Modell-Typen**: Überblick über verschiedene LLM-Modelle und ihre spezifischen Eigenschaften\n",
    "\n",
    "### Teil 2: Architektur & State Management\n",
    "- **Chaining**: Verkettung von LLM-Aufrufen mit der LangChain Expression Language (LCEL)\n",
    "- **Architekturkonzepte**: Agentenbasierte Ansätze, ReAct-Pattern und fortgeschrittene RAG-Architekturen\n",
    "- **State Management**: Verwaltung von Zuständen in LLM-Anwendungen mit verschiedenen Memory-Typen\n",
    "\n",
    "### Teil 3: Datenverwaltung & Infrastruktur\n",
    "- **Vektordatenbanken**: Speicherung und Abfrage von Embeddings für effiziente semantische Suche\n",
    "- **Model Context Protocol (MCP)**: Strategien zur optimalen Nutzung des Kontextfensters\n",
    "- **Prompt-Management**: Organisation und Versionierung von Prompts mit LangChain Hub\n",
    "\n",
    "### Teil 4: Tools & Frameworks\n",
    "- **LangGraph**: Erstellung komplexer Workflows und Agenten mit gerichtetem Graph\n",
    "- **LangFuse**: Monitoring und Tracing von LLM-Anwendungen für verbesserte Observability\n",
    "\n",
    "### Teil 5: Qualitätssicherung & Evaluierung\n",
    "- **Evaluationsmethoden**: LLM-As-A-Judge, NLP-Metriken und RAG-spezifische Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Praktische Anwendungsmöglichkeiten\n",
    "\n",
    "Die gelernten Konzepte und Techniken können in verschiedenen Anwendungsbereichen eingesetzt werden:\n",
    "\n",
    "### Unternehmensanwendungen\n",
    "- **Dokumentenanalyse**: Automatisierte Extraktion von Informationen aus unstrukturierten Dokumenten\n",
    "- **Wissensmanagement**: RAG-Systeme für den Zugriff auf Unternehmenswissen\n",
    "- **Kundenservice**: Chatbots und virtuelle Assistenten mit Zugriff auf Produktdokumentation\n",
    "\n",
    "### Datenanalyse und -aufbereitung\n",
    "- **Datenbereinigung**: Automatisierte Korrektur und Standardisierung von Datensätzen\n",
    "- **Daten-zu-Text**: Generierung von natürlichsprachlichen Berichten aus strukturierten Daten\n",
    "- **Explorative Analyse**: Konversationelle Schnittstelle für Datenabfragen\n",
    "\n",
    "### Content-Erstellung\n",
    "- **Textgenerierung**: Automatisierte Erstellung von Artikeln, Zusammenfassungen oder Marketingmaterial\n",
    "- **Übersetzung**: Kontextsensitive mehrsprachige Übersetzungen\n",
    "- **Content-Optimierung**: Verbesserung bestehender Texte für spezifische Zielgruppen\n",
    "\n",
    "### Prozessautomatisierung\n",
    "- **Workflow-Automation**: KI-gesteuerte Entscheidungsprozesse\n",
    "- **Dokumentenverarbeitung**: Automatische Extraktion und Kategorisierung von Informationen\n",
    "- **Code-Generierung**: Assistenzsysteme für Softwareentwicklung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Beispiel einer vollständigen LLM-Anwendung\n",
    "\n",
    "Hier ist ein Beispiel für eine vollständige LLM-Anwendung, die verschiedene der im Workshop behandelten Konzepte kombiniert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "import operator\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "load_dotenv()\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Embeddings und Vektordatenbank initialisieren\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Beispielhafte Dokumente - in einer echten Anwendung würden Sie hier Ihre eigenen Dokumente laden\n",
    "documents = [\n",
    "    \"Vektordatenbanken speichern hochdimensionale Vektoren für schnelle Ähnlichkeitssuche.\",\n",
    "    \"RAG-Systeme kombinieren Retrieval und Generation für kontextbasierte Antworten.\",\n",
    "    \"LangChain ist ein Framework zur Erstellung von LLM-Anwendungen mit modularem Design.\",\n",
    "    \"LangGraph ermöglicht die Erstellung komplexer Workflows mit gerichteten Graphen.\"\n",
    "]\n",
    "texts = [{\"content\": doc, \"metadata\": {\"source\": f\"doc_{i}\"}} for i, doc in enumerate(documents)]\n",
    "vectorstore = Chroma.from_texts([doc for doc in documents], embeddings)\n",
    "\n",
    "# Tool für die Vektorsuche definieren\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Durchsucht die Wissensdatenbank nach relevanten Informationen zur Anfrage.\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    if results:\n",
    "        return \"\\n\".join([doc.page_content for doc in results])\n",
    "    return \"Keine relevanten Informationen gefunden.\"\n",
    "\n",
    "# State-Typ für den Graphen definieren\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    response: str\n",
    "    messages: Annotated[List[Dict[str, Any]], operator.add]\n",
    "\n",
    "# Knoten für den Graphen definieren\n",
    "def retrieve_context(state: AgentState) -> AgentState:\n",
    "    \"\"\"Ruft relevanten Kontext aus der Wissensdatenbank ab\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = search_knowledge_base(query)\n",
    "    return {\"context\": context}\n",
    "\n",
    "def generate_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generiert eine Antwort basierend auf der Anfrage und dem Kontext\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent, der präzise Antworten gibt. \"\n",
    "                   \"Verwende den folgenden Kontext, um die Frage zu beantworten. \"\n",
    "                   \"Falls der Kontext nicht ausreicht, gib ehrlich zu, dass du es nicht weißt.\\n\\n\"\n",
    "                   \"Kontext: {context}\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n",
    "# Graph erstellen\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"retrieve_context\", retrieve_context)\n",
    "workflow.add_node(\"generate_response\", generate_response)\n",
    "\n",
    "# Kanten definieren\n",
    "workflow.set_entry_point(\"retrieve_context\")\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Graph kompilieren\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Anwendung testen\n",
    "query = \"Was ist LangGraph und wofür wird es verwendet?\"\n",
    "\n",
    "result = app.invoke({\"query\": query, \"messages\": []})\n",
    "print(f\"Anfrage: {query}\\n\")\n",
    "print(f\"Abgerufener Kontext: {result['context']}\\n\")\n",
    "print(f\"Generierte Antwort: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weiterführende Ressourcen\n",
    "\n",
    "### Dokumentation und Tutorials\n",
    "- [LangChain Dokumentation](https://python.langchain.com/docs/) - Umfassende Dokumentation zu LangChain\n",
    "- [LangGraph Dokumentation](https://langchain-ai.github.io/langgraph/) - Anleitung zur Erstellung komplexer LLM-Workflows\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook) - Praxisnahe Beispiele für OpenAI-Modelle\n",
    "- [Llamaindex Dokumentation](https://docs.llamaindex.ai/) - Alternative zu LangChain mit Fokus auf Datenindexierung\n",
    "\n",
    "### Bücher und Kurse\n",
    "- \"Generative AI with LangChain\" von Ben Auffarth\n",
    "- \"Building LLM Powered Applications\" von Packt Publishing\n",
    "- DeepLearning.AI-Kurse zu LLMs und Prompt Engineering\n",
    "\n",
    "### Communities und Foren\n",
    "- [LangChain Discord](https://discord.gg/langchain)\n",
    "- [Hugging Face Forum](https://discuss.huggingface.co/)\n",
    "- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) auf Reddit\n",
    "\n",
    "### Blogs und Newsletter\n",
    "- [LangChain Blog](https://blog.langchain.dev/)\n",
    "- [The Batch](https://www.deeplearning.ai/the-batch/) - Andrew Ng's Newsletter zu KI\n",
    "- [Weights & Biases](https://wandb.ai/fully-connected) - Blog zu ML und KI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aktuelle Trends und zukünftige Entwicklungen\n",
    "\n",
    "### Aktuelle Trends\n",
    "\n",
    "1. **Lokal ausführbare Modelle**\n",
    "   - Zunehmender Einsatz von kleineren, aber effizienten Modellen wie Llama 3, Mistral und Gemma\n",
    "   - On-device Ausführung für datenschutzsensible Anwendungen\n",
    "\n",
    "2. **Multimodale Modelle**\n",
    "   - Integration von Text, Bild, Audio und Video in einem einzigen Modell\n",
    "   - Beispiele: GPT-4o, Claude 3 Opus, Gemini\n",
    "\n",
    "3. **Spezialisierte LLMs**\n",
    "   - Domain-spezifische Modelle für Bereiche wie Medizin, Recht, Finanzen\n",
    "   - Fine-Tuning auf spezifische Aufgaben für bessere Performance\n",
    "\n",
    "4. **Agentenbasierte Systeme**\n",
    "   - Autonome KI-Agenten, die komplexe Aufgaben selbstständig lösen\n",
    "   - Multi-Agenten-Systeme mit spezialisierten Rollen\n",
    "\n",
    "### Zukünftige Entwicklungen\n",
    "\n",
    "1. **Verbesserte Reasoning-Fähigkeiten**\n",
    "   - Fortschritte bei logischem Denken, Planung und Problemlösung\n",
    "   - Integration von symbolischen und neuronalen Ansätzen\n",
    "\n",
    "2. **Langzeit-Speicher und kontinuierliches Lernen**\n",
    "   - Modelle, die Informationen über längere Zeiträume speichern können\n",
    "   - Kontinuierliche Aktualisierung mit neuen Informationen\n",
    "\n",
    "3. **Verbesserte Evaluierungsmethoden**\n",
    "   - Standardisierte Benchmarks für spezifische Anwendungsbereiche\n",
    "   - Menschzentrierte Evaluierungsmethoden jenseits von BLEU und ROUGE\n",
    "\n",
    "4. **KI-Ökosysteme**\n",
    "   - Integrierte Plattformen für Entwicklung, Bereitstellung und Überwachung\n",
    "   - Nahtlose Integration mit bestehenden Geschäftsprozessen\n",
    "\n",
    "5. **Ethik und Governance**\n",
    "   - Verstärkte Fokussierung auf verantwortungsvolle KI-Entwicklung\n",
    "   - Frameworks für Transparenz, Fairness und Accountability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Offene Fragerunde\n",
    "\n",
    "Jetzt ist Zeit für Ihre Fragen! Hier sind einige Themen, die häufig in Fragerunden aufkommen:\n",
    "\n",
    "1. **Spezifische Anwendungsfälle**\n",
    "   - Wie könnte man die gelernten Konzepte auf konkrete Geschäftsprobleme anwenden?\n",
    "   - Welche Architektur eignet sich am besten für bestimmte Anwendungsfälle?\n",
    "\n",
    "2. **Technische Details**\n",
    "   - Vertiefende Fragen zu bestimmten Frameworks oder Techniken\n",
    "   - Problemlösung für spezifische Implementierungsherausforderungen\n",
    "\n",
    "3. **Ressourcenbedarf und Skalierung**\n",
    "   - Welche Hardware-Anforderungen haben verschiedene Modelle?\n",
    "   - Wie lassen sich LLM-Anwendungen kostengünstig skalieren?\n",
    "\n",
    "4. **Ethik und Datenschutz**\n",
    "   - Wie lassen sich Datenschutzbestimmungen wie die DSGVO einhalten?\n",
    "   - Welche ethischen Überlegungen sollten bei LLM-Anwendungen beachtet werden?\n",
    "\n",
    "5. **Weiterführende Lernressourcen**\n",
    "   - Empfehlungen für spezialisierte Themen\n",
    "   - Nächste Schritte zur Vertiefung des Gelernten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feedback zum Workshop\n",
    "\n",
    "Wir freuen uns über Ihr Feedback zum Workshop. Bitte teilen Sie uns mit:\n",
    "\n",
    "1. Was hat Ihnen am Workshop besonders gut gefallen?\n",
    "2. Welche Themen hätten ausführlicher behandelt werden können?\n",
    "3. Welche zusätzlichen Themen wären für Sie interessant gewesen?\n",
    "4. Wie bewerten Sie die Balance zwischen Theorie und praktischen Übungen?\n",
    "5. Haben Sie Vorschläge zur Verbesserung des Workshop-Formats?\n",
    "\n",
    "Ihre Rückmeldungen helfen uns, zukünftige Workshops noch besser zu gestalten!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Nächste Schritte\n",
    "\n",
    "### Nach dem Workshop\n",
    "\n",
    "1. **Workshop-Materialien**\n",
    "   - Alle Code-Beispiele und Präsentationen werden Ihnen zur Verfügung gestellt\n",
    "   - Sie erhalten Zugang zum GitHub-Repository mit allen Notebooks\n",
    "\n",
    "2. **Follow-up**\n",
    "   - Sie erhalten eine E-Mail mit zusätzlichen Ressourcen und Links\n",
    "   - Bei Bedarf können wir ein Follow-up-Meeting für weiterführende Fragen vereinbaren\n",
    "\n",
    "3. **Community**\n",
    "   - Treten Sie unserer Community bei, um sich mit anderen Teilnehmern auszutauschen\n",
    "   - Bleiben Sie über neue Entwicklungen und Veranstaltungen informiert\n",
    "\n",
    "### Persönliche Lernreise fortsetzen\n",
    "\n",
    "1. **Eigenes Projekt starten**\n",
    "   - Beginnen Sie mit einem eigenen LLM-Projekt basierend auf dem Gelernten\n",
    "   - Wenden Sie die Workshop-Inhalte auf reale Probleme an\n",
    "\n",
    "2. **Vertiefung in spezifischen Bereichen**\n",
    "   - Wählen Sie ein oder zwei Themenbereiche für eine tiefere Erkundung\n",
    "   - Experimentieren Sie mit fortgeschrittenen Konzepten wie Multi-Agenten-Systemen\n",
    "\n",
    "3. **Aktiv bleiben**\n",
    "   - Verfolgen Sie die schnelle Entwicklung im LLM-Bereich\n",
    "   - Beteiligen Sie sich an Open-Source-Projekten oder Diskussionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vielen Dank für Ihre Teilnahme!\n",
    "\n",
    "Wir hoffen, dass dieser Workshop Ihnen wertvolle Einblicke und praktische Fähigkeiten für die Entwicklung mit Large Language Models vermittelt hat. Wir wünschen Ihnen viel Erfolg bei Ihren zukünftigen KI-Projekten!\n",
    "\n",
    "Bei Fragen oder für weitere Unterstützung stehen wir Ihnen gerne zur Verfügung."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
