{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vektordatenbanken in LangChain\n",
    "\n",
    "In diesem Notebook lernen wir den Umgang mit Vektordatenbanken für semantische Suche und Retrieval-Augmented Generation (RAG) mit LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlagen: Embeddings erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Bibliotheken importieren\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# API-Schlüssel laden\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI Embeddings importieren\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Embedding-Modell initialisieren\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings für verschiedene Texte erstellen\n",
    "text1 = \"Vektordatenbanken sind spezialisierte Datenbanken für Ähnlichkeitssuche.\"\n",
    "text2 = \"Datenbanken zur Suche nach ähnlichen Vektoren werden Vektordatenbanken genannt.\"\n",
    "text3 = \"Machine Learning basiert auf mathematischen Modellen und Algorithmen.\"\n",
    "\n",
    "# Vektoren generieren\n",
    "vector1 = embeddings.embed_query(text1)\n",
    "vector2 = embeddings.embed_query(text2)\n",
    "vector3 = embeddings.embed_query(text3)\n",
    "\n",
    "# Länge eines Vektors anzeigen\n",
    "print(f\"Länge des Embedding-Vektors: {len(vector1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ähnlichkeiten zwischen Vektoren berechnen\n",
    "similarity_1_2 = 1 - cosine(vector1, vector2)  # Ähnliche Bedeutung\n",
    "similarity_1_3 = 1 - cosine(vector1, vector3)  # Unterschiedliche Bedeutung\n",
    "\n",
    "print(f\"Ähnlichkeit zwischen Text 1 und Text 2: {similarity_1_2:.4f}\")\n",
    "print(f\"Ähnlichkeit zwischen Text 1 und Text 3: {similarity_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dokumente laden und aufbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispieldaten erstellen\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Vektordatenbanken speichern und indizieren Vektoren für schnelle Ähnlichkeitssuche.\",\n",
    "        metadata={\"source\": \"definition.txt\", \"category\": \"database\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Chroma ist eine eingebettete Vektordatenbank für Python mit einfacher API.\",\n",
    "        metadata={\"source\": \"chroma.txt\", \"category\": \"database\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Qdrant ist eine hochleistungsfähige Vektordatenbank mit Filterung und Clustering.\",\n",
    "        metadata={\"source\": \"qdrant.txt\", \"category\": \"database\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Pinecone ist ein vollständig verwalteter Vektordatenbank-Service in der Cloud.\",\n",
    "        metadata={\"source\": \"pinecone.txt\", \"category\": \"cloud\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG (Retrieval Augmented Generation) verbindet Sprachmodelle mit externen Datenquellen.\",\n",
    "        metadata={\"source\": \"rag.txt\", \"category\": \"technique\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Embedding-Modelle wandeln Text in numerische Vektoren für maschinelles Lernen um.\",\n",
    "        metadata={\"source\": \"embeddings.txt\", \"category\": \"nlp\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vektordatenbank erstellen mit Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Erstellen einer temporären In-Memory Chroma Datenbank\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Anzahl der Dokumente in der Datenbank: {db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Einfache Ähnlichkeitssuche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine einfache Ähnlichkeitssuche durchführen\n",
    "query = \"Welche Vektordatenbanken gibt es?\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Top 3 ähnlichste Dokumente:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Quelle: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Suche mit Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ähnlichkeitssuche mit Scores\n",
    "results_with_scores = db.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(\"Top 3 ähnlichste Dokumente mit Ähnlichkeitswerten:\")\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Quelle: {doc.metadata['source']}\")\n",
    "    print(f\"   Ähnlichkeitswert: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metadaten-Filterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtern nach Metadaten\n",
    "filter_results = db.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    filter={\"category\": \"cloud\"}\n",
    ")\n",
    "\n",
    "print(\"Gefilterte Ergebnisse (nur 'cloud' Kategorie):\")\n",
    "for i, doc in enumerate(filter_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Kategorie: {doc.metadata['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vektordatenbank persistieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vektordatenbank persistieren (auf Festplatte speichern)\n",
    "persist_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Datenbank explizit speichern\n",
    "persist_db.persist()\n",
    "print(\"Vektordatenbank wurde in ./chroma_db gespeichert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gespeicherte Datenbank laden\n",
    "loaded_db = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Prüfen, ob die Daten korrekt geladen wurden\n",
    "loaded_results = loaded_db.similarity_search(query, k=2)\n",
    "print(\"Ergebnisse aus der geladenen Datenbank:\")\n",
    "for i, doc in enumerate(loaded_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dokumente mit TextSplitter vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Längerer Text als Beispiel\n",
    "long_text = \"\"\"\n",
    "Vektordatenbanken sind eine Klasse von Datenbankmanagementsystemen, die speziell für die Speicherung, \n",
    "Verwaltung und Abfrage von Vektordaten optimiert sind. Diese Vektoren repräsentieren typischerweise \n",
    "Embeddings oder numerische Darstellungen von Objekten wie Texten, Bildern oder anderen Daten.\n",
    "\n",
    "Die Hauptfunktion einer Vektordatenbank ist die effiziente Durchführung von Ähnlichkeitssuchen in \n",
    "hochdimensionalen Räumen. Das bedeutet, dass sie für gegebene Vektoren die ähnlichsten Vektoren \n",
    "in der Datenbank finden kann. Diese Funktionalität ist für viele Anwendungen unerlässlich, darunter:\n",
    "\n",
    "1. Semantische Suche: Finden von Dokumenten basierend auf der Bedeutung, nicht nur auf Schlüsselwörtern\n",
    "2. Empfehlungssysteme: Empfehlen ähnlicher Produkte oder Inhalte\n",
    "3. Bildsuche: Finden ähnlicher Bilder\n",
    "4. Anomalieerkennung: Identifizieren ungewöhnlicher Datenpunkte\n",
    "\n",
    "Populäre Vektordatenbanken sind Chroma, Qdrant, Pinecone, Weaviate und Milvus. Jede hat ihre eigenen \n",
    "Stärken und Schwächen hinsichtlich Geschwindigkeit, Skalierbarkeit, Features und Einfachheit der Nutzung.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Text-Splitter erstellen\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # Maximale Größe eines Chunks in Zeichen\n",
    "    chunk_overlap=50,  # Überlappung zwischen Chunks in Zeichen\n",
    "    length_function=len,  # Funktion zur Längenmessung\n",
    "    is_separator_regex=False,  # Kein regex für Separatoren\n",
    ")\n",
    "\n",
    "# Text in Chunks aufteilen\n",
    "chunks = text_splitter.create_documents([long_text])\n",
    "\n",
    "print(f\"Text wurde in {len(chunks)} Chunks aufgeteilt:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i} (Länge: {len(chunk.page_content)} Zeichen):\")\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks in Vektordatenbank speichern\n",
    "chunks_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Suche in den Chunks\n",
    "chunk_results = chunks_db.similarity_search(\n",
    "    \"Wofür werden Vektordatenbanken verwendet?\",\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(\"Ergebnisse aus den Chunks:\")\n",
    "for i, doc in enumerate(chunk_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# RAG-Chain erstellen\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=chunks_db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True  # Quellen zurückgeben\n",
    ")\n",
    "\n",
    "# Anfrage stellen\n",
    "result = qa_chain.invoke({\"query\": \"Für welche Anwendungen werden Vektordatenbanken eingesetzt?\"})\n",
    "\n",
    "print(f\"Antwort: {result['result']}\")\n",
    "print(\"\\nVerwendete Quellen:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\nQuelle {i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Erweiterte Retrieval-Techniken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# MultiQuery-Retriever erstellen\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chunks_db.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Anfrage mit mehreren generierten Varianten\n",
    "multi_query_results = multi_query_retriever.get_relevant_documents(\n",
    "    \"Was sind die besten Vektordatenbanken?\"\n",
    ")\n",
    "\n",
    "print(f\"MultiQueryRetriever hat {len(multi_query_results)} Dokumente gefunden:\")\n",
    "for i, doc in enumerate(multi_query_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Dokument-Kompressor erstellen\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# ContextualCompressionRetriever erstellen\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=chunks_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    doc_compressor=compressor\n",
    ")\n",
    "\n",
    "# Komprimierte Suche durchführen\n",
    "compressed_results = compression_retriever.get_relevant_documents(\n",
    "    \"Welche Anwendungsfälle haben Vektordatenbanken?\"\n",
    ")\n",
    "\n",
    \"ContextualCompressionRetriever hat {len(compressed_results)} komprimierte Dokumente zurückgegeben:\")\n",
    "for i, doc in enumerate(compressed_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integration mit Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dieser Code funktioniert nur, wenn Qdrant lokal oder als Service verfügbar ist\n",
    "# from langchain_community.vectorstores import Qdrant\n",
    "# import qdrant_client\n",
    "\n",
    "# # Qdrant-Client erstellen (lokal oder remote)\n",
    "# client = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# # Sammlung in Qdrant erstellen (falls sie noch nicht existiert)\n",
    "# try:\n",
    "#     client.get_collection(\"vector_db_demo\")\n",
    "# except Exception:\n",
    "#     client.create_collection(\n",
    "#         collection_name=\"vector_db_demo\",\n",
    "#         vectors_config=qdrant_client.http.models.VectorsConfig(\n",
    "#             size=1536,  # Dimensionen des OpenAI Embedding-Modells\n",
    "#             distance=qdrant_client.http.models.Distance.COSINE\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Dokumente in Qdrant speichern\n",
    "# qdrant_db = Qdrant.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embeddings,\n",
    "#     url=\"http://localhost:6333\",\n",
    "#     collection_name=\"vector_db_demo\",\n",
    "# )\n",
    "\n",
    "# # Suche mit Metadaten-Filter\n",
    "# qdrant_results = qdrant_db.similarity_search_with_score(\n",
    "#     \"Welche Vektordatenbanken gibt es?\",\n",
    "#     k=2\n",
    "# )\n",
    "\n",
    "print(\"Hinweis: Um Qdrant zu verwenden, entfernen Sie die Kommentare im obigen Code und stellen Sie sicher, dass ein Qdrant-Server läuft.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance-Optimierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: MMR (Maximum Marginal Relevance) für diversere Ergebnisse\n",
    "mmr_results = db.max_marginal_relevance_search(\n",
    "    \"Welche Vektordatenbanken gibt es?\",\n",
    "    k=3,      # Anzahl der zurückzugebenden Dokumente\n",
    "    fetch_k=6,  # Anzahl der initialen Dokumente vor Diversifizierung\n",
    "    lambda_mult=0.5  # Balance zwischen Relevanz (1.0) und Diversität (0.0)\n",
    ")\n",
    "\n",
    "print(\"MMR Ergebnisse (diversifiziert):\")\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching für Embedding-Berechnung\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "# Cache für LLM-Aufrufe aktivieren\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# Dieser zweite Aufruf sollte den Cache nutzen und schneller sein\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "vector_first = embeddings.embed_query(\"Vektordatenbanken sind leistungsfähige Werkzeuge für Ähnlichkeitssuche.\")\n",
    "first_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "vector_second = embeddings.embed_query(\"Vektordatenbanken sind leistungsfähige Werkzeuge für Ähnlichkeitssuche.\")\n",
    "second_duration = time.time() - start_time\n",
    "\n",
    "print(f\"Erste Embedding-Berechnung: {first_duration:.4f} Sekunden\")\n",
    "print(f\"Zweite Embedding-Berechnung: {second_duration:.4f} Sekunden\")\n",
    "print(f\"Geschwindigkeitsvorteil: {first_duration/second_duration:.2f}x schneller beim zweiten Mal (bei Cache-Treffer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir gelernt:\n",
    "\n",
    "1. Grundlagen von Vektordatenbanken und Embeddings\n",
    "2. Wie man Texte in Vektoren umwandelt und Ähnlichkeiten berechnet\n",
    "3. Dokumente zu chunken und in Vektordatenbanken zu speichern\n",
    "4. Ähnlichkeitssuche mit verschiedenen Parametern durchzuführen\n",
    "5. RAG-Implementierung für bessere LLM-Antworten\n",
    "6. Fortgeschrittene Retrieval-Techniken wie MultiQuery und Compression\n",
    "7. Performance-Optimierungen für Vektordatenbanken\n",
    "\n",
    "Diese Techniken bilden die Grundlage für moderne KI-Anwendungen, die externe Daten und Wissensquellen in große Sprachmodelle integrieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}