{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangFuse: Monitoring und Tracing von LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook lernen Sie, wie Sie LangFuse für Monitoring, Tracing und Evaluierung Ihrer LLM-Anwendungen einsetzen können. LangFuse ist ein Open-Source-Tool, das Ihnen hilft, die Leistung und Qualität Ihrer KI-Anwendungen zu überwachen und zu verbessern.\n",
    "\n",
    "## Überblick\n",
    "\n",
    "- Einführung in LangFuse\n",
    "- Installation und Konfiguration\n",
    "- Einfaches Tracing einer LLM-Anfrage\n",
    "- Integration mit LangChain\n",
    "- Traces mit Spans und Generations\n",
    "- Metriken und Evaluierungsmöglichkeiten\n",
    "- Praktische Anwendungsfälle\n",
    "\n",
    "## Voraussetzungen\n",
    "\n",
    "Um diesem Notebook zu folgen, benötigen Sie:\n",
    "\n",
    "- LangFuse-Konto (entweder LangFuse Cloud oder selbst gehostete Instanz)\n",
    "- LangFuse API-Schlüssel\n",
    "- OpenAI API-Schlüssel\n",
    "\n",
    "Wenn Sie noch kein LangFuse-Konto haben, können Sie sich [hier](https://cloud.langfuse.com) registrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation der erforderlichen Pakete\n",
    "\n",
    "Zunächst müssen wir die erforderlichen Pakete installieren. Wir benötigen die LangFuse-Bibliothek und die LangChain-Integration mit LangFuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation der benötigten Bibliotheken\n",
    "!pip install langfuse openai langchain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Umgebungsvariablen konfigurieren\n",
    "\n",
    "Wir müssen unsere API-Schlüssel konfigurieren. In einer Produktionsumgebung würden Sie diese als Umgebungsvariablen setzen. Für dieses Notebook werden wir sie direkt im Code definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitte setzen Sie die LangFuse API-Schlüssel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Laden der Umgebungsvariablen aus einer .env-Datei (falls vorhanden)\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "\n",
    "# LangFuse-Konfiguration\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"\")\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "\n",
    "# OpenAI-Konfiguration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Überprüfen, ob die API-Schlüssel gesetzt sind\n",
    "if not LANGFUSE_PUBLIC_KEY or not LANGFUSE_SECRET_KEY:\n",
    "    print(\"Bitte setzen Sie die LangFuse API-Schlüssel\")\n",
    "    \n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Bitte setzen Sie den OpenAI API-Schlüssel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Einfaches Tracing mit LangFuse\n",
    "\n",
    "Beginnen wir mit einem einfachen Beispiel, um zu verstehen, wie LangFuse funktioniert. Wir werden einen Trace erstellen und eine LLM-Anfrage verfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Langfuse client is disabled since no public_key was provided as a parameter or environment variable 'LANGFUSE_PUBLIC_KEY'. See our docs: https://langfuse.com/docs/sdk/python/low-level-sdk#initialize-client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace erstellt mit ID: f7707b72-55f6-4b80-9abe-ef29ece9036e\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialisieren des LangFuse-Clients\n",
    "langfuse = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")\n",
    "\n",
    "# Einen Trace erstellen\n",
    "trace = langfuse.trace(\n",
    "    name=\"einfaches_beispiel\",\n",
    "    metadata={\"beispiel_typ\": \"einfach\", \"benutzer_id\": \"demo-user-123\"}\n",
    ")\n",
    "\n",
    "print(f\"Trace erstellt mit ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun fügen wir eine Generation zu diesem Trace hinzu. Eine Generation ist eine Aufzeichnung einer LLM-Anfrage und der entsprechenden Antwort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Generation zum Trace hinzufügen\n",
    "generation = trace.generation(\n",
    "    name=\"erste_anfrage\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=\"Erkläre in einem kurzen Absatz, was künstliche Intelligenz ist.\",\n",
    "    completion=\"Künstliche Intelligenz (KI) bezeichnet Systeme, die menschenähnliche Intelligenz simulieren können, indem sie Daten analysieren, Muster erkennen und selbstständig Entscheidungen treffen. Diese Technologie umfasst Bereiche wie maschinelles Lernen, natürliche Sprachverarbeitung und Computer Vision und findet Anwendung in zahlreichen Bereichen von virtuellen Assistenten bis hin zu autonomen Fahrzeugen.\",\n",
    "    metadata={\"zweck\": \"einfache_erklaerung\", \"zielgruppe\": \"anfaenger\"}\n",
    ")\n",
    "\n",
    "print(f\"Generation hinzugefügt mit ID: {generation.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch einen Span hinzufügen, um eine längere Operation zu verfolgen. Ein Span ist eine zeitlich begrenzte Operation, die Teil eines Traces ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Einen Span zum Trace hinzufügen\n",
    "span = trace.span(name=\"datenverarbeitung\")\n",
    "\n",
    "# Eine Operation simulieren\n",
    "print(\"Verarbeite Daten...\")\n",
    "time.sleep(1.5)  # Simulation einer zeitaufwändigen Operation\n",
    "\n",
    "# Den Span beenden\n",
    "span.end()\n",
    "\n",
    "print(f\"Span beendet mit ID: {span.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließlich können wir den Trace beenden und Feedback hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback zum Trace hinzufügen\n",
    "feedback = langfuse.feedback(\n",
    "    trace_id=trace.id,\n",
    "    name=\"klarheit\",\n",
    "    value=0.9,  # Wert zwischen 0 und 1\n",
    "    comment=\"Die Erklärung war sehr klar und leicht verständlich.\"\n",
    ")\n",
    "\n",
    "print(f\"Feedback hinzugefügt mit ID: {feedback.id}\")\n",
    "\n",
    "# Trace beenden\n",
    "trace.update(status=\"success\")\n",
    "\n",
    "print(f\"Trace erfolgreich aktualisiert: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reale LLM-Interaktion mit LangFuse Tracing\n",
    "\n",
    "Jetzt werden wir eine echte Interaktion mit OpenAI durchführen und diese mit LangFuse verfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# OpenAI-Client initialisieren\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Neuen Trace erstellen\n",
    "trace_real = langfuse.trace(name=\"echte_llm_anfrage\")\n",
    "\n",
    "# Zeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Einen Span für die gesamte Operation erstellen\n",
    "with trace_real.span(name=\"vollstaendige_operation\") as operation_span:\n",
    "    \n",
    "    # Prompt vorbereiten\n",
    "    with operation_span.span(name=\"prompt_vorbereitung\") as prompt_span:\n",
    "        prompt = \"\"\"Du bist ein hilfreicher Assistent für Python-Programmierung. \n",
    "        Bitte erstelle eine einfache Funktion, die überprüft, \n",
    "        ob eine Zahl eine Primzahl ist.\"\"\"\n",
    "        prompt_span.end()\n",
    "    \n",
    "    # LLM-Anfrage durchführen und als Generation aufzeichnen\n",
    "    with operation_span.span(name=\"llm_anfrage\") as llm_span:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Python-Programmierassistent.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Antwort extrahieren\n",
    "        completion = response.choices[0].message.content\n",
    "        llm_span.end()\n",
    "    \n",
    "    # Die Generation zum Trace hinzufügen\n",
    "    generation = trace_real.generation(\n",
    "        name=\"primzahl_funktion\",\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        completion=completion,\n",
    "        usage={\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        metadata={\n",
    "            \"zweck\": \"code_generation\",\n",
    "            \"programmiersprache\": \"python\",\n",
    "            \"aufgabe\": \"primzahl_prüfung\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Gesamtzeit berechnen    \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Trace aktualisieren\n",
    "trace_real.update(\n",
    "    status=\"success\",\n",
    "    metadata={\n",
    "        \"dauer_sekunden\": duration,\n",
    "        \"token_gesamt\": response.usage.total_tokens\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"LLM-Antwort:\")\n",
    "print(completion)\n",
    "print(f\"\\nTrace ID: {trace_real.id}\")\n",
    "print(f\"Dauer: {duration:.2f} Sekunden\")\n",
    "print(f\"Gesamt-Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration mit LangChain\n",
    "\n",
    "LangFuse lässt sich nahtlos in LangChain integrieren, um Chains, Agents und andere LangChain-Komponenten zu verfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import LangfuseCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# LangFuse Callback-Handler erstellen\n",
    "langfuse_handler = LangfuseCallbackHandler(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")\n",
    "\n",
    "# LLM mit Callback-Handler initialisieren\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    callbacks=[langfuse_handler]\n",
    ")\n",
    "\n",
    "# Prompt-Template erstellen\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte für {thema}. Beantworte die folgende Frage.\"),\n",
    "    HumanMessage(content=\"{frage}\")\n",
    "])\n",
    "\n",
    "# Chain erstellen\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "\n",
    "# Chain ausführen\n",
    "response = chain.invoke({\n",
    "    \"thema\": \"deutsche Geschichte\",\n",
    "    \"frage\": \"Was waren die wichtigsten Ereignisse beim Fall der Berliner Mauer?\"\n",
    "})\n",
    "\n",
    "print(\"LangChain-Antwort:\")\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fortgeschrittenes Tracing einer mehrschichtigen Anwendung\n",
    "\n",
    "Jetzt werden wir eine komplexere Anwendung mit mehreren Schritten erstellen und mit LangFuse verfolgen. Dies simuliert einen typischen RAG-Workflow (Retrieval Augmented Generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.client import StatelessTracer\n",
    "\n",
    "# Stateless Tracer für komplexere Anwendungen\n",
    "tracer = StatelessTracer(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")\n",
    "\n",
    "# Einen neuen Trace für die RAG-Anwendung erstellen\n",
    "trace_rag = tracer.trace(name=\"rag_anwendung\", tags=[\"demo\", \"rag\", \"workshop\"])\n",
    "\n",
    "# Benutzerfrage\n",
    "query = \"Was sind die Vorteile von Vektordatenbanken für RAG-Anwendungen?\"\n",
    "\n",
    "# Simulierter RAG-Workflow\n",
    "with trace_rag.span(name=\"rag_workflow\") as rag_span:\n",
    "    \n",
    "    # 1. Schritt: Query-Verständnis\n",
    "    with rag_span.span(name=\"query_verstaendnis\") as query_span:\n",
    "        # Simulieren der Query-Analyse\n",
    "        print(\"Analysiere Anfrage...\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Generierung für die Query-Analyse\n",
    "        query_analysis = trace_rag.generation(\n",
    "            name=\"query_analyse\",\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            prompt=f\"Analysiere die folgende Frage und extrahiere die Hauptthemen: {query}\",\n",
    "            completion=\"Hauptthemen: Vektordatenbanken, RAG-Anwendungen, Vorteile/Nutzen\",\n",
    "            metadata={\"schritt\": \"1_analyse\"}\n",
    "        )\n",
    "        query_span.end()\n",
    "    \n",
    "    # 2. Schritt: Dokumenten-Retrieval\n",
    "    with rag_span.span(name=\"dokument_retrieval\") as retrieval_span:\n",
    "        print(\"Hole relevante Dokumente aus der Vektordatenbank...\")\n",
    "        time.sleep(1.0)\n",
    "        \n",
    "        # Simulieren von gefundenen Dokumenten\n",
    "        retrieved_docs = [\n",
    "            \"Vektordatenbanken speichern Embeddings für schnelle Ähnlichkeitssuche...\",\n",
    "            \"RAG-Anwendungen kombinieren Retrieval mit generativen Modellen...\",\n",
    "            \"Vorteile von Vektordatenbanken sind Geschwindigkeit und semantische Suche...\"\n",
    "        ]\n",
    "        \n",
    "        # Eigenschaften des Retrievals aufzeichnen\n",
    "        retrieval_span.update(\n",
    "            metadata={\n",
    "                \"anzahl_dokumente\": len(retrieved_docs),\n",
    "                \"retrieval_methode\": \"vector_similarity\",\n",
    "                \"top_k\": 3\n",
    "            }\n",
    "        )\n",
    "        retrieval_span.end()\n",
    "    \n",
    "    # 3. Schritt: Prompt-Engineering für RAG\n",
    "    with rag_span.span(name=\"prompt_engineering\") as prompt_span:\n",
    "        print(\"Erstelle RAG-Prompt...\")\n",
    "        \n",
    "        # Erstellen des RAG-Prompts\n",
    "        context = \"\\n\\n\".join(retrieved_docs)\n",
    "        rag_prompt = f\"\"\"Basierend auf folgenden Informationen, beantworte die Frage.\n",
    "        \n",
    "        KONTEXT:\n",
    "        {context}\n",
    "        \n",
    "        FRAGE:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt_span.end()\n",
    "    \n",
    "    # 4. Schritt: Generierung der Antwort\n",
    "    with rag_span.span(name=\"antwort_generierung\") as generation_span:\n",
    "        print(\"Generiere Antwort mit LLM...\")\n",
    "        time.sleep(1.2)\n",
    "        \n",
    "        # Simulierte Antwortgenerierung\n",
    "        answer = \"\"\"Vektordatenbanken bieten mehrere entscheidende Vorteile für RAG-Anwendungen:\n",
    "        \n",
    "        1. Geschwindigkeit: Sie ermöglichen schnelle Ähnlichkeitssuchen in großen Datenmengen\n",
    "        2. Semantische Suche: Sie finden Dokumente basierend auf Bedeutung, nicht nur auf exakten Wortübereinstimmungen\n",
    "        3. Skalierbarkeit: Sie können effizient mit wachsenden Datenmengen umgehen\n",
    "        4. Flexibilität: Sie unterstützen verschiedene Embedding-Modelle und Ähnlichkeitsmaße\n",
    "        5. Filterungsmöglichkeiten: Sie ermöglichen die Kombination von Vektorsuche mit Metadatenfilterung\n",
    "        \n",
    "        Diese Eigenschaften machen Vektordatenbanken zum idealen Speichermechanismus für RAG-Anwendungen, da sie die relevantesten Informationen für die Eingabeaufforderung des generativen Modells liefern können.\"\"\"\n",
    "        \n",
    "        # Aufzeichnen der Generierung\n",
    "        final_generation = trace_rag.generation(\n",
    "            name=\"finale_antwort\",\n",
    "            model=\"gpt-4\",\n",
    "            prompt=rag_prompt,\n",
    "            completion=answer,\n",
    "            metadata={\n",
    "                \"schritt\": \"4_generierung\",\n",
    "                \"kontext_laenge\": len(context),\n",
    "                \"verwendete_dokumente\": len(retrieved_docs)\n",
    "            }\n",
    "        )\n",
    "        generation_span.end()\n",
    "\n",
    "# Trace abschließen\n",
    "trace_rag.update(status=\"success\")\n",
    "\n",
    "print(\"\\nFertige Antwort:\")\n",
    "print(answer)\n",
    "print(f\"\\nRAG-Trace ID: {trace_rag.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metriken und Evaluierung mit LangFuse\n",
    "\n",
    "LangFuse ermöglicht es nicht nur, Anfragen zu verfolgen, sondern auch, die Qualität und Leistung zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feedback-Beispiel: Benutzerfeedback hinzufügen\n",
    "user_feedback = langfuse.feedback(\n",
    "    trace_id=trace_rag.id,\n",
    "    name=\"nuetzlichkeit\",\n",
    "    value=0.85,  # 0-1 Skala\n",
    "    comment=\"Die Antwort war informativ und gut strukturiert.\"\n",
    ")\n",
    "\n",
    "print(f\"Benutzerfeedback hinzugefügt: {user_feedback.id}\")\n",
    "\n",
    "# 2. Automatische Evaluierung mit einem LLM\n",
    "eval_prompt = f\"\"\"\n",
    "Bewerte die folgende Antwort auf die Frage: \"{query}\"\n",
    "\n",
    "ANTWORT:\n",
    "{answer}\n",
    "\n",
    "Bewerte die Antwort auf einer Skala von 0 bis 10 in folgenden Kategorien:\n",
    "1. Relevanz zur Frage\n",
    "2. Korrektheit der Informationen\n",
    "3. Vollständigkeit\n",
    "4. Klarheit und Verständlichkeit\n",
    "\"\"\"\n",
    "\n",
    "eval_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein kritischer Evaluator für KI-generierte Antworten.\"},\n",
    "        {\"role\": \"user\", \"content\": eval_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_result = eval_response.choices[0].message.content\n",
    "\n",
    "# Feedback basierend auf der automatischen Evaluierung hinzufügen\n",
    "auto_feedback = langfuse.feedback(\n",
    "    trace_id=trace_rag.id,\n",
    "    name=\"llm_evaluierung\",\n",
    "    comment=eval_result,\n",
    "    metadata={\"evaluierung_methode\": \"llm_as_judge\", \"evaluator_model\": \"gpt-3.5-turbo\"}\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatische Evaluierung:\")\n",
    "print(eval_result)\n",
    "print(f\"Evaluierungs-Feedback-ID: {auto_feedback.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse und Visualisierung in LangFuse UI\n",
    "\n",
    "Die bisher erstellten Traces können in der LangFuse-Benutzeroberfläche angesehen werden.\n",
    "\n",
    "- Besuchen Sie [LangFuse Cloud](https://cloud.langfuse.com) (oder Ihre selbst gehostete Instanz)\n",
    "- Melden Sie sich mit Ihren Zugangsdaten an\n",
    "- Navigieren Sie zum Bereich \"Traces\"\n",
    "- Suchen Sie nach den erstellten Trace-IDs oder dem Namen \"rag_anwendung\"\n",
    "\n",
    "In der UI können Sie:\n",
    "\n",
    "1. Den vollständigen Ablauf eines Traces mit allen Spans und Generations sehen\n",
    "2. Die Dauer jedes Schritts analysieren\n",
    "3. Prompts und Completions im Detail betrachten\n",
    "4. Metriken wie Token-Nutzung und Kosten einsehen\n",
    "5. Feedback und Evaluierungen überprüfen\n",
    "\n",
    "![LangFuse UI Beispiel](https://docs.langfuse.com/img/ui/dataset-detail.jpg)\n",
    "\n",
    "*Bild: Beispiel einer LangFuse-Trace-Visualisierung (Quelle: LangFuse-Dokumentation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysen und Dashboards\n",
    "\n",
    "LangFuse bietet verschiedene Analysemöglichkeiten, um die Leistung Ihrer KI-Anwendungen zu überwachen und zu verbessern.\n",
    "\n",
    "### Wichtige Metriken in LangFuse\n",
    "\n",
    "1. **Latenz**: Wie schnell antwortet Ihre Anwendung?\n",
    "2. **Kosten**: Wie viel kosten Ihre LLM-Anfragen?\n",
    "3. **Token-Nutzung**: Wie viele Tokens verbrauchen Ihre Prompts und Antworten?\n",
    "4. **Qualität**: Wie gut sind die generierten Antworten basierend auf Feedback?\n",
    "5. **Trace-Vollständigkeit**: Wie durchgängig ist Ihr Tracing?\n",
    "\n",
    "### Einsatz im Entwicklungszyklus\n",
    "\n",
    "- **Entwicklung**: Debugging und Optimierung von Prompts\n",
    "- **Qualitätssicherung**: Identifizierung von Schwachstellen\n",
    "- **Produktion**: Überwachung von Leistung und Kosten\n",
    "- **Verbesserung**: Datengestützte Entscheidungen für Verbesserungen\n",
    "\n",
    "### Praktische Tipps zur Nutzung von LangFuse-Dashboards\n",
    "\n",
    "1. Erstellen Sie benutzerdefinierte Dashboards für verschiedene Anwendungsfälle\n",
    "2. Setzen Sie Alerts für ungewöhnliche Metriken wie hohe Tokenverbrauch oder Latenz\n",
    "3. Vergleichen Sie verschiedene Modelle und Prompt-Strategien\n",
    "4. Nutzen Sie Datasets für systematische Evaluierungen\n",
    "5. Exportieren Sie Daten für weiterführende Analysen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Beispiel: A/B-Testing mit LangFuse\n",
    "\n",
    "Ein wichtiger Anwendungsfall für LangFuse ist A/B-Testing verschiedener Modelle oder Prompt-Strategien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# A/B-Test für verschiedene Prompt-Strukturen\n",
    "test_query = \"Erkläre den Unterschied zwischen Supervised und Unsupervised Learning.\"\n",
    "\n",
    "# Zwei verschiedene Prompt-Strategien\n",
    "prompt_a = f\"\"\"Als Experte für maschinelles Lernen, erkläre den Unterschied zwischen \n",
    "Supervised und Unsupervised Learning. Gib Beispiele für beide Ansätze.\"\"\"\n",
    "\n",
    "prompt_b = f\"\"\"Definiere kurz und präzise:\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Hauptunterschiede\n",
    "4. Typische Anwendungsfälle für jeden Ansatz\"\"\"\n",
    "\n",
    "# Test-IDs zur Identifizierung der Varianten\n",
    "test_id = f\"prompt_test_{int(time.time())}\"\n",
    "\n",
    "# Funktion zum Durchführen eines Tests\n",
    "def run_test_variant(variant, prompt):\n",
    "    # Trace mit Test-Metadaten erstellen\n",
    "    trace = langfuse.trace(\n",
    "        name=\"ab_test_ml_erklaerung\",\n",
    "        tags=[\"ab_test\", f\"variant_{variant}\"],\n",
    "        metadata={\n",
    "            \"test_id\": test_id,\n",
    "            \"variant\": variant,\n",
    "            \"query\": test_query\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # LLM-Anfrage\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Du bist ein Experte für maschinelles Lernen.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    completion = response.choices[0].message.content\n",
    "    \n",
    "    # Generation aufzeichnen\n",
    "    generation = trace.generation(\n",
    "        name=f\"variant_{variant}\",\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        completion=completion,\n",
    "        usage={\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Trace abschließen\n",
    "    trace.update(status=\"success\")\n",
    "    \n",
    "    return {\n",
    "        \"trace_id\": trace.id,\n",
    "        \"variant\": variant,\n",
    "        \"completion\": completion,\n",
    "        \"tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Tests durchführen\n",
    "result_a = run_test_variant(\"A\", prompt_a)\n",
    "result_b = run_test_variant(\"B\", prompt_b)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f\"Test-ID: {test_id}\\n\")\n",
    "\n",
    "print(f\"=== Variante A (Trace-ID: {result_a['trace_id']}) ===\")\n",
    "print(f\"Tokens: {result_a['tokens']}\")\n",
    "print(\"Antwort:\")\n",
    "print(result_a['completion'][:200] + \"...\" if len(result_a['completion']) > 200 else result_a['completion'])\n",
    "\n",
    "print(f\"\\n=== Variante B (Trace-ID: {result_b['trace_id']}) ===\")\n",
    "print(f\"Tokens: {result_b['tokens']}\")\n",
    "print(\"Antwort:\")\n",
    "print(result_b['completion'][:200] + \"...\" if len(result_b['completion']) > 200 else result_b['completion'])\n",
    "\n",
    "print(\"\\nBeide Varianten sind in LangFuse verfügbar und können dort verglichen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verwendung von LangFuse in einem Produktionskontext\n",
    "\n",
    "In einem Produktionskontext sollten Sie die folgenden Best Practices beachten:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "1. **Strukturierte Tracing-Hierarchie**\n",
    "   - Verwenden Sie konsistente Namenskonventionen für Traces, Spans und Generations\n",
    "   - Ordnen Sie Spans logisch in einer hierarchischen Struktur an\n",
    "   - Nutzen Sie Tags und Metadaten für eine einfache Filterung\n",
    "\n",
    "2. **Fehlerbehandlung**\n",
    "   - Zeichnen Sie Fehler explizit auf, indem Sie den Status auf `error` setzen\n",
    "   - Fügen Sie Fehlermeldungen und Stack-Traces als Metadaten hinzu\n",
    "   - Implementieren Sie Try-Catch-Blöcke um das Tracing\n",
    "\n",
    "3. **Datenschutz**\n",
    "   - Entfernen Sie personenbezogene Daten aus Prompts und Antworten\n",
    "   - Verwenden Sie Filter oder Masking-Techniken für sensible Informationen\n",
    "   - Setzen Sie `user_id` nur verschlüsselt oder anonymisiert ein\n",
    "\n",
    "4. **Batch-Integration**\n",
    "   - Für hohe Durchsatzraten: Verwenden Sie die Batch-API für eine effiziente Datenübermittlung\n",
    "   - Implementieren Sie eine lokale Zwischenspeicherung für den Fall von Konnektivitätsproblemen\n",
    "\n",
    "5. **Monitoring**\n",
    "   - Richten Sie Alerts für außergewöhnliche Aktivitäten ein\n",
    "   - Überwachen Sie die API-Nutzung und Kosten\n",
    "   - Erstellen Sie Dashboards für wichtige Leistungsindikatoren\n",
    "\n",
    "### Beispiel für eine Produktionsintegration\n",
    "\n",
    "```python\n",
    "from langfuse import Langfuse\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "# Konfiguration der Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"llm_app\")\n",
    "\n",
    "# LangFuse-Client initialisieren\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    ")\n",
    "\n",
    "# Decorator für Tracing von Funktionen\n",
    "def trace_function(name=None, tags=None):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            func_name = name or func.__name__\n",
    "            trace = langfuse_client.trace(name=func_name, tags=tags or [])\n",
    "            \n",
    "            try:\n",
    "                with trace.span(name=f\"{func_name}_execution\") as span:\n",
    "                    result = func(*args, **kwargs, trace=trace)\n",
    "                    trace.update(status=\"success\")\n",
    "                    return result\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error in {func_name}: {str(e)}\")\n",
    "                trace.update(\n",
    "                    status=\"error\",\n",
    "                    metadata={\n",
    "                        \"error\": str(e),\n",
    "                        \"error_type\": type(e).__name__\n",
    "                    }\n",
    "                )\n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Beispiel für eine Funktion mit Tracing\n",
    "@trace_function(name=\"process_customer_query\", tags=[\"production\", \"customer_service\"])\n",
    "def process_customer_query(query, customer_id, trace=None):\n",
    "    # Anonymisieren von Daten\n",
    "    safe_query = anonymize_pii(query)  # Fiktive Funktion zur PII-Entfernung\n",
    "    \n",
    "    # LLM-Verarbeitung mit Tracing\n",
    "    with trace.span(name=\"query_processing\") as processing_span:\n",
    "        # LLM-Verarbeitung hier...\n",
    "        response = \"Antwort auf die Kundenanfrage\"\n",
    "        \n",
    "    return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Übungen\n",
    "\n",
    "Versuchen Sie, die folgenden Übungen zu lösen, um Ihr Verständnis von LangFuse zu vertiefen.\n",
    "\n",
    "### Übung 1: Grundlegende Trace-Erstellung\n",
    "\n",
    "Erstellen Sie einen Trace, der eine einfache Konversation mit einem LLM verfolgt. Die Konversation sollte mindestens 3 Nachrichtenwechsel enthalten, und Sie sollten für jede Nachricht eine separate Generation aufzeichnen.\n",
    "\n",
    "### Übung 2: Leistungsvergleich\n",
    "\n",
    "Vergleichen Sie die Leistung von zwei verschiedenen LLM-Modellen (z.B. GPT-3.5-Turbo und GPT-4) für die gleiche Aufgabe. Zeichnen Sie Metriken wie Antwortzeit, Token-Verbrauch und Antwortqualität auf.\n",
    "\n",
    "### Übung 3: Integration mit eigenen Workflows\n",
    "\n",
    "Integrieren Sie LangFuse in einen eigenen LangChain-Workflow Ihrer Wahl. Der Workflow sollte mindestens einen Agenten oder eine Chain verwenden und mindestens ein Tool einbinden.\n",
    "\n",
    "### Übung 4: Automatisierte Qualitätsbewertung\n",
    "\n",
    "Implementieren Sie eine automatisierte Qualitätsbewertung für LLM-Antworten mit LangFuse. Verwenden Sie dazu das LLM-as-a-Judge-Konzept, um die Qualität von Antworten auf einer Skala von 1-10 zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Zusätzliche Ressourcen\n",
    "\n",
    "- [LangFuse Dokumentation](https://langfuse.com/docs)\n",
    "- [LangFuse GitHub Repository](https://github.com/langfuse/langfuse)\n",
    "- [LangChain-LangFuse Integration](https://python.langchain.com/docs/integrations/callbacks/langfuse)\n",
    "- [Blog: Using LangFuse to Debug and Optimize LLM Applications](https://langfuse.com/blog)\n",
    "- [Discord Community](https://discord.gg/7NXusRtVa8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben Sie gelernt:\n",
    "\n",
    "- Wie LangFuse für das Monitoring und Tracing von LLM-Anwendungen eingesetzt wird\n",
    "- Wie Sie einfaches und fortgeschrittenes Tracing implementieren können\n",
    "- Wie LangFuse mit LangChain integriert werden kann\n",
    "- Wie Sie Metriken und Evaluierungen für Ihre LLM-Anwendungen erfassen können\n",
    "- Wie Sie A/B-Tests mit LangFuse durchführen können\n",
    "- Best Practices für den Einsatz von LangFuse in Produktionsumgebungen\n",
    "\n",
    "LangFuse ist ein leistungsstarkes Werkzeug, das Ihnen hilft, Ihre LLM-Anwendungen besser zu verstehen, zu debuggen und zu optimieren. Durch die Integration von LangFuse in Ihren Entwicklungsworkflow können Sie die Qualität Ihrer KI-Anwendungen kontinuierlich verbessern und gleichzeitig Kosten und Leistung im Auge behalten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
