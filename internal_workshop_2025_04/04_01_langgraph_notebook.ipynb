{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph\n",
    "\n",
    "In diesem Notebook werden wir uns mit LangGraph beschäftigen, einer Erweiterung des LangChain-Frameworks, die darauf ausgerichtet ist, komplexe, zustandsbehaftete Workflows mit LLMs zu modellieren.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- LangGraph's Architektur und Grundkonzepte verstehen\n",
    "- Den Unterschied zwischen LangChain und LangGraph erkennen\n",
    "- Einfache Agenten mit LangGraph erstellen\n",
    "- Komplexere Agenten mit mehreren Tools implementieren\n",
    "- LangGraph in bestehende Systeme integrieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Pakete importieren\n",
    "from typing import Annotated, Dict, List, Tuple, TypedDict, Union, Any\n",
    "import operator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph import MessageGraph\n",
    "\n",
    "load_dotenv()  # Laden der Umgebungsvariablen aus .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM initialisieren\n",
    "def llm(temperature=0.0, model_name=\"gpt-4o\"):\n",
    "    return ChatOpenAI(temperature=temperature, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Was ist LangGraph?\n",
    "\n",
    "LangGraph ist ein Framework, das auf LangChain aufbaut und speziell für die Erstellung zustandsbehafteter, multi-actor Anwendungen mit LLMs konzipiert wurde. Es stellt einen Übergang von den einfachen sequentiellen Ketten in LangChain zu komplexen Graphen mit Verzweigungen, Schleifen und mehreren Akteuren dar.\n",
    "\n",
    "### Hauptmerkmale von LangGraph:\n",
    "\n",
    "- **Zustandsverwaltung**: Explizite Verwaltung von Zuständen während des Ausführungsflusses\n",
    "- **Gerichtete Graphen**: Anwendungslogik wird als gerichteter Graph modelliert\n",
    "- **Verzweigungen und Konditionen**: Möglichkeit, basierend auf Bedingungen verschiedene Pfade einzuschlagen\n",
    "- **Typisierung**: Starke Typisierung für bessere Entwicklererfahrung und Fehlererkennung\n",
    "- **Persistenz**: Optional zustandsbehaftete Ausführung über mehrere Anfragen hinweg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LangChain vs. LangGraph\n",
    "\n",
    "### LangChain:\n",
    "- Fokus auf sequentielle Verarbeitung (Chains)\n",
    "- Komponentenorientiert (LLMs, Embeddings, Vektorspeicher, etc.)\n",
    "- Gut für einfache Pipelines wie RAG-Anwendungen\n",
    "\n",
    "### LangGraph:\n",
    "- Fokus auf komplexe zustandsbehaftete Workflows\n",
    "- Graphenbasierte Struktur mit bedingter Logik\n",
    "- Unterstützt Zyklen/Schleifen für iterative Prozesse\n",
    "- Ideal für Agenten, die mehrere Schritte und Entscheidungen erfordern\n",
    "\n",
    "**LangGraph ergänzt LangChain, ersetzt es aber nicht.** Beide Frameworks können und sollten zusammen verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Einfaches Beispiel: Ein simpler Chatbot mit LangGraph\n",
    "\n",
    "Beginnen wir mit einem einfachen Beispiel: Ein Chatbot, der auf Nachrichten reagiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfachen MessageGraph erstellen\n",
    "model = llm()\n",
    "\n",
    "# Graph bauen\n",
    "graph_builder = MessageGraph()\n",
    "\n",
    "# Wir definieren unser LLM als einzigen Knoten\n",
    "graph_builder.add_node(\"chatbot_node\", model)\n",
    "\n",
    "# Dieser Knoten wird der Entrypoint\n",
    "graph_builder.set_entry_point(\"chatbot_node\")\n",
    "\n",
    "# Von diesem Knoten geht es direkt zu \"END\", dem vordefinierten Terminalknoten\n",
    "graph_builder.set_finish_point(\"chatbot_node\")\n",
    "\n",
    "# Jetzt kompilieren wir\n",
    "simple_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen des Chatbots\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hallo! Wie geht es dir?\")\n",
    "]\n",
    "\n",
    "response = simple_graph.invoke(messages)\n",
    "print(response[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zustandsbehafteter Graph mit typisierten Zuständen\n",
    "\n",
    "Jetzt implementieren wir einen zustandsbehafteten Graphen mit expliziter Typisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren des Zustands mit Typisierung\n",
    "class ChatState(TypedDict):\n",
    "    \"\"\"Repräsentiert den Zustand unseres Chat-Agenten.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], operator.add]  # Liste von Nachrichten, die mit operator.add kombiniert werden können\n",
    "    count: int  # Zähler für die Anzahl der Interaktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren von zwei Knoten für unseren Graphen\n",
    "\n",
    "# 1. LLM Node - generiert Antworten\n",
    "def call_llm(state: ChatState) -> ChatState:\n",
    "    \"\"\"Ruft das LLM auf und gibt eine Antwort zurück.\"\"\"\n",
    "    # Fügt Systemanweisungen hinzu, wenn es die erste Nachricht ist\n",
    "    if state[\"count\"] == 0:\n",
    "        messages = [SystemMessage(content=\"Du bist ein hilfsbereicher KI-Assistent. Halte Antworten kurz und präzise.\")]\n",
    "        messages.extend(state[\"messages\"])\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    # LLM aufrufen\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Neuen Zustand zurückgeben\n",
    "    return {\"messages\": [response], \"count\": state[\"count\"] + 1}\n",
    "\n",
    "# 2. Counter Node - zählt die Anzahl der Nachrichten\n",
    "def check_count(state: ChatState) -> str:\n",
    "    \"\"\"Überprüft den Zähler und entscheidet, ob das Gespräch fortgesetzt werden soll.\"\"\"\n",
    "    if state[\"count\"] >= 5:\n",
    "        # Nach 5 Nachrichten beenden wir die Konversation\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph erstellen\n",
    "chat_graph_builder = StateGraph(ChatState)\n",
    "\n",
    "# Knoten hinzufügen\n",
    "chat_graph_builder.add_node(\"llm\", call_llm)\n",
    "\n",
    "# Startpunkt definieren\n",
    "chat_graph_builder.set_entry_point(\"llm\")\n",
    "\n",
    "# Bedingte Kanten hinzufügen\n",
    "chat_graph_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    check_count,\n",
    "    {\n",
    "        \"continue\": \"llm\",  # Schleife zurück zum LLM\n",
    "        \"end\": END  # Ende der Konversation\n",
    "    }\n",
    ")\n",
    "\n",
    "# Graph kompilieren\n",
    "chat_graph = chat_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen des zustandsbehafteten Chatbots\n",
    "initial_state = {\"messages\": [HumanMessage(content=\"Erzähle mir etwas über KI.\")], \"count\": 0}\n",
    "\n",
    "# Wir verwenden stream, um die Antworten live zu sehen\n",
    "for event in chat_graph.stream(initial_state):\n",
    "    if 'messages' in event:\n",
    "        for message in event['messages']:\n",
    "            if isinstance(message, AIMessage):\n",
    "                print(f\"Runde {event['count']-1}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent mit Tool-Verwendung\n",
    "\n",
    "Jetzt erstellen wir einen komplexeren Agenten, der Tools verwenden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# Tool 1: Aktuelle Zeit anzeigen\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Gibt die aktuelle Zeit und das aktuelle Datum zurück.\"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    return f\"Aktuelles Datum und Uhrzeit: {now.strftime('%d.%m.%Y %H:%M:%S')}\"\n",
    "\n",
    "# Tool 2: Mathematische Berechnungen durchführen\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Berechnet den Wert eines mathematischen Ausdrucks. \n",
    "    Unterstützt grundlegende Operationen wie +, -, *, /, sowie math-Funktionen.\n",
    "    \n",
    "    Args:\n",
    "        expression: Mathematischer Ausdruck als String, z.B. \"2 * 3 + 4\" oder \"math.sin(0.5)\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sicherer Namespace mit nur bestimmten math-Funktionen\n",
    "        safe_dict = {\n",
    "            'abs': abs, 'round': round,\n",
    "            'math': math\n",
    "        }\n",
    "        \n",
    "        result = eval(expression, {\"__builtins__\": {}}, safe_dict)\n",
    "        return f\"Das Ergebnis von {expression} ist {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Fehler bei der Berechnung: {str(e)}\"\n",
    "\n",
    "# Tool 3: Einfache Datensuche (simuliert)\n",
    "@tool\n",
    "def search_database(query: str) -> str:\n",
    "    \"\"\"Sucht in einer simulierten Datenbank nach Informationen.\n",
    "    \n",
    "    Args:\n",
    "        query: Suchbegriff oder -phrase\n",
    "    \"\"\"\n",
    "    # Simulierte Datenbankeinträge\n",
    "    database = {\n",
    "        \"python\": \"Python ist eine interpretierte Hochsprache, die einfach zu erlernen ist und vielseitig eingesetzt wird.\",\n",
    "        \"langchain\": \"LangChain ist ein Framework zur Entwicklung von Anwendungen mit Sprachmodellen.\",\n",
    "        \"langgraph\": \"LangGraph ist eine Erweiterung von LangChain für komplexe zustandsbehaftete Workflows.\",\n",
    "        \"llm\": \"LLM steht für Large Language Model, ein Sprachmodell, das auf großen Textmengen trainiert wurde.\"\n",
    "    }\n",
    "    \n",
    "    # Einfache Suche nach Schlüsselwörtern\n",
    "    query = query.lower()\n",
    "    for key, value in database.items():\n",
    "        if query in key or key in query:\n",
    "            return value\n",
    "    \n",
    "    return f\"Keine Informationen zu '{query}' gefunden.\"\n",
    "\n",
    "# Alle Tools in einer Liste sammeln\n",
    "tools = [get_current_time, calculate, search_database]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM mit Tool-Unterstützung konfigurieren\n",
    "tool_model = llm().bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Typ für unseren Agenten definieren\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State-Typ für unseren Tool-verwendenden Agenten.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # Weitere Felder können je nach Bedarf hinzugefügt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die beiden Hauptfunktionen für unseren Agenten definieren\n",
    "\n",
    "# 1. Funktion zum Aufrufen des LLM\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"Ruft das LLM auf und entscheidet, ob Tools verwendet werden sollen.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = tool_model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 2. Funktion zum Ausführen der Tools\n",
    "def call_tools(state: AgentState) -> AgentState:\n",
    "    \"\"\"Führt Tools basierend auf LLM-Anforderungen aus.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Überprüfen, ob Tool-Calls vorhanden sind\n",
    "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    # Tools-Dictionary zum schnellen Nachschlagen erstellen\n",
    "    tool_dict = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    # Neue Nachrichten für die Tool-Ergebnisse\n",
    "    new_messages = []\n",
    "    \n",
    "    # Jede Tool-Anfrage ausführen\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call.name\n",
    "        arguments = json.loads(tool_call.args)\n",
    "        \n",
    "        # Überprüfen, ob das Tool existiert\n",
    "        if tool_name in tool_dict:\n",
    "            tool_to_call = tool_dict[tool_name]\n",
    "            \n",
    "            # Tool mit Argumenten aufrufen\n",
    "            if arguments:\n",
    "                tool_result = tool_to_call(**arguments)\n",
    "            else:\n",
    "                tool_result = tool_to_call()\n",
    "                \n",
    "            # Nachrichten mit Tool-Ergebnis hinzufügen\n",
    "            new_messages.append(\n",
    "                AIMessage(\n",
    "                    content=\"\",\n",
    "                    tool_call_id=tool_call.id,\n",
    "                    name=tool_name,\n",
    "                    tool_calls=[],\n",
    "                    additional_kwargs={\"tool_responses\": [tool_result]}\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return {\"messages\": new_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfs- und Routing-Funktionen\n",
    "\n",
    "# Funktion zum Überprüfen, ob weitere Tool-Calls nötig sind\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Bestimmt, ob der Agent weiter Tools ausführen soll oder fertig ist.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Wenn das letzte Nachricht keine Tool-Calls hat und vom LLM kommt, beenden\n",
    "    if isinstance(last_message, AIMessage) and not hasattr(last_message, \"tool_calls\") or \\\n",
    "       (hasattr(last_message, \"tool_calls\") and not last_message.tool_calls):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Sonst: Weiter mit Tool-Ausführung oder Planung\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph aufbauen\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tools)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Bedingungen für Verzweigungen hinzufügen\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Graph kompilieren\n",
    "agent_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systemanweisung hinzufügen\n",
    "system_message = SystemMessage(\n",
    "    content=\"Du bist ein hilfreicher KI-Assistent. Du kannst verschiedene Tools verwenden, um Aufgaben zu erledigen. \"\n",
    "             \"Verwende die Tools, wenn es hilfreich ist, um die Anfrage des Benutzers zu beantworten.\"\n",
    ")\n",
    "\n",
    "# Testen des Agenten mit einer Frage, die Tool-Verwendung erfordert\n",
    "human_message = HumanMessage(\n",
    "    content=\"Kannst du mir sagen, wie spät es ist und dann die Quadratwurzel von 144 berechnen?\"\n",
    ")\n",
    "\n",
    "# Initial State mit System- und Human-Message\n",
    "initial_state = {\"messages\": [system_message, human_message]}\n",
    "\n",
    "# Agent aufrufen und Ergebnisse anzeigen\n",
    "result = agent_graph.invoke(initial_state)\n",
    "\n",
    "# Alle Nachrichten im Dialog ausgeben\n",
    "for message in initial_state[\"messages\"] + result[\"messages\"]:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        print(f\"System: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        if hasattr(message, \"tool_call_id\") and message.tool_call_id:\n",
    "            # Tool-Antworten anzeigen\n",
    "            tool_responses = message.additional_kwargs.get(\"tool_responses\", [])\n",
    "            if tool_responses:\n",
    "                print(f\"Tool ({message.name}): {tool_responses[0]}\")\n",
    "        elif hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "            # Tool-Anfragen anzeigen\n",
    "            print(f\"AI (thinking): Ich sollte Tools verwenden...\")\n",
    "            for tool_call in message.tool_calls:\n",
    "                args = json.loads(tool_call.args)\n",
    "                args_str = \", \".join([f\"{k}='{v}'\" for k, v in args.items()])\n",
    "                print(f\"AI (tool request): {tool_call.name}({args_str})\")\n",
    "        else:\n",
    "            # Normale Nachricht anzeigen\n",
    "            print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weitere Beispiele für Tool-Verwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weitere Anfragen an unseren Agenten stellen\n",
    "questions = [\n",
    "    \"Was ist LangChain?\",\n",
    "    \"Berechne den Sinus von 30 Grad.\",\n",
    "    \"Was ist LangGraph und wie unterscheidet es sich von LangChain?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n=== Neue Anfrage: {question} ===\")\n",
    "    human_message = HumanMessage(content=question)\n",
    "    initial_state = {\"messages\": [system_message, human_message]}\n",
    "    \n",
    "    result = agent_graph.invoke(initial_state)\n",
    "    \n",
    "    # Alle Nachrichten im Dialog ausgeben\n",
    "    for message in initial_state[\"messages\"] + result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"Human: {message.content}\")\n",
    "        elif isinstance(message, SystemMessage):\n",
    "            pass  # System-Nachrichten nicht jedes Mal anzeigen\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, \"tool_call_id\") and message.tool_call_id:\n",
    "                # Tool-Antworten anzeigen\n",
    "                tool_responses = message.additional_kwargs.get(\"tool_responses\", [])\n",
    "                if tool_responses:\n",
    "                    print(f\"Tool ({message.name}): {tool_responses[0]}\")\n",
    "            elif hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "                # Tool-Anfragen anzeigen\n",
    "                print(f\"AI (thinking): Ich sollte Tools verwenden...\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    args = json.loads(tool_call.args)\n",
    "                    args_str = \", \".join([f\"{k}='{v}'\" for k, v in args.items()])\n",
    "                    print(f\"AI (tool request): {tool_call.name}({args_str})\")\n",
    "            else:\n",
    "                # Normale Nachricht anzeigen\n",
    "                print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration von LangGraph in bestehende Systeme\n",
    "\n",
    "LangGraph kann sehr gut in bestehende Systeme integriert werden. Hier sind einige wichtige Aspekte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für persisted graph (Zustandspersistenz über mehrere Anfragen)\n",
    "from langgraph.graph.graph import MemorylessGraph\n",
    "import uuid\n",
    "\n",
    "# Wir erstellen den gleichen Graphen wie oben, aber für persistente Verwendung\n",
    "persisted_graph = agent_graph.with_config(\n",
    "    channel_factories={\"thread\": lambda: uuid.uuid4().hex}  # Thread-ID für persistenten Zustand\n",
    ")\n",
    "\n",
    "# Verwendung mit persist=True, um Zustand zu speichern\n",
    "thread_id = persisted_graph.get_channel(\"thread\")\n",
    "print(f\"Thread ID: {thread_id}\")\n",
    "\n",
    "# Erste Nachricht\n",
    "first_question = \"Welcher Tag ist heute und welches Jahr haben wir?\"\n",
    "initial_state = {\"messages\": [system_message, HumanMessage(content=first_question)]}\n",
    "response = persisted_graph.invoke(initial_state, config={\"configurable\": {\"thread\": thread_id}})\n",
    "\n",
    "print(\"=== Erste Anfrage ===\")\n",
    "for message in response[\"messages\"]:\n",
    "    if isinstance(message, AIMessage):\n",
    "        if hasattr(message, \"tool_call_id\") and message.tool_call_id:\n",
    "            tool_responses = message.additional_kwargs.get(\"tool_responses\", [])\n",
    "            if tool_responses:\n",
    "                print(f\"Tool ({message.name}): {tool_responses[0]}\")\n",
    "        elif hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "            pass  # Tool-Anfragen nicht anzeigen für Übersichtlichkeit\n",
    "        else:\n",
    "            print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zweite Nachricht mit Bezug zur ersten\n",
    "second_question = \"Wie viele Tage sind es bis zum Ende des Jahres?\"\n",
    "follow_up_state = {\"messages\": [HumanMessage(content=second_question)]}\n",
    "\n",
    "# Den gleichen thread_id verwenden, um den Zustand beizubehalten\n",
    "response = persisted_graph.invoke(follow_up_state, config={\"configurable\": {\"thread\": thread_id}})\n",
    "\n",
    "print(\"\\n=== Zweite Anfrage (mit Bezug zur ersten) ===\")\n",
    "for message in response[\"messages\"]:\n",
    "    if isinstance(message, AIMessage):\n",
    "        if hasattr(message, \"tool_call_id\") and message.tool_call_id:\n",
    "            tool_responses = message.additional_kwargs.get(\"tool_responses\", [])\n",
    "            if tool_responses:\n",
    "                print(f\"Tool ({message.name}): {tool_responses[0]}\")\n",
    "        elif hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "            pass  # Tool-Anfragen nicht anzeigen für Übersichtlichkeit\n",
    "        else:\n",
    "            print(f\"AI: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Praktische Anwendungsfälle für LangGraph\n",
    "\n",
    "LangGraph eignet sich besonders gut für:\n",
    "\n",
    "1. **Multi-Agent-Systeme**: Verschiedene Agenten arbeiten zusammen, jeder mit eigener Expertise\n",
    "2. **Komplexe Entscheidungsbäume**: Anwendungen, die mehrere Entscheidungen treffen müssen\n",
    "3. **Iterative Prozesse**: Aufgaben, die mehrere Zyklen von Planung, Ausführung und Bewertung erfordern\n",
    "4. **Geschäftsprozessautomatisierung**: Abbildung von realen Workflows mit Entscheidungspunkten\n",
    "5. **Konversationsagenten mit Gedächtnis**: Chatbots, die Kontext über mehrere Sitzungen beibehalten\n",
    "\n",
    "Praktische Beispiele:\n",
    "- Kundenservice-Agenten, die Anfragen kategorisieren und an spezialisierte Sub-Agenten weiterleiten\n",
    "- Forschungsassistenten, die iterativ Informationen sammeln, bewerten und strukturieren\n",
    "- Workflow-Automatisierung in Unternehmen mit mehreren Genehmigungsebenen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vor- und Nachteile von LangGraph\n",
    "\n",
    "### Vorteile:\n",
    "- Explizite Zustandsverwaltung für komplexe Workflows\n",
    "- Typsicherheit durch TypedDict und Annotationen\n",
    "- Flexible Verzweigungslogik für bedingte Ausführung\n",
    "- Unterstützung für zustandsbehaftete Anwendungen\n",
    "- Nahtlose Integration mit LangChain\n",
    "\n",
    "### Nachteile:\n",
    "- Steilere Lernkurve im Vergleich zu einfachen LangChain-Ketten\n",
    "- Mehr Boilerplate-Code für einfache Anwendungsfälle\n",
    "- Noch in aktiver Entwicklung, API kann sich ändern\n",
    "- Komplexere Fehlerbehebung bei größeren Graphen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Übung: Erweitere den Agenten\n",
    "\n",
    "Jetzt bist du an der Reihe! Erweitere den Agenten um mindestens eine der folgenden Funktionalitäten:\n",
    "\n",
    "1. Füge ein neues Tool hinzu (z.B. Wetter-API, Übersetzung, etc.)\n",
    "2. Implementiere eine bedingte Verzweigung basierend auf dem Nachrichteninhalt\n",
    "3. Füge einen Logging-Knoten hinzu, der Nachrichten protokolliert\n",
    "4. Implementiere ein \"Human-in-the-loop\"-Feedback-System für kritische Entscheidungen\n",
    "\n",
    "Hier ist ein Gerüst, das du als Ausgangspunkt verwenden kannst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dein Code hier\n",
    "\n",
    "# Beispiel für ein zusätzliches Tool:\n",
    "@tool\n",
    "def translate(text: str, target_language: str) -> str:\n",
    "    \"\"\"Übersetzt einen Text in die Zielsprache (simuliert).\n",
    "    \n",
    "    Args:\n",
    "        text: Der zu übersetzende Text\n",
    "        target_language: Die Zielsprache (z.B. 'englisch', 'französisch')\n",
    "    \"\"\"\n",
    "    # Hier würde man normalerweise eine echte Übersetzungs-API aufrufen\n",
    "    # Für Übungszwecke simulieren wir die Antwort\n",
    "    languages = {\n",
    "        \"englisch\": \"This is a simulated translation to English.\",\n",
    "        \"französisch\": \"C'est une traduction simulée en français.\",\n",
    "        \"spanisch\": \"Esta es una traducción simulada al español.\",\n",
    "        \"italienisch\": \"Questa è una traduzione simulata in italiano.\"\n",
    "    }\n",
    "    \n",
    "    target_language = target_language.lower()\n",
    "    if target_language in languages:\n",
    "        return f\"Übersetzung von '{text}': {languages[target_language]}\"\n",
    "    else:\n",
    "        return f\"Die Sprache '{target_language}' wird nicht unterstützt.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir die Grundlagen von LangGraph kennengelernt und wie es sich von LangChain unterscheidet. Wir haben:\n",
    "\n",
    "1. Die Architektur und Grundkonzepte von LangGraph kennengelernt\n",
    "2. Den Unterschied zwischen LangChain und LangGraph verstanden\n",
    "3. Einfache Chatbots mit MessageGraph erstellt\n",
    "4. Zustandsbehaftete Graphen mit TypedDict implementiert\n",
    "5. Komplexere Agenten mit Tool-Unterstützung entwickelt\n",
    "6. Persistente Zustandsverwaltung über mehrere Anfragen hinweg gesehen\n",
    "7. Praktische Anwendungsfälle für LangGraph besprochen\n",
    "\n",
    "LangGraph ist ein leistungsstarkes Framework für die Entwicklung komplexer KI-Anwendungen, die über einfache sequentielle Prozesse hinausgehen. Es ergänzt LangChain optimal und bietet die nötige Flexibilität für anspruchsvolle Anwendungsfälle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
