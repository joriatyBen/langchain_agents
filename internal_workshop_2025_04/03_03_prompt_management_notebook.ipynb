{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Management in LangChain\n",
    "\n",
    "In diesem Notebook lernen wir die wichtigsten Aspekte des Prompt-Managements kennen:\n",
    "- Organisation und Versionierung von Prompts\n",
    "- LangChain Hub für Prompt-Sharing\n",
    "- Integration mit Langfuse für Monitoring und Tracking\n",
    "- Best Practices für Prompt-Management in größeren Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung: Benötigte Bibliotheken installieren\n",
    "\n",
    "Falls noch nicht installiert, müssen wir die benötigten Bibliotheken installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der benötigten Pakete (bei Bedarf ausführen)\n",
    "# !pip install langchain langchain-openai langfuse langchain-langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "import os\n",
    "from datetime import datetime\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "import json\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "\n",
    "# Optional: Für Langfuse-Integration\n",
    "# from langfuse import Langfuse\n",
    "# from langfuse.client import StatelessTracer\n",
    "# from langchain_langfuse import LangfuseCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlagen: Organisiertes Prompt-Management\n",
    "\n",
    "Effektives Prompt-Management umfasst folgende Aspekte:\n",
    "- Strukturierte Speicherung von Prompts\n",
    "- Versionierung für Nachvollziehbarkeit\n",
    "- Wiederverwendbarkeit durch modulare Gestaltung\n",
    "- Testing und Evaluation\n",
    "\n",
    "Wir beginnen mit einfachen Methoden zur strukturierten Verwaltung von Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lokale Prompt-Verwaltung mit einfacher Versionierung\n",
    "class PromptManager:\n",
    "    def __init__(self, base_path=\"./prompts\"):\n",
    "        self.base_path = base_path\n",
    "        # Erstelle den Verzeichnispfad, falls er nicht existiert\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    def save_prompt(self, prompt, name, version=None, metadata=None):\n",
    "        # Generiere einen Zeitstempel als Version, falls keine angegeben wurde\n",
    "        if version is None:\n",
    "            version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Erstelle das Verzeichnis für diesen Prompt, falls es nicht existiert\n",
    "        prompt_dir = os.path.join(self.base_path, name)\n",
    "        os.makedirs(prompt_dir, exist_ok=True)\n",
    "\n",
    "        # Speichere den Prompt und Metadaten\n",
    "        prompt_data = {\n",
    "            \"content\": prompt.to_json() if hasattr(prompt, \"to_json\") else str(prompt),\n",
    "            \"version\": version,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "\n",
    "        # Speichere als JSON-Datei\n",
    "        file_path = os.path.join(prompt_dir, f\"{version}.json\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(prompt_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def list_versions(self, name):\n",
    "        prompt_dir = os.path.join(self.base_path, name)\n",
    "        if not os.path.exists(prompt_dir):\n",
    "            return []\n",
    "\n",
    "        # Liste alle JSON-Dateien im Verzeichnis\n",
    "        versions = [f.replace(\".json\", \"\") for f in os.listdir(prompt_dir) if f.endswith(\".json\")]\n",
    "        return sorted(versions)\n",
    "\n",
    "    def get_latest_version(self, name):\n",
    "        versions = self.list_versions(name)\n",
    "        if not versions:\n",
    "            return None\n",
    "        return versions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type SystemMessagePromptTemplate is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 12\u001B[0m\n\u001B[1;32m      5\u001B[0m kundenservice_prompt \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_messages([\n\u001B[1;32m      6\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDu bist ein hilfreicher Kundendienstmitarbeiter für \u001B[39m\u001B[38;5;132;01m{unternehmen}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDu sollst immer freundlich, professionell und lösungsorientiert antworten.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      8\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{kundenfrage}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m ])\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Prompt speichern mit Metadaten\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m prompt_path \u001B[38;5;241m=\u001B[39m \u001B[43mprompt_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkundenservice_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkundenservice_basic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mautor\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWorkshop-Team\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverwendung\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mKundenservice-Chatbot\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mempfohlenes_modell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrompt gespeichert unter: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVerfügbare Versionen: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt_manager\u001B[38;5;241m.\u001B[39mlist_versions(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkundenservice_basic\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 28\u001B[0m, in \u001B[0;36mPromptManager.save_prompt\u001B[0;34m(self, prompt, name, version, metadata)\u001B[0m\n\u001B[1;32m     26\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(prompt_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 28\u001B[0m     \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_ascii\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file_path\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/__init__.py:179\u001B[0m, in \u001B[0;36mdump\u001B[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[1;32m    173\u001B[0m     iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(skipkeys\u001B[38;5;241m=\u001B[39mskipkeys, ensure_ascii\u001B[38;5;241m=\u001B[39mensure_ascii,\n\u001B[1;32m    174\u001B[0m         check_circular\u001B[38;5;241m=\u001B[39mcheck_circular, allow_nan\u001B[38;5;241m=\u001B[39mallow_nan, indent\u001B[38;5;241m=\u001B[39mindent,\n\u001B[1;32m    175\u001B[0m         separators\u001B[38;5;241m=\u001B[39mseparators,\n\u001B[1;32m    176\u001B[0m         default\u001B[38;5;241m=\u001B[39mdefault, sort_keys\u001B[38;5;241m=\u001B[39msort_keys, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\u001B[38;5;241m.\u001B[39miterencode(obj)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# a debuggability cost\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m    180\u001B[0m     fp\u001B[38;5;241m.\u001B[39mwrite(chunk)\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:431\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_list(o, _current_indent_level)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(o, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 431\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_dict(o, _current_indent_level)\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:325\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_list\u001B[0;34m(lst, _current_indent_level)\u001B[0m\n\u001B[1;32m    323\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    324\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 325\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:438\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    436\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCircular reference detected\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    437\u001B[0m     markers[markerid] \u001B[38;5;241m=\u001B[39m o\n\u001B[0;32m--> 438\u001B[0m o \u001B[38;5;241m=\u001B[39m \u001B[43m_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m _iterencode(o, _current_indent_level)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/json/encoder.py:179\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    180\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Object of type SystemMessagePromptTemplate is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Beispiel: Erstellen und Speichern eines Prompts\n",
    "prompt_manager = PromptManager()\n",
    "\n",
    "# Einen einfachen Prompt erstellen\n",
    "kundenservice_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Kundendienstmitarbeiter für {unternehmen}. \"\n",
    "             \"Du sollst immer freundlich, professionell und lösungsorientiert antworten.\"),\n",
    "    (\"human\", \"{kundenfrage}\")\n",
    "])\n",
    "\n",
    "# Prompt speichern mit Metadaten\n",
    "prompt_path = prompt_manager.save_prompt(\n",
    "    prompt=kundenservice_prompt,\n",
    "    name=\"kundenservice_basic\",\n",
    "    metadata={\n",
    "        \"autor\": \"Workshop-Team\",\n",
    "        \"verwendung\": \"Kundenservice-Chatbot\",\n",
    "        \"empfohlenes_modell\": \"gpt-3.5-turbo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Prompt gespeichert unter: {prompt_path}\")\n",
    "print(f\"Verfügbare Versionen: {prompt_manager.list_versions('kundenservice_basic')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LangChain Hub für Prompt-Sharing\n",
    "\n",
    "Der LangChain Hub ist eine zentrale Plattform zur Verwaltung und gemeinsamen Nutzung von Prompts. Er bietet folgende Vorteile:\n",
    "- Zentrales Repository für Prompts\n",
    "- Versionskontrolle\n",
    "- Einfache Integration in LangChain-Anwendungen\n",
    "- Zusammenarbeit im Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.10/site-packages/langsmith/client.py:234: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladener Prompt aus dem Hub:\n",
      "input_variables=['client_letter', 'format_instructions'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'borislove', 'lc_hub_repo': 'customer-sentiment-analysis', 'lc_hub_commit_hash': '45eafd27466db2b27a448e29132a77a88a53092b7d4ad09838d13ef23908c36f'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"As a customer service representative, you receive the following email from a customer.\\nYour task is to identify the customer's sentiment and categorize it based on the scale below:\\n    0 - Calm: Customer asks questions but does not seem upset; is just seeking information.\\n    1 - Slightly Frustrated: Customer shows subtle signs of irritation but is still open to solutions.\\n    2 - Frustrated: Customer explicitly states being unhappy or irritated but is willing to discuss a solution.\\n    3 - Very Frustrated: Customer is clearly agitated, uses strong language, or mentions the problem repeatedly.\\n    4 - Extremely Frustrated: Customer is intensely unhappy, may raise their voice or use aggressive language.\\n    5 - Overwhelmed: Customer seems emotionally upset, says things like 'I can't take this anymore' or 'This is the worst experience ever.'\\nIf you cannot identify the sentiment for some reason, simply respond with 'Unknown'\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['client_letter', 'format_instructions'], input_types={}, partial_variables={}, template=\"Letter: \\n'''{client_letter}'''\\n\\n{format_instructions}\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Prompt aus dem LangChain Hub laden\n",
    "sentiment_prompt = hub.pull(\"borislove/customer-sentiment-analysis\")\n",
    "\n",
    "print(\"Geladener Prompt aus dem Hub:\")\n",
    "print(sentiment_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatierter Prompt:\n",
      "System: As a customer service representative, you receive the following email from a customer.\n",
      "Your task is to identify the customer's sentiment and categorize it based on the scale below:\n",
      "    0 - Calm: Customer asks questions but does not seem upset; is just seeking information.\n",
      "    1 - Slightly Frustrated: Customer shows subtle signs of irritation but is still open to solutions.\n",
      "    2 - Frustrated: Customer explicitly states being unhappy or irritated but is willing to discuss a solution.\n",
      "    3 - Very Frustrated: Customer is clearly agitated, uses strong language, or mentions the problem repeatedly.\n",
      "    4 - Extremely Frustrated: Customer is intensely unhappy, may raise their voice or use aggressive language.\n",
      "    5 - Overwhelmed: Customer seems emotionally upset, says things like 'I can't take this anymore' or 'This is the worst experience ever.'\n",
      "If you cannot identify the sentiment for some reason, simply respond with 'Unknown'\n",
      "Human: Letter: \n",
      "'''Ich bin von dem Produkt zutiefst enttäuscht. Nach nur zwei Wochen Nutzung ist die Verarbeitung bereits mangelhaft. Mehrere Knöpfe sind abgefallen und die Naht hat sich gelöst. Ich erwarte eine vollständige Rückerstattung und werde das Produkt definitiv nicht weiterempfehlen.'''\n",
      "\n",
      "Zusätzlich zur numerischen Klassifizierung sollst du eine kurze Begründung (max. 50 Wörter) für deine Einschätzung geben. Formatiere deine Antwort wie folgt:\n",
      "Sentiment: [positiv/negativ/neutral]\n",
      "Score: [Zahl zwischen 1-10]\n",
      "Begründung: [Deine Erklärung]\n"
     ]
    }
   ],
   "source": [
    "# Beispiel für die Verwendung des geladenen Prompts\n",
    "client_letter = \"\"\"Ich bin von dem Produkt zutiefst enttäuscht. Nach nur zwei Wochen Nutzung ist die Verarbeitung bereits mangelhaft. Mehrere Knöpfe sind abgefallen und die Naht hat sich gelöst. Ich erwarte eine vollständige Rückerstattung und werde das Produkt definitiv nicht weiterempfehlen.\"\"\"\n",
    "\n",
    "format_instructions = \"\"\"Zusätzlich zur numerischen Klassifizierung sollst du eine kurze Begründung (max. 50 Wörter) für deine Einschätzung geben. Formatiere deine Antwort wie folgt:\n",
    "Sentiment: [positiv/negativ/neutral]\n",
    "Score: [Zahl zwischen 1-10]\n",
    "Begründung: [Deine Erklärung]\"\"\"\n",
    "\n",
    "# Formatierter Prompt\n",
    "formatted_prompt = sentiment_prompt.format(\n",
    "    client_letter=client_letter,\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "print(\"\\nFormatierter Prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-Antwort:\n",
      "Sentiment: Negativ\n",
      "Score: 4\n",
      "Begründung: Der Kunde ist sehr frustriert und enttäuscht über die mangelhafte Qualität des Produkts nach nur kurzer Nutzungsdauer. Er fordert eine Rückerstattung und gibt an, das Produkt nicht weiterzuempfehlen. Die Verwendung von starken Worten wie \"zutiefst enttäuscht\" deutet auf eine hohe Frustrationsebene hin.\n"
     ]
    }
   ],
   "source": [
    "# Prompt ausführen mit LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "\n",
    "result = sentiment_chain.invoke({\n",
    "    \"client_letter\": client_letter,\n",
    "    \"format_instructions\": format_instructions\n",
    "})\n",
    "\n",
    "print(\"\\nLLM-Antwort:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenen Prompt im Hub veröffentlichen\n",
    "\n",
    "Um einen eigenen Prompt im LangChain Hub zu veröffentlichen, benötigen Sie einen API-Schlüssel. Hier ist der Prozess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatPromptTemplate' object has no attribute 'to_string'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 16\u001B[0m\n\u001B[1;32m      5\u001B[0m product_review_prompt \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_messages([\n\u001B[1;32m      6\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDu bist ein Experte für Produktbewertungen. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnalysiere die folgende Produktbewertung und extrahiere wichtige Informationen.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      8\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{review}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m ])\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# 2. Hochladen zum Hub (nur ausführen, wenn Sie einen API-Schlüssel haben)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# hub.push(\"mein_username/product-review-analysis\", product_review_prompt)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# print(\"Prompt erfolgreich auf den Hub hochgeladen!\")\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Alternativ: Anzeigen, wie der Prompt aussieht\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mproduct_review_prompt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m())\n",
      "File \u001B[0;32m/opt/tljh/user/lib/python3.10/site-packages/pydantic/main.py:856\u001B[0m, in \u001B[0;36mBaseModel.__getattr__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(item)  \u001B[38;5;66;03m# Raises AttributeError if appropriate\u001B[39;00m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# this is the current error\u001B[39;00m\n\u001B[0;32m--> 856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ChatPromptTemplate' object has no attribute 'to_string'"
     ]
    }
   ],
   "source": [
    "# Beispiel: Eigenen Prompt erstellen und auf den Hub hochladen\n",
    "# Hinweis: Erfordert einen API-Schlüssel von LangChain\n",
    "\n",
    "# 1. Erstellen eines eigenen Prompts\n",
    "product_review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Experte für Produktbewertungen. \"\n",
    "              \"Analysiere die folgende Produktbewertung und extrahiere wichtige Informationen.\"),\n",
    "    (\"human\", \"{review}\")\n",
    "])\n",
    "\n",
    "# 2. Hochladen zum Hub (nur ausführen, wenn Sie einen API-Schlüssel haben)\n",
    "# hub.push(\"mein_username/product-review-analysis\", product_review_prompt)\n",
    "# print(\"Prompt erfolgreich auf den Hub hochgeladen!\")\n",
    "\n",
    "# Alternativ: Anzeigen, wie der Prompt aussieht\n",
    "print(product_review_prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration mit Langfuse für Monitoring und Tracking\n",
    "\n",
    "Langfuse ist ein Observability-Tool für LLM-Anwendungen, das Monitoring, Tracing und Evaluation von Prompt-Ausführungen ermöglicht. Die Integration mit LangChain ermöglicht die Überwachung und Optimierung von Prompts im Produktionseinsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Langfuse Integration Demo ---\n",
      "1. Langfuse erstellt automatisch Traces für alle LLM-Aufrufe\n",
      "2. Jede Prompt-Ausführung wird mit Metadaten (Modell, Tokens, Latenz, Kosten) protokolliert\n",
      "3. Sie können Prompts basierend auf Performance-Metriken optimieren\n",
      "4. Feedback-Schleifen ermöglichen kontinuierliche Verbesserung\n"
     ]
    }
   ],
   "source": [
    "# Langfuse-Integration (Kommentiert, da API-Schlüssel erforderlich)\n",
    "\"\"\"\n",
    "# Langfuse initialisieren\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    ")\n",
    "\n",
    "# LangChain-Callback-Handler für Langfuse\n",
    "handler = LangfuseCallbackHandler(\n",
    "    public_key=os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.environ.get(\"LANGFUSE_SECRET_KEY\")\n",
    ")\n",
    "\n",
    "# LLM mit Langfuse-Integration\n",
    "llm_with_tracing = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    callbacks=[handler]\n",
    ")\n",
    "\n",
    "# Prompt mit Tracing ausführen\n",
    "chain_with_tracing = product_review_prompt | llm_with_tracing | StrOutputParser()\n",
    "result = chain_with_tracing.invoke({\"review\": \"Ein wirklich tolles Produkt! Sehr empfehlenswert.\"})\n",
    "\"\"\"\n",
    "\n",
    "# Stattdessen zeigen wir, wie Langfuse in einem realen Projekt funktionieren würde\n",
    "print(\"--- Langfuse Integration Demo ---\")\n",
    "print(\"1. Langfuse erstellt automatisch Traces für alle LLM-Aufrufe\")\n",
    "print(\"2. Jede Prompt-Ausführung wird mit Metadaten (Modell, Tokens, Latenz, Kosten) protokolliert\")\n",
    "print(\"3. Sie können Prompts basierend auf Performance-Metriken optimieren\")\n",
    "print(\"4. Feedback-Schleifen ermöglichen kontinuierliche Verbesserung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: Visualisierung von Langfuse-Daten\n",
    "\n",
    "In der Langfuse-Oberfläche können Sie folgende Informationen einsehen:\n",
    "\n",
    "- Vollständige Traces aller LLM-Interaktionen\n",
    "- Prompt-Performance (Latenz, Token-Nutzung, Kosten)\n",
    "- Modellvergleiche für verschiedene Prompts\n",
    "- Versionierungseffekte auf die Performance\n",
    "\n",
    "![Langfuse Dashboard Beispiel](https://docs.langfuse.com/img/observability/dashboard.png)\n",
    "\n",
    "*(Bild: Beispiel eines Langfuse Dashboards, Quelle: Langfuse Dokumentation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Practices für Prompt-Management in Teams\n",
    "\n",
    "### Strukturierung und Namenskonventionen\n",
    "\n",
    "Eine konsistente Struktur für Prompts erleichtert die Wartung und Entwicklung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatierter modularer Prompt:\n",
      "System: Du bist einr freundlicher und lösungsorientierter \n",
      "Kundendienstmitarbeiterr von TechSupport GmbH. \n",
      "Du hilfst Kunden bei Problemen mit Elektronikgeräten.\n",
      "Wichtige Unternehmensrichtlinien:\n",
      "- Rückgabe innerhalb von 14 Tagen ohne Angabe von Gründen möglich\n",
      "- Bei Defekten innerhalb der Garantiezeit erfolgt kostenloser Ersatz\n",
      "\n",
      "Human: Hier sind einige Beispiele für gute Antworten: [{'question': 'Mein Produkt funktioniert nicht mehr', 'answer': 'Es tut mir leid zu hören, dass Sie Probleme mit Ihrem Produkt haben. Können Sie mir bitte das genaue Modell und den Fehler beschreiben, damit ich Ihnen besser helfen kann?'}]\n",
      "Human: Wie kann ich mein defektes Smartphone zurücksenden?\n"
     ]
    }
   ],
   "source": [
    "# Beispiel für strukturierte Prompt-Entwicklung\n",
    "\n",
    "# 1. System-Rolle definieren (separat für bessere Wiederverwendbarkeit)\n",
    "customer_service_system = \"\"\"Du bist ein{gender} freundliche{gender} und lösungsorientierte{gender}\n",
    "Kundendienstmitarbeiter{gender} von {company_name}.\n",
    "Du hilfst Kunden bei Problemen mit {product_category}.\n",
    "Wichtige Unternehmensrichtlinien:\n",
    "- {policy1}\n",
    "- {policy2}\n",
    "\"\"\"\n",
    "\n",
    "# 2. Beispiele als Demonstration des gewünschten Verhaltens\n",
    "examples = [\n",
    "    {\"question\": \"Mein Produkt funktioniert nicht mehr\",\n",
    "     \"answer\": \"Es tut mir leid zu hören, dass Sie Probleme mit Ihrem Produkt haben. \"\n",
    "               \"Können Sie mir bitte das genaue Modell und den Fehler beschreiben, \"\n",
    "               \"damit ich Ihnen besser helfen kann?\"},\n",
    "    # Weitere Beispiele...\n",
    "]\n",
    "\n",
    "# 3. Modularer Prompt-Aufbau mit ChatPromptTemplate\n",
    "modular_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", customer_service_system),\n",
    "    (\"human\", \"Hier sind einige Beispiele für gute Antworten: {examples}\"),\n",
    "    (\"human\", \"{customer_query}\")\n",
    "])\n",
    "\n",
    "# 4. Verwendung mit konkreten Werten\n",
    "formatted_modular_prompt = modular_prompt.format(\n",
    "    gender=\"r\",  # für weibliche Form\n",
    "    company_name=\"TechSupport GmbH\",\n",
    "    product_category=\"Elektronikgeräten\",\n",
    "    policy1=\"Rückgabe innerhalb von 14 Tagen ohne Angabe von Gründen möglich\",\n",
    "    policy2=\"Bei Defekten innerhalb der Garantiezeit erfolgt kostenloser Ersatz\",\n",
    "    examples=str(examples),  # Vereinfachte Darstellung für das Beispiel\n",
    "    customer_query=\"Wie kann ich mein defektes Smartphone zurücksenden?\"\n",
    ")\n",
    "\n",
    "print(\"Formatierter modularer Prompt:\")\n",
    "print(formatted_modular_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versionierungs-Workflow in Teams\n",
    "\n",
    "Ein effektiver Workflow für Prompt-Entwicklung im Team umfasst diese Schritte:\n",
    "\n",
    "1. **Entwicklung**: Lokale Iteration und Testing\n",
    "2. **Review**: Peer-Review von Prompt-Änderungen\n",
    "3. **Testing**: Automatisierte Tests für konsistente Ergebnisse\n",
    "4. **Staging**: Testen in einer produktionsähnlichen Umgebung\n",
    "5. **Produktion**: Deployment mit Monitoring\n",
    "6. **Evaluation**: Kontinuierliche Bewertung und Feedback-Sammlung\n",
    "\n",
    "Hier ist ein Beispiel für ein einfaches Testframework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfaches Test-Framework für Prompts\n",
    "\n",
    "def test_prompt(prompt_template, test_cases, llm=None):\n",
    "    \"\"\"Test eines Prompts mit mehreren Testfällen\"\"\"\n",
    "    if llm is None:\n",
    "        llm = ChatOpenAI(temperature=0)  # Deterministisches Verhalten für Tests\n",
    "\n",
    "    results = []\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        try:\n",
    "            # Prompt mit Testdaten ausführen\n",
    "            response = chain.invoke(test_case[\"input\"])\n",
    "\n",
    "            # Einfache Validierung (kann mit spezifischeren Prüfungen erweitert werden)\n",
    "            validation = {\n",
    "                \"passed\": True,\n",
    "                \"notes\": \"Test bestanden\"\n",
    "            }\n",
    "\n",
    "            # Wenn expected_contains definiert ist, prüfen ob es in der Antwort enthalten ist\n",
    "            if \"expected_contains\" in test_case:\n",
    "                for expected in test_case[\"expected_contains\"]:\n",
    "                    if expected.lower() not in response.lower():\n",
    "                        validation[\"passed\"] = False\n",
    "                        validation[\"notes\"] = f\"Erwarteter Text nicht gefunden: {expected}\"\n",
    "\n",
    "            # Wenn expected_format definiert ist (z.B. JSON), Format überprüfen\n",
    "            if \"expected_format\" in test_case and test_case[\"expected_format\"] == \"json\":\n",
    "                try:\n",
    "                    json.loads(response)\n",
    "                except json.JSONDecodeError:\n",
    "                    validation[\"passed\"] = False\n",
    "                    validation[\"notes\"] = \"Antwort ist kein gültiges JSON\"\n",
    "\n",
    "            results.append({\n",
    "                \"test_case\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"response\": response,\n",
    "                \"validation\": validation\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"test_case\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"error\": str(e),\n",
    "                \"validation\": {\"passed\": False, \"notes\": f\"Fehler bei der Ausführung: {e}\"}\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testfall 1:\n",
      "Status: ✅ Bestanden\n",
      "Notizen: Test bestanden\n",
      "Antwort: Tauche ein in deine Lieblingsmusik mit den Wireless Kopfhörern XZ-500! Dank Bluetooth 5.0 Technologie genießt du kabellose Freiheit und mit einer Akku...\n",
      "\n",
      "Testfall 2:\n",
      "Status: ✅ Bestanden\n",
      "Notizen: Test bestanden\n",
      "Antwort: Entdecke die ultimative Ergonomie und Leistung mit der ergonomischen Bürotastatur K-2000! Mit mechanischen Switches für ein präzises Tippgefühl, RGB-B...\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Prompt-Tests durchführen\n",
    "\n",
    "# Einfacher Testprompt für Produktbeschreibungen\n",
    "product_desc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Produktbeschreibungs-Generator für einen Online-Shop. \"\n",
    "              \"Erstelle eine kurze, überzeugende Beschreibung für das folgende Produkt.\"),\n",
    "    (\"human\", \"Produkt: {product_name}\\nKategorie: {category}\\nHauptmerkmale: {features}\")\n",
    "])\n",
    "\n",
    "# Testfälle definieren\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"product_name\": \"Wireless Kopfhörer XZ-500\",\n",
    "            \"category\": \"Audio\",\n",
    "            \"features\": \"Bluetooth 5.0, 30h Akkulaufzeit, Noise-Cancelling\"\n",
    "        },\n",
    "        \"expected_contains\": [\"Kopfhörer\", \"Akku\", \"Noise\"]\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"product_name\": \"Ergonomische Bürotastatur K-2000\",\n",
    "            \"category\": \"Computer-Zubehör\",\n",
    "            \"features\": \"Mechanische Switches, RGB-Beleuchtung, Handballenauflage\"\n",
    "        },\n",
    "        \"expected_contains\": [\"Tastatur\", \"ergonomisch\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tests ausführen\n",
    "test_results = test_prompt(product_desc_prompt, test_cases)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for result in test_results:\n",
    "    print(f\"\\nTestfall {result['test_case']}:\")\n",
    "    print(f\"Status: {'✅ Bestanden' if result['validation']['passed'] else '❌ Fehlgeschlagen'}\")\n",
    "    print(f\"Notizen: {result['validation']['notes']}\")\n",
    "    print(f\"Antwort: {result['response'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zusammenfassung: Best Practices für Prompt-Management\n",
    "\n",
    "1. **Strukturierung und Modularität**\n",
    "   - Prompts in wiederverwendbare Komponenten aufteilen\n",
    "   - Klare Namenskonventionen verwenden\n",
    "   - Metadaten für Kontext und Nutzung hinzufügen\n",
    "\n",
    "2. **Versionierung**\n",
    "   - Jede Version eindeutig identifizierbar machen\n",
    "   - Änderungshistorie dokumentieren\n",
    "   - A/B-Testing für neue Versionen durchführen\n",
    "\n",
    "3. **Zusammenarbeit**\n",
    "   - Zentrale Repositories wie LangChain Hub nutzen\n",
    "   - Review-Prozesse implementieren\n",
    "   - Dokumentation für jede Prompt-Familie führen\n",
    "\n",
    "4. **Testing und Evaluation**\n",
    "   - Automatisierte Tests mit erwarteten Ergebnissen\n",
    "   - Unterschiedliche Eingabeszenarien abdecken\n",
    "   - Edge Cases berücksichtigen\n",
    "\n",
    "5. **Monitoring und Optimierung**\n",
    "   - Tools wie Langfuse für Observability einsetzen\n",
    "   - Performance-Metriken verfolgen (Latenz, Token, Kosten)\n",
    "   - Nutzer-Feedback systematisch erfassen und einbeziehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übungsaufgaben\n",
    "\n",
    "1. Erstellen Sie einen parametrisierten Prompt für einen bestimmten Anwendungsfall (z.B. Produkt-Rezensionen, Support-Anfragen)\n",
    "2. Implementieren Sie eine lokale Prompt-Versionierung mit dem PromptManager\n",
    "3. Testen Sie Ihren Prompt mit verschiedenen Eingabeszenarien\n",
    "4. Erstellen Sie eine verbesserte Version Ihres Prompts und vergleichen Sie die Ergebnisse\n",
    "5. (Optional) Veröffentlichen Sie Ihren Prompt im LangChain Hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
