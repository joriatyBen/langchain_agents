{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP)\n",
    "\n",
    "In diesem Notebook werden wir verschiedene Techniken zum Kontextmanagement in LLMs erkunden. Wir werden verschiedene Memory-Typen in LangChain implementieren und vergleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlagen des Kontextmanagements\n",
    "\n",
    "Beim Arbeiten mit LLMs ist das Kontextmanagement entscheidend, da alle Modelle ein begrenztes Kontextfenster haben. Dieses Fenster begrenzt, wie viel Information das Modell bei einer Anfrage berücksichtigen kann.\n",
    "\n",
    "Hier einige wichtige Konzepte:\n",
    "- **Kontextfenster**: Die maximale Anzahl an Tokens, die ein Modell verarbeiten kann\n",
    "- **Token**: Eine Grundeinheit der Textverarbeitung (etwa 4 Zeichen im Englischen)\n",
    "- **Memory**: Mechanismus zur Speicherung relevanter Teile einer Konversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken importieren\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# LangChain Importe\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Memory-Typen\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "load_dotenv()\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kontextfenster verschiedener Modelle\n",
    "\n",
    "Verschiedene LLMs haben unterschiedlich große Kontextfenster:\n",
    "\n",
    "| Modell            | Kontextfenster | Besonderheiten                    |\n",
    "|-------------------|----------------|-----------------------------------|\n",
    "| GPT-3.5 Turbo     | 16K Tokens     | Kostengünstig                     |\n",
    "| GPT-4 Turbo       | 128K Tokens    | Teurer, aber umfangreicherer Kontext |\n",
    "| Claude 3 Opus     | 200K Tokens    | Sehr großes Kontextfenster        |\n",
    "| Gemini Pro        | 32K Tokens     | Gutes Preis-Leistungs-Verhältnis  |\n",
    "| Mistral Large     | 32K Tokens     | Open-Source Alternative           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token-Zählung verstehen\n",
    "\n",
    "Lassen Sie uns untersuchen, wie Texte in Tokens umgewandelt werden. Dies ist wichtig, um das Kontextfenster effizient zu nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Zählt die Anzahl der Tokens in einem Text für ein bestimmtes Modell.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def show_token_breakdown(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Zeigt die einzelnen Tokens eines Textes an.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    decoded_tokens = [encoding.decode_single_token_bytes(token).decode(\"utf-8\", errors=\"replace\") for token in tokens]\n",
    "    return tokens, decoded_tokens\n",
    "\n",
    "# Beispieltexte\n",
    "text1 = \"KI ist eine tolle Sache.\"\n",
    "text2 = \"Künstliche Intelligenz revolutioniert viele Bereiche unseres Lebens.\"\n",
    "\n",
    "# Token-Anzahl ausgeben\n",
    "print(f\"Text 1: '{text1}'\")\n",
    "print(f\"Anzahl Tokens: {count_tokens(text1)}\")\n",
    "print(f\"Text 2: '{text2}'\")\n",
    "print(f\"Anzahl Tokens: {count_tokens(text2)}\")\n",
    "\n",
    "# Token-Breakdown für ersten Text\n",
    "tokens, decoded = show_token_breakdown(text1)\n",
    "print(\"\\nToken-Breakdown für Text 1:\")\n",
    "for i, (token, decoded_token) in enumerate(zip(tokens, decoded)):\n",
    "    print(f\"Token {i+1}: {token} -> '{decoded_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Typen in LangChain\n",
    "\n",
    "LangChain bietet verschiedene Memory-Typen, die für unterschiedliche Anwendungsfälle optimiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Buffer Memory\n",
    "\n",
    "Der einfachste Typ - speichert alle Nachrichten in voller Länge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "# Konversation hinzufügen\n",
    "buffer_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "buffer_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "buffer_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "\n",
    "# Abrufen des gesamten Kontexts\n",
    "context = buffer_memory.load_memory_variables({})\n",
    "print(context[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Summary Memory\n",
    "\n",
    "Erstellt eine fortlaufende Zusammenfassung der Konversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Konversation hinzufügen\n",
    "summary_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "summary_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "summary_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "summary_memory.save_context({\"input\": \"Ich interessiere mich für künstliche Intelligenz.\"}, \n",
    "                          {\"output\": \"Das ist ein spannendes Thema! KI umfasst viele Bereiche wie Machine Learning, neuronale Netze und natürliche Sprachverarbeitung.\"})\n",
    "\n",
    "# Zusammenfassung abrufen\n",
    "summary = summary_memory.load_memory_variables({})\n",
    "print(\"Zusammenfassung der Konversation:\")\n",
    "print(summary[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Window Memory\n",
    "\n",
    "Behält nur die letzten n Nachrichten bei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=2)  # Nur die letzten 2 Nachrichten behalten\n",
    "\n",
    "# Konversation hinzufügen\n",
    "window_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "window_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "window_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "\n",
    "# Nur die letzten 2 Nachrichten abrufen\n",
    "window_context = window_memory.load_memory_variables({})\n",
    "print(\"Window Memory (letzte 2 Nachrichten):\")\n",
    "print(window_context[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Token Buffer Memory\n",
    "\n",
    "Begrenzt die Anzahl der gespeicherten Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "# Längere Konversation hinzufügen\n",
    "token_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "token_memory.save_context({\"input\": \"Erzähl mir etwas über künstliche Intelligenz\"}, \n",
    "                        {\"output\": \"Künstliche Intelligenz (KI) ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens befasst. KI-Systeme können lernen, Probleme lösen und Entscheidungen treffen.\"})\n",
    "token_memory.save_context({\"input\": \"Was sind Anwendungsbeispiele?\"}, \n",
    "                        {\"output\": \"Anwendungsbeispiele für KI sind Spracherkennung, autonomes Fahren, medizinische Diagnose, Bilderkennung und natürlich Sprachmodelle wie ich.\"})\n",
    "\n",
    "# Token-begrenzte Geschichte abrufen\n",
    "token_context = token_memory.load_memory_variables({})\n",
    "print(\"Token Buffer Memory (max. 100 Tokens):\")\n",
    "print(token_context[\"history\"])\n",
    "\n",
    "# Anzahl der Tokens in der Geschichte\n",
    "print(f\"\\nAnzahl Tokens im Memory: {count_tokens(token_context['history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Vector Store Memory\n",
    "\n",
    "Speichert die Konversation in einer Vektordatenbank und ruft die relevantesten Nachrichten basierend auf der aktuellen Anfrage ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfache Vektordatenbank erstellen\n",
    "vectorstore = Chroma(embedding_function=embeddings, collection_name=\"memory_store\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Vector Memory initialisieren\n",
    "vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Verschiedene Informationen hinzufügen\n",
    "vector_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "vector_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "vector_memory.save_context({\"input\": \"Meine Lieblingsfarbe ist blau\"}, {\"output\": \"Blau ist eine schöne Farbe.\"})\n",
    "vector_memory.save_context({\"input\": \"Ich mag Pizza und Pasta\"}, {\"output\": \"Italienisches Essen ist sehr beliebt.\"})\n",
    "vector_memory.save_context({\"input\": \"Ich arbeite als Softwareentwickler\"}, {\"output\": \"Softwareentwicklung ist ein spannendes Berufsfeld.\"})\n",
    "\n",
    "# Relevanten Kontext für eine Anfrage abrufen\n",
    "query_info = vector_memory.load_memory_variables({\"prompt\": \"Was war noch mal mein Name und woher komme ich?\"})\n",
    "print(\"Vector Memory (relevante Nachrichten basierend auf der Abfrage):\")\n",
    "print(query_info[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory-Typen in der Praxis vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation_chain(memory, name):\n",
    "    \"\"\"Erstellt eine Konversationskette mit dem angegebenen Memory-Typ.\"\"\"\n",
    "    return ConversationChain(\n",
    "        llm=llm, \n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "# Verschiedene Memory-Typen erstellen\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# Konversationsketten erstellen\n",
    "buffer_chain = create_conversation_chain(buffer_memory, \"Buffer Memory\")\n",
    "summary_chain = create_conversation_chain(summary_memory, \"Summary Memory\")\n",
    "window_chain = create_conversation_chain(window_memory, \"Window Memory\")\n",
    "\n",
    "# Funktion zum Ausführen und Messen einer Konversation\n",
    "def run_conversation(chain, name, messages):\n",
    "    \"\"\"Führt eine Konversation mit der angegebenen Kette durch und misst die Antwortzeit.\"\"\"\n",
    "    print(f\"\\n=== {name} ===\\n\")\n",
    "    responses = []\n",
    "    times = []\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"Nachricht {i+1}: {message}\")\n",
    "        start_time = time.time()\n",
    "        response = chain.predict(input=message)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        \n",
    "        responses.append(response)\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        print(f\"Antwort: {response}\")\n",
    "        print(f\"Zeit: {elapsed:.2f} Sekunden\\n\")\n",
    "    \n",
    "    return responses, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testreihe mit verschiedenen Fragen\n",
    "test_messages = [\n",
    "    \"Mein Name ist Maria und ich komme aus München.\",\n",
    "    \"Ich bin 35 Jahre alt und arbeite als Ärztin.\",\n",
    "    \"In meiner Freizeit spiele ich gerne Klavier.\",\n",
    "    \"Ich habe zwei Kinder, Thomas und Lisa.\",\n",
    "    \"Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\"\n",
    "]\n",
    "\n",
    "# Konversation mit verschiedenen Memory-Typen durchführen\n",
    "buffer_responses, buffer_times = run_conversation(buffer_chain, \"Buffer Memory\", test_messages)\n",
    "summary_responses, summary_times = run_conversation(summary_chain, \"Summary Memory\", test_messages)\n",
    "window_responses, window_times = run_conversation(window_chain, \"Window Memory\", test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vergleich der Antwortqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich der letzten Antworten\n",
    "print(\"Vergleich der letzten Antworten (Frage: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?)\")\n",
    "print(\"\\nBuffer Memory Antwort:\")\n",
    "print(buffer_responses[-1])\n",
    "print(\"\\nSummary Memory Antwort:\")\n",
    "print(summary_responses[-1])\n",
    "print(\"\\nWindow Memory Antwort:\")\n",
    "print(window_responses[-1])\n",
    "\n",
    "# Vergleich der Antwortzeiten\n",
    "print(\"\\nDurchschnittliche Antwortzeiten:\")\n",
    "print(f\"Buffer Memory: {sum(buffer_times)/len(buffer_times):.2f} Sekunden\")\n",
    "print(f\"Summary Memory: {sum(summary_times)/len(summary_times):.2f} Sekunden\")\n",
    "print(f\"Window Memory: {sum(window_times)/len(window_times):.2f} Sekunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manuelle Implementation eines hierarchischen Gedächtnisses\n",
    "\n",
    "Bei komplexen Anwendungen kann ein hierarchischer Ansatz sinnvoll sein, der verschiedene Memory-Typen kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalMemory:\n",
    "    def __init__(self, llm):\n",
    "        # Kurzzeit-Gedächtnis: die letzten n Nachrichten (Window Memory)\n",
    "        self.short_term = ConversationBufferWindowMemory(k=3)\n",
    "        \n",
    "        # Mittelfristiges Gedächtnis: Zusammenfassung bisheriger Konversation\n",
    "        self.medium_term = ConversationSummaryMemory(llm=llm)\n",
    "        \n",
    "        # Langzeit-Gedächtnis: Wichtige Fakten in strukturierter Form\n",
    "        self.long_term_facts = {}\n",
    "        \n",
    "        self.llm = llm\n",
    "    \n",
    "    def save_context(self, input_text, output_text):\n",
    "        # Speichern im Kurzzeit- und mittelfristigen Gedächtnis\n",
    "        self.short_term.save_context({\"input\": input_text}, {\"output\": output_text})\n",
    "        self.medium_term.save_context({\"input\": input_text}, {\"output\": output_text})\n",
    "        \n",
    "        # Wichtige Fakten extrahieren und im Langzeitgedächtnis speichern\n",
    "        self.extract_facts(input_text)\n",
    "    \n",
    "    def extract_facts(self, input_text):\n",
    "        # Einfache Faktenextraktion durch Schlüsselwörter\n",
    "        if \"mein Name ist\" in input_text.lower():\n",
    "            parts = input_text.split(\"mein Name ist\", 1)\n",
    "            if len(parts) > 1:\n",
    "                name = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"name\"] = name\n",
    "        \n",
    "        if \"ich komme aus\" in input_text.lower():\n",
    "            parts = input_text.split(\"ich komme aus\", 1)\n",
    "            if len(parts) > 1:\n",
    "                location = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"location\"] = location\n",
    "        \n",
    "        if \"ich arbeite als\" in input_text.lower():\n",
    "            parts = input_text.split(\"ich arbeite als\", 1)\n",
    "            if len(parts) > 1:\n",
    "                job = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"job\"] = job\n",
    "    \n",
    "    def get_relevant_context(self, query):\n",
    "        # Kontext aus verschiedenen Quellen zusammenstellen\n",
    "        context = \"\"\n",
    "        \n",
    "        # 1. Wichtige Fakten aus dem Langzeitgedächtnis\n",
    "        if self.long_term_facts:\n",
    "            context += \"Wichtige Fakten:\\n\"\n",
    "            for key, value in self.long_term_facts.items():\n",
    "                context += f\"- {key.capitalize()}: {value}\\n\"\n",
    "        \n",
    "        # 2. Zusammenfassung der bisherigen Konversation\n",
    "        medium_context = self.medium_term.load_memory_variables({})\n",
    "        if medium_context[\"history\"]:\n",
    "            context += \"\\nZusammenfassung der Konversation:\\n\"\n",
    "            context += medium_context[\"history\"]\n",
    "        \n",
    "        # 3. Letzte Nachrichten aus dem Kurzzeitgedächtnis\n",
    "        short_context = self.short_term.load_memory_variables({})\n",
    "        if short_context[\"history\"]:\n",
    "            context += \"\\nLetzte Nachrichten:\\n\"\n",
    "            context += short_context[\"history\"]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchisches Gedächtnis testen\n",
    "hierarchical_memory = HierarchicalMemory(llm)\n",
    "\n",
    "# Konversation simulieren\n",
    "conversation = [\n",
    "    (\"Mein Name ist Klaus Schmidt.\", \"Hallo Klaus, schön dich kennenzulernen!\"),\n",
    "    (\"Ich komme aus Hamburg.\", \"Hamburg ist eine schöne Stadt am Wasser.\"),\n",
    "    (\"Ich arbeite als Softwareentwickler.\", \"Softwareentwicklung ist ein spannender Beruf mit vielen Möglichkeiten.\"),\n",
    "    (\"Ich interessiere mich für künstliche Intelligenz.\", \"Das Gebiet der KI entwickelt sich rasant und bietet viele interessante Anwendungsmöglichkeiten.\"),\n",
    "    (\"Ich habe letztes Jahr ein Projekt mit Python umgesetzt.\", \"Python ist eine großartige Sprache für KI-Projekte und viele andere Anwendungen.\"),\n",
    "]\n",
    "\n",
    "for input_text, output_text in conversation:\n",
    "    hierarchical_memory.save_context(input_text, output_text)\n",
    "    print(f\"User: {input_text}\")\n",
    "    print(f\"AI: {output_text}\\n\")\n",
    "\n",
    "# Relevanten Kontext für eine Anfrage abrufen\n",
    "query = \"Wie heißt du und was machst du beruflich?\"\n",
    "context = hierarchical_memory.get_relevant_context(query)\n",
    "print(\"=== Hierarchisches Gedächtnis ===\\n\")\n",
    "print(context)\n",
    "\n",
    "# Langzeitgedächtnis überprüfen\n",
    "print(\"\\n=== Langzeitgedächtnis (extrahierte Fakten) ===\\n\")\n",
    "for key, value in hierarchical_memory.long_term_facts.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Praktische Anwendung: Chat mit effizientem Kontextmanagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_chat():\n",
    "    # Für einen effizienten Chat kombinieren wir verschiedene Memory-Typen\n",
    "    \n",
    "    # System-Prompt definieren\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent. Verwende die folgenden Informationen, um Fragen zu beantworten:\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    # Hierarchisches Gedächtnis für effizienten Kontext\n",
    "    memory = HierarchicalMemory(llm)\n",
    "    \n",
    "    # Chat-Funktion erstellen\n",
    "    def chat(message):\n",
    "        context = memory.get_relevant_context(message)\n",
    "        \n",
    "        # Konversationshistorie aus dem Kurzzeitgedächtnis laden\n",
    "        history = memory.short_term.load_memory_variables({})[\"history\"]\n",
    "        \n",
    "        # LLM-Anfrage stellen\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"Du bist ein hilfreicher Assistent. Verwende die folgenden Informationen, um Fragen zu beantworten:\\n{context}\"),\n",
    "            HumanMessage(content=message)\n",
    "        ]\n",
    "        response = llm.invoke(messages).content\n",
    "        \n",
    "        # Kontext aktualisieren\n",
    "        memory.save_context(message, response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    return chat\n",
    "\n",
    "# Chat-Funktion erstellen\n",
    "chat = create_optimized_chat()\n",
    "\n",
    "# Beispiel-Konversation durchführen\n",
    "messages = [\n",
    "    \"Hallo! Ich bin Julia und komme aus München.\",\n",
    "    \"Ich bin 32 Jahre alt und arbeite als Grafikdesignerin.\",\n",
    "    \"Ich habe einen Hund namens Bello und eine Katze namens Mimi.\",\n",
    "    \"In meiner Freizeit gehe ich gerne wandern und fotografieren.\",\n",
    "    \"Kannst du mir zusammenfassen, was du über mich weißt?\"\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    response = chat(message)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices für Kontextmanagement\n",
    "\n",
    "1. **Relevanz priorisieren**: Nicht jede Information ist gleich wichtig. Priorisieren Sie kritische Informationen wie Nutzerdaten, Präferenzen und aktuelle Anfragen.\n",
    "\n",
    "2. **Hierarchischen Ansatz verwenden**: Kombinieren Sie verschiedene Memory-Typen für verschiedene Informationsebenen:\n",
    "   - **Kurzzeitgedächtnis**: Aktuelle Konversation (letzte 3-5 Nachrichten)\n",
    "   - **Mittelfristiges Gedächtnis**: Zusammenfassung der bisherigen Konversation\n",
    "   - **Langzeitgedächtnis**: Wichtige Fakten und Präferenzen\n",
    "\n",
    "3. **Kontext regelmäßig komprimieren**: Verwenden Sie Summary Memory oder LLM-basierte Zusammenfassungen, um lange Konversationen zu komprimieren.\n",
    "\n",
    "4. **Dynamische Kontexterweiterung**: Laden Sie nur dann externe Informationen, wenn sie für die aktuelle Anfrage relevant sind.\n",
    "\n",
    "5. **Token-Überwachung**: Überwachen Sie die Anzahl der verwendeten Tokens, um Kosten zu kontrollieren und das Kontextfenster optimal zu nutzen.\n",
    "\n",
    "6. **Metadaten nutzen**: Speichern Sie Zeitstempel und Wichtigkeitsbewertungen für jede Information, um später besser filtern zu können.\n",
    "\n",
    "7. **Vergessen implementieren**: Entfernen Sie weniger relevante oder veraltete Informationen aus dem Kontext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fazit\n",
    "\n",
    "Effektives Kontextmanagement ist entscheidend für die Leistung von LLM-Anwendungen. Die richtige Strategie hängt vom konkreten Anwendungsfall ab:\n",
    "\n",
    "- **Buffer Memory**: Gut für kurze Konversationen mit vollständigem Kontext\n",
    "- **Summary Memory**: Sinnvoll für längere Konversationen, bei denen Details weniger wichtig sind\n",
    "- **Window Memory**: Effizient für aufgabenorientierte Dialoge, wo nur der aktuelle Kontext wichtig ist\n",
    "- **Vector Memory**: Ideal für sehr lange Konversationen oder große Wissensdatenbanken\n",
    "- **Hierarchischer Ansatz**: Die beste Lösung für komplexe Anwendungen mit unterschiedlichen Kontextanforderungen\n",
    "\n",
    "Durch die richtige Kombination dieser Techniken können Sie die Effektivität Ihrer LLM-basierten Anwendungen maximieren und gleichzeitig Kosten und Latenz minimieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
