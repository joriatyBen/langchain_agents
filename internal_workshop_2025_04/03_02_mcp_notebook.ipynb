{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP)\n",
    "\n",
    "In diesem Notebook werden wir verschiedene Techniken zum Kontextmanagement in LLMs erkunden. Wir werden verschiedene Memory-Typen in LangChain implementieren und vergleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlagen des Kontextmanagements\n",
    "\n",
    "Beim Arbeiten mit LLMs ist das Kontextmanagement entscheidend, da alle Modelle ein begrenztes Kontextfenster haben. Dieses Fenster begrenzt, wie viel Information das Modell bei einer Anfrage berücksichtigen kann.\n",
    "\n",
    "Hier einige wichtige Konzepte:\n",
    "- **Kontextfenster**: Die maximale Anzahl an Tokens, die ein Modell verarbeiten kann\n",
    "- **Token**: Eine Grundeinheit der Textverarbeitung (etwa 4 Zeichen im Englischen)\n",
    "- **Memory**: Mechanismus zur Speicherung relevanter Teile einer Konversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken importieren\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# LangChain Importe\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Memory-Typen\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kontextfenster verschiedener Modelle\n",
    "\n",
    "Verschiedene LLMs haben unterschiedlich große Kontextfenster:\n",
    "\n",
    "| Modell            | Kontextfenster | Besonderheiten                    |\n",
    "|-------------------|----------------|-----------------------------------|\n",
    "| GPT-3.5 Turbo     | 16K Tokens     | Kostengünstig                     |\n",
    "| GPT-4 Turbo       | 128K Tokens    | Teurer, aber umfangreicherer Kontext |\n",
    "| Claude 3 Opus     | 200K Tokens    | Sehr großes Kontextfenster        |\n",
    "| Gemini Pro        | 32K Tokens     | Gutes Preis-Leistungs-Verhältnis  |\n",
    "| Mistral Large     | 32K Tokens     | Open-Source Alternative           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token-Zählung verstehen\n",
    "\n",
    "Lassen Sie uns untersuchen, wie Texte in Tokens umgewandelt werden. Dies ist wichtig, um das Kontextfenster effizient zu nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: 'KI ist eine tolle Sache.'\n",
      "Anzahl Tokens: 8\n",
      "Text 2: 'Künstliche Intelligenz revolutioniert viele Bereiche unseres Lebens.'\n",
      "Anzahl Tokens: 16\n",
      "\n",
      "Token-Breakdown für Text 1:\n",
      "Token 1: 82071 -> 'KI'\n",
      "Token 2: 6127 -> ' ist'\n",
      "Token 3: 10021 -> ' eine'\n",
      "Token 4: 39674 -> ' tol'\n",
      "Token 5: 273 -> 'le'\n",
      "Token 6: 328 -> ' S'\n",
      "Token 7: 1815 -> 'ache'\n",
      "Token 8: 13 -> '.'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Zählt die Anzahl der Tokens in einem Text für ein bestimmtes Modell.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def show_token_breakdown(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Zeigt die einzelnen Tokens eines Textes an.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    decoded_tokens = [encoding.decode_single_token_bytes(token).decode(\"utf-8\", errors=\"replace\") for token in tokens]\n",
    "    return tokens, decoded_tokens\n",
    "\n",
    "# Beispieltexte\n",
    "text1 = \"KI ist eine tolle Sache.\"\n",
    "text2 = \"Künstliche Intelligenz revolutioniert viele Bereiche unseres Lebens.\"\n",
    "\n",
    "# Token-Anzahl ausgeben\n",
    "print(f\"Text 1: '{text1}'\")\n",
    "print(f\"Anzahl Tokens: {count_tokens(text1)}\")\n",
    "print(f\"Text 2: '{text2}'\")\n",
    "print(f\"Anzahl Tokens: {count_tokens(text2)}\")\n",
    "\n",
    "# Token-Breakdown für ersten Text\n",
    "tokens, decoded = show_token_breakdown(text1)\n",
    "print(\"\\nToken-Breakdown für Text 1:\")\n",
    "for i, (token, decoded_token) in enumerate(zip(tokens, decoded)):\n",
    "    print(f\"Token {i+1}: {token} -> '{decoded_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Typen in LangChain\n",
    "\n",
    "LangChain bietet verschiedene Memory-Typen, die für unterschiedliche Anwendungsfälle optimiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Buffer Memory\n",
    "\n",
    "Der einfachste Typ - speichert alle Nachrichten in voller Länge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mein Name ist Hans\n",
      "AI: Hallo Hans!\n",
      "Human: Ich komme aus Berlin\n",
      "AI: Berlin ist eine schöne Stadt.\n",
      "Human: Was ist mein Name?\n",
      "AI: Dein Name ist Hans.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/800293615.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  buffer_memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "# Konversation hinzufügen\n",
    "buffer_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "buffer_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "buffer_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "\n",
    "# Abrufen des gesamten Kontexts\n",
    "context = buffer_memory.load_memory_variables({})\n",
    "print(context[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Summary Memory\n",
    "\n",
    "Erstellt eine fortlaufende Zusammenfassung der Konversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/2706353524.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung der Konversation:\n",
      "Hans from Berlin is introduced. The AI responds with a friendly greeting and acknowledges the beauty of Berlin. Hans asks \"What is my name?\" and the AI responds, \"Your name is Hans.\" The human expresses interest in artificial intelligence in German, to which the AI responds enthusiastically, mentioning areas such as Machine Learning, neural networks, and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Konversation hinzufügen\n",
    "summary_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "summary_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "summary_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "summary_memory.save_context({\"input\": \"Ich interessiere mich für künstliche Intelligenz.\"}, \n",
    "                          {\"output\": \"Das ist ein spannendes Thema! KI umfasst viele Bereiche wie Machine Learning, neuronale Netze und natürliche Sprachverarbeitung.\"})\n",
    "\n",
    "# Zusammenfassung abrufen\n",
    "summary = summary_memory.load_memory_variables({})\n",
    "print(\"Zusammenfassung der Konversation:\")\n",
    "print(summary[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Window Memory\n",
    "\n",
    "Behält nur die letzten n Nachrichten bei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Memory (letzte 2 Nachrichten):\n",
      "Human: Ich komme aus Berlin\n",
      "AI: Berlin ist eine schöne Stadt.\n",
      "Human: Was ist mein Name?\n",
      "AI: Dein Name ist Hans.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/2547049578.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationBufferWindowMemory(k=2)  # Nur die letzten 2 Nachrichten behalten\n"
     ]
    }
   ],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=2)  # Nur die letzten 2 Nachrichten behalten\n",
    "\n",
    "# Konversation hinzufügen\n",
    "window_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "window_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "window_memory.save_context({\"input\": \"Was ist mein Name?\"}, {\"output\": \"Dein Name ist Hans.\"})\n",
    "\n",
    "# Nur die letzten 2 Nachrichten abrufen\n",
    "window_context = window_memory.load_memory_variables({})\n",
    "print(\"Window Memory (letzte 2 Nachrichten):\")\n",
    "print(window_context[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Token Buffer Memory\n",
    "\n",
    "Begrenzt die Anzahl der gespeicherten Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Buffer Memory (max. 100 Tokens):\n",
      "Human: Was sind Anwendungsbeispiele?\n",
      "AI: Anwendungsbeispiele für KI sind Spracherkennung, autonomes Fahren, medizinische Diagnose, Bilderkennung und natürlich Sprachmodelle wie ich.\n",
      "\n",
      "Anzahl Tokens im Memory: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/2684029810.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  token_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    }
   ],
   "source": [
    "token_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "# Längere Konversation hinzufügen\n",
    "token_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "token_memory.save_context({\"input\": \"Erzähl mir etwas über künstliche Intelligenz\"}, \n",
    "                        {\"output\": \"Künstliche Intelligenz (KI) ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens befasst. KI-Systeme können lernen, Probleme lösen und Entscheidungen treffen.\"})\n",
    "token_memory.save_context({\"input\": \"Was sind Anwendungsbeispiele?\"}, \n",
    "                        {\"output\": \"Anwendungsbeispiele für KI sind Spracherkennung, autonomes Fahren, medizinische Diagnose, Bilderkennung und natürlich Sprachmodelle wie ich.\"})\n",
    "\n",
    "# Token-begrenzte Geschichte abrufen\n",
    "token_context = token_memory.load_memory_variables({})\n",
    "print(\"Token Buffer Memory (max. 100 Tokens):\")\n",
    "print(token_context[\"history\"])\n",
    "\n",
    "# Anzahl der Tokens in der Geschichte\n",
    "print(f\"\\nAnzahl Tokens im Memory: {count_tokens(token_context['history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Vector Store Memory\n",
    "\n",
    "Speichert die Konversation in einer Vektordatenbank und ruft die relevantesten Nachrichten basierend auf der aktuellen Anfrage ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/2368775854.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(embedding_function=embeddings, collection_name=\"memory_store\")\n",
      "/tmp/ipykernel_51918/2368775854.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Memory (relevante Nachrichten basierend auf der Abfrage):\n",
      "input: Mein Name ist Hans\n",
      "output: Hallo Hans!\n",
      "input: Ich komme aus Berlin\n",
      "output: Berlin ist eine schöne Stadt.\n",
      "input: Meine Lieblingsfarbe ist blau\n",
      "output: Blau ist eine schöne Farbe.\n"
     ]
    }
   ],
   "source": [
    "# Einfache Vektordatenbank erstellen\n",
    "vectorstore = Chroma(embedding_function=embeddings, collection_name=\"memory_store\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Vector Memory initialisieren\n",
    "vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Verschiedene Informationen hinzufügen\n",
    "vector_memory.save_context({\"input\": \"Mein Name ist Hans\"}, {\"output\": \"Hallo Hans!\"})\n",
    "vector_memory.save_context({\"input\": \"Ich komme aus Berlin\"}, {\"output\": \"Berlin ist eine schöne Stadt.\"})\n",
    "vector_memory.save_context({\"input\": \"Meine Lieblingsfarbe ist blau\"}, {\"output\": \"Blau ist eine schöne Farbe.\"})\n",
    "vector_memory.save_context({\"input\": \"Ich mag Pizza und Pasta\"}, {\"output\": \"Italienisches Essen ist sehr beliebt.\"})\n",
    "vector_memory.save_context({\"input\": \"Ich arbeite als Softwareentwickler\"}, {\"output\": \"Softwareentwicklung ist ein spannendes Berufsfeld.\"})\n",
    "\n",
    "# Relevanten Kontext für eine Anfrage abrufen\n",
    "query_info = vector_memory.load_memory_variables({\"prompt\": \"Was war noch mal mein Name und woher komme ich?\"})\n",
    "print(\"Vector Memory (relevante Nachrichten basierend auf der Abfrage):\")\n",
    "print(query_info[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory-Typen in der Praxis vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51918/84099166.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  return ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "def create_conversation_chain(memory, name):\n",
    "    \"\"\"Erstellt eine Konversationskette mit dem angegebenen Memory-Typ.\"\"\"\n",
    "    return ConversationChain(\n",
    "        llm=llm, \n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "# Verschiedene Memory-Typen erstellen\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# Konversationsketten erstellen\n",
    "buffer_chain = create_conversation_chain(buffer_memory, \"Buffer Memory\")\n",
    "summary_chain = create_conversation_chain(summary_memory, \"Summary Memory\")\n",
    "window_chain = create_conversation_chain(window_memory, \"Window Memory\")\n",
    "\n",
    "# Funktion zum Ausführen und Messen einer Konversation\n",
    "def run_conversation(chain, name, messages):\n",
    "    \"\"\"Führt eine Konversation mit der angegebenen Kette durch und misst die Antwortzeit.\"\"\"\n",
    "    print(f\"\\n=== {name} ===\\n\")\n",
    "    responses = []\n",
    "    times = []\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"Nachricht {i+1}: {message}\")\n",
    "        start_time = time.time()\n",
    "        response = chain.predict(input=message)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        \n",
    "        responses.append(response)\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        print(f\"Antwort: {response}\")\n",
    "        print(f\"Zeit: {elapsed:.2f} Sekunden\\n\")\n",
    "    \n",
    "    return responses, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Buffer Memory ===\n",
      "\n",
      "Nachricht 1: Mein Name ist Maria und ich komme aus München.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Hallo Maria! Schön, dich kennenzulernen. München ist eine sehr schöne Stadt in Deutschland. Hast du schon lange dort gelebt?\n",
      "Zeit: 0.69 Sekunden\n",
      "\n",
      "Nachricht 2: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön, dich kennenzulernen. München ist eine sehr schöne Stadt in Deutschland. Hast du schon lange dort gelebt?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist sehr beeindruckend, Maria! Als Ärztin leistest du sicherlich wichtige Arbeit für die Gesundheit der Menschen in München. Wie lange arbeitest du schon in diesem Beruf?\n",
      "Zeit: 0.85 Sekunden\n",
      "\n",
      "Nachricht 3: In meiner Freizeit spiele ich gerne Klavier.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön, dich kennenzulernen. München ist eine sehr schöne Stadt in Deutschland. Hast du schon lange dort gelebt?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI: Das ist sehr beeindruckend, Maria! Als Ärztin leistest du sicherlich wichtige Arbeit für die Gesundheit der Menschen in München. Wie lange arbeitest du schon in diesem Beruf?\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist wunderbar, Maria! Klavierspielen ist eine tolle Möglichkeit, sich zu entspannen und kreativ zu sein. Seit wann spielst du schon Klavier und welche Art von Musik spielst du am liebsten?\n",
      "Zeit: 0.98 Sekunden\n",
      "\n",
      "Nachricht 4: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön, dich kennenzulernen. München ist eine sehr schöne Stadt in Deutschland. Hast du schon lange dort gelebt?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI: Das ist sehr beeindruckend, Maria! Als Ärztin leistest du sicherlich wichtige Arbeit für die Gesundheit der Menschen in München. Wie lange arbeitest du schon in diesem Beruf?\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI: Das ist wunderbar, Maria! Klavierspielen ist eine tolle Möglichkeit, sich zu entspannen und kreativ zu sein. Seit wann spielst du schon Klavier und welche Art von Musik spielst du am liebsten?\n",
      "Human: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist schön zu hören, Maria! Wie alt sind deine Kinder und was sind ihre Interessen? Es muss sehr aufregend sein, Mutter von zwei Kindern zu sein.\n",
      "Zeit: 0.70 Sekunden\n",
      "\n",
      "Nachricht 5: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön, dich kennenzulernen. München ist eine sehr schöne Stadt in Deutschland. Hast du schon lange dort gelebt?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI: Das ist sehr beeindruckend, Maria! Als Ärztin leistest du sicherlich wichtige Arbeit für die Gesundheit der Menschen in München. Wie lange arbeitest du schon in diesem Beruf?\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI: Das ist wunderbar, Maria! Klavierspielen ist eine tolle Möglichkeit, sich zu entspannen und kreativ zu sein. Seit wann spielst du schon Klavier und welche Art von Musik spielst du am liebsten?\n",
      "Human: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "AI: Das ist schön zu hören, Maria! Wie alt sind deine Kinder und was sind ihre Interessen? Es muss sehr aufregend sein, Mutter von zwei Kindern zu sein.\n",
      "Human: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Natürlich, Maria! Dein Name ist Maria und du arbeitest als Ärztin in München. Es ist schön, dich kennenzulernen und mehr über dich zu erfahren.\n",
      "Zeit: 0.75 Sekunden\n",
      "\n",
      "\n",
      "=== Summary Memory ===\n",
      "\n",
      "Nachricht 1: Mein Name ist Maria und ich komme aus München.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Hallo Maria! Es ist schön, dich kennenzulernen. München ist eine wunderschöne Stadt in Deutschland. Hast du dort schon lange gelebt?\n",
      "Zeit: 1.60 Sekunden\n",
      "\n",
      "Nachricht 2: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces herself as Maria from Munich. The AI responds warmly and acknowledges the beauty of Munich.\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Schön, Maria! München ist wirklich eine wunderschöne Stadt. Als Ärztin hast du bestimmt viel zu tun. Welches Fachgebiet übst du aus?\n",
      "Zeit: 1.67 Sekunden\n",
      "\n",
      "Nachricht 3: In meiner Freizeit spiele ich gerne Klavier.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, Maria, introduces herself as a 35-year-old doctor from Munich. The AI responds warmly and acknowledges the beauty of Munich, expressing interest in Maria's medical specialization.\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist wunderbar! Klavierspielen ist eine wunderbare Art, sich zu entspannen und kreativ zu sein. Hast du schon lange Klavier gespielt?\n",
      "Zeit: 1.86 Sekunden\n",
      "\n",
      "Nachricht 4: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Maria introduces herself as a 35-year-old doctor from Munich. The AI responds warmly and acknowledges the beauty of Munich, expressing interest in Maria's medical specialization. Maria mentions that she enjoys playing the piano in her free time. The AI responds positively, noting that playing the piano is a wonderful way to relax and be creative, and asks Maria how long she has been playing.\n",
      "Human: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist wunderbar! Wie alt sind Thomas und Lisa?\n",
      "Zeit: 1.93 Sekunden\n",
      "\n",
      "Nachricht 5: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Maria introduces herself as a 35-year-old doctor from Munich. The AI responds warmly and acknowledges the beauty of Munich, expressing interest in Maria's medical specialization. Maria mentions that she enjoys playing the piano in her free time. The AI responds positively, noting that playing the piano is a wonderful way to relax and be creative, and asks Maria how long she has been playing. The human then says, \"Ich habe zwei Kinder, Thomas und Lisa.\" The AI responds, \"Das ist wunderbar! Wie alt sind Thomas und Lisa?\"\n",
      "Human: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Natürlich! Du hast dich als Maria vorgestellt und gesagt, dass du Ärztin bist.\n",
      "Zeit: 6.44 Sekunden\n",
      "\n",
      "\n",
      "=== Window Memory ===\n",
      "\n",
      "Nachricht 1: Mein Name ist Maria und ich komme aus München.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Hallo Maria! Schön dich kennenzulernen. München ist eine wunderschöne Stadt in Deutschland mit einer reichen Geschichte und vielen kulturellen Sehenswürdigkeiten. Wie kann ich dir heute helfen?\n",
      "Zeit: 0.91 Sekunden\n",
      "\n",
      "Nachricht 2: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön dich kennenzulernen. München ist eine wunderschöne Stadt in Deutschland mit einer reichen Geschichte und vielen kulturellen Sehenswürdigkeiten. Wie kann ich dir heute helfen?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist wirklich interessant, Maria! Als Ärztin hast du bestimmt viel Erfahrung im medizinischen Bereich. Wenn du irgendwelche Fragen zu Gesundheitsthemen hast, stehe ich gerne zur Verfügung, um zu helfen. Gibt es etwas Bestimmtes, worüber du sprechen möchtest?\n",
      "Zeit: 1.10 Sekunden\n",
      "\n",
      "Nachricht 3: In meiner Freizeit spiele ich gerne Klavier.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mein Name ist Maria und ich komme aus München.\n",
      "AI: Hallo Maria! Schön dich kennenzulernen. München ist eine wunderschöne Stadt in Deutschland mit einer reichen Geschichte und vielen kulturellen Sehenswürdigkeiten. Wie kann ich dir heute helfen?\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI: Das ist wirklich interessant, Maria! Als Ärztin hast du bestimmt viel Erfahrung im medizinischen Bereich. Wenn du irgendwelche Fragen zu Gesundheitsthemen hast, stehe ich gerne zur Verfügung, um zu helfen. Gibt es etwas Bestimmtes, worüber du sprechen möchtest?\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das ist toll, Maria! Klavierspielen ist eine wunderbare Möglichkeit, sich zu entspannen und kreativ zu sein. Welche Art von Musik spielst du gerne auf dem Klavier? Hast du Lieblingsstücke oder Komponisten, die du besonders magst? Ich kann dir auch Informationen über verschiedene Musikstile und Klaviermusik geben, wenn du interessiert bist.\n",
      "Zeit: 1.37 Sekunden\n",
      "\n",
      "Nachricht 4: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Ich bin 35 Jahre alt und arbeite als Ärztin.\n",
      "AI: Das ist wirklich interessant, Maria! Als Ärztin hast du bestimmt viel Erfahrung im medizinischen Bereich. Wenn du irgendwelche Fragen zu Gesundheitsthemen hast, stehe ich gerne zur Verfügung, um zu helfen. Gibt es etwas Bestimmtes, worüber du sprechen möchtest?\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI: Das ist toll, Maria! Klavierspielen ist eine wunderbare Möglichkeit, sich zu entspannen und kreativ zu sein. Welche Art von Musik spielst du gerne auf dem Klavier? Hast du Lieblingsstücke oder Komponisten, die du besonders magst? Ich kann dir auch Informationen über verschiedene Musikstile und Klaviermusik geben, wenn du interessiert bist.\n",
      "Human: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Das klingt nach einer schönen Familie, Maria! Es muss eine aufregende Zeit sein, zwei Kinder großzuziehen. Wie alt sind Thomas und Lisa? Haben sie auch Interessen oder Hobbys, die sie gerne verfolgen? Als Mutter von zwei Kindern hast du bestimmt viel Erfahrung im Jonglieren von Familie und Arbeit. Wenn du Tipps oder Ratschläge brauchst, stehe ich gerne zur Verfügung.\n",
      "Zeit: 1.30 Sekunden\n",
      "\n",
      "Nachricht 5: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: In meiner Freizeit spiele ich gerne Klavier.\n",
      "AI: Das ist toll, Maria! Klavierspielen ist eine wunderbare Möglichkeit, sich zu entspannen und kreativ zu sein. Welche Art von Musik spielst du gerne auf dem Klavier? Hast du Lieblingsstücke oder Komponisten, die du besonders magst? Ich kann dir auch Informationen über verschiedene Musikstile und Klaviermusik geben, wenn du interessiert bist.\n",
      "Human: Ich habe zwei Kinder, Thomas und Lisa.\n",
      "AI: Das klingt nach einer schönen Familie, Maria! Es muss eine aufregende Zeit sein, zwei Kinder großzuziehen. Wie alt sind Thomas und Lisa? Haben sie auch Interessen oder Hobbys, die sie gerne verfolgen? Als Mutter von zwei Kindern hast du bestimmt viel Erfahrung im Jonglieren von Familie und Arbeit. Wenn du Tipps oder Ratschläge brauchst, stehe ich gerne zur Verfügung.\n",
      "Human: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Antwort: Leider habe ich keine Informationen über deinen Namen oder deinen Beruf, Maria. Aber wenn du mir mehr darüber erzählen möchtest, bin ich hier, um zuzuhören und zu lernen. Gerne kannst du mir mehr über dich erzählen, damit ich dich besser kennenlernen kann.\n",
      "Zeit: 1.63 Sekunden\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testreihe mit verschiedenen Fragen\n",
    "test_messages = [\n",
    "    \"Mein Name ist Maria und ich komme aus München.\",\n",
    "    \"Ich bin 35 Jahre alt und arbeite als Ärztin.\",\n",
    "    \"In meiner Freizeit spiele ich gerne Klavier.\",\n",
    "    \"Ich habe zwei Kinder, Thomas und Lisa.\",\n",
    "    \"Kannst du mir sagen, wie ich heiße und was ich beruflich mache?\"\n",
    "]\n",
    "\n",
    "# Konversation mit verschiedenen Memory-Typen durchführen\n",
    "buffer_responses, buffer_times = run_conversation(buffer_chain, \"Buffer Memory\", test_messages)\n",
    "summary_responses, summary_times = run_conversation(summary_chain, \"Summary Memory\", test_messages)\n",
    "window_responses, window_times = run_conversation(window_chain, \"Window Memory\", test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vergleich der Antwortqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergleich der letzten Antworten (Frage: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?)\n",
      "\n",
      "Buffer Memory Antwort:\n",
      "Natürlich, Maria! Dein Name ist Maria und du arbeitest als Ärztin in München. Es ist schön, dich kennenzulernen und mehr über dich zu erfahren.\n",
      "\n",
      "Summary Memory Antwort:\n",
      "Natürlich! Du hast dich als Maria vorgestellt und gesagt, dass du Ärztin bist.\n",
      "\n",
      "Window Memory Antwort:\n",
      "Leider habe ich keine Informationen über deinen Namen oder deinen Beruf, Maria. Aber wenn du mir mehr darüber erzählen möchtest, bin ich hier, um zuzuhören und zu lernen. Gerne kannst du mir mehr über dich erzählen, damit ich dich besser kennenlernen kann.\n",
      "\n",
      "Durchschnittliche Antwortzeiten:\n",
      "Buffer Memory: 0.80 Sekunden\n",
      "Summary Memory: 2.70 Sekunden\n",
      "Window Memory: 1.26 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Vergleich der letzten Antworten\n",
    "print(\"Vergleich der letzten Antworten (Frage: Kannst du mir sagen, wie ich heiße und was ich beruflich mache?)\")\n",
    "print(\"\\nBuffer Memory Antwort:\")\n",
    "print(buffer_responses[-1])\n",
    "print(\"\\nSummary Memory Antwort:\")\n",
    "print(summary_responses[-1])\n",
    "print(\"\\nWindow Memory Antwort:\")\n",
    "print(window_responses[-1])\n",
    "\n",
    "# Vergleich der Antwortzeiten\n",
    "print(\"\\nDurchschnittliche Antwortzeiten:\")\n",
    "print(f\"Buffer Memory: {sum(buffer_times)/len(buffer_times):.2f} Sekunden\")\n",
    "print(f\"Summary Memory: {sum(summary_times)/len(summary_times):.2f} Sekunden\")\n",
    "print(f\"Window Memory: {sum(window_times)/len(window_times):.2f} Sekunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manuelle Implementation eines hierarchischen Gedächtnisses\n",
    "\n",
    "Bei komplexen Anwendungen kann ein hierarchischer Ansatz sinnvoll sein, der verschiedene Memory-Typen kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalMemory:\n",
    "    def __init__(self, llm):\n",
    "        # Kurzzeit-Gedächtnis: die letzten n Nachrichten (Window Memory)\n",
    "        self.short_term = ConversationBufferWindowMemory(k=3)\n",
    "        \n",
    "        # Mittelfristiges Gedächtnis: Zusammenfassung bisheriger Konversation\n",
    "        self.medium_term = ConversationSummaryMemory(llm=llm)\n",
    "        \n",
    "        # Langzeit-Gedächtnis: Wichtige Fakten in strukturierter Form\n",
    "        self.long_term_facts = {}\n",
    "        \n",
    "        self.llm = llm\n",
    "    \n",
    "    def save_context(self, input_text, output_text):\n",
    "        # Speichern im Kurzzeit- und mittelfristigen Gedächtnis\n",
    "        self.short_term.save_context({\"input\": input_text}, {\"output\": output_text})\n",
    "        self.medium_term.save_context({\"input\": input_text}, {\"output\": output_text})\n",
    "        \n",
    "        # Wichtige Fakten extrahieren und im Langzeitgedächtnis speichern\n",
    "        self.extract_facts(input_text)\n",
    "    \n",
    "    def extract_facts(self, input_text):\n",
    "        # Einfache Faktenextraktion durch Schlüsselwörter\n",
    "        if \"mein Name ist\" in input_text.lower():\n",
    "            parts = input_text.split(\"mein Name ist\", 1)\n",
    "            if len(parts) > 1:\n",
    "                name = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"name\"] = name\n",
    "        \n",
    "        if \"ich komme aus\" in input_text.lower():\n",
    "            parts = input_text.split(\"ich komme aus\", 1)\n",
    "            if len(parts) > 1:\n",
    "                location = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"location\"] = location\n",
    "        \n",
    "        if \"ich arbeite als\" in input_text.lower():\n",
    "            parts = input_text.split(\"ich arbeite als\", 1)\n",
    "            if len(parts) > 1:\n",
    "                job = parts[1].split(\".\")[0].strip()\n",
    "                self.long_term_facts[\"job\"] = job\n",
    "    \n",
    "    def get_relevant_context(self, query):\n",
    "        # Kontext aus verschiedenen Quellen zusammenstellen\n",
    "        context = \"\"\n",
    "        \n",
    "        # 1. Wichtige Fakten aus dem Langzeitgedächtnis\n",
    "        if self.long_term_facts:\n",
    "            context += \"Wichtige Fakten:\\n\"\n",
    "            for key, value in self.long_term_facts.items():\n",
    "                context += f\"- {key.capitalize()}: {value}\\n\"\n",
    "        \n",
    "        # 2. Zusammenfassung der bisherigen Konversation\n",
    "        medium_context = self.medium_term.load_memory_variables({})\n",
    "        if medium_context[\"history\"]:\n",
    "            context += \"\\nZusammenfassung der Konversation:\\n\"\n",
    "            context += medium_context[\"history\"]\n",
    "        \n",
    "        # 3. Letzte Nachrichten aus dem Kurzzeitgedächtnis\n",
    "        short_context = self.short_term.load_memory_variables({})\n",
    "        if short_context[\"history\"]:\n",
    "            context += \"\\nLetzte Nachrichten:\\n\"\n",
    "            context += short_context[\"history\"]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Mein Name ist Klaus Schmidt.\n",
      "AI: Hallo Klaus, schön dich kennenzulernen!\n",
      "\n",
      "User: Ich komme aus Hamburg.\n",
      "AI: Hamburg ist eine schöne Stadt am Wasser.\n",
      "\n",
      "User: Ich arbeite als Softwareentwickler.\n",
      "AI: Softwareentwicklung ist ein spannender Beruf mit vielen Möglichkeiten.\n",
      "\n",
      "User: Ich interessiere mich für künstliche Intelligenz.\n",
      "AI: Das Gebiet der KI entwickelt sich rasant und bietet viele interessante Anwendungsmöglichkeiten.\n",
      "\n",
      "User: Ich habe letztes Jahr ein Projekt mit Python umgesetzt.\n",
      "AI: Python ist eine großartige Sprache für KI-Projekte und viele andere Anwendungen.\n",
      "\n",
      "=== Hierarchisches Gedächtnis ===\n",
      "\n",
      "\n",
      "Zusammenfassung der Konversation:\n",
      "Klaus Schmidt introduces himself to the AI, and they exchange greetings. Klaus mentions he is from Hamburg, and the AI responds by saying it's a beautiful city by the water. The human shares that they work as a software developer and mentions completing a project with Python last year. The AI responds by saying Python is a great language for AI projects and many other applications. The human expresses interest in artificial intelligence, and the AI responds by saying that the field of AI is developing rapidly and offers many interesting application possibilities.\n",
      "Letzte Nachrichten:\n",
      "Human: Ich arbeite als Softwareentwickler.\n",
      "AI: Softwareentwicklung ist ein spannender Beruf mit vielen Möglichkeiten.\n",
      "Human: Ich interessiere mich für künstliche Intelligenz.\n",
      "AI: Das Gebiet der KI entwickelt sich rasant und bietet viele interessante Anwendungsmöglichkeiten.\n",
      "Human: Ich habe letztes Jahr ein Projekt mit Python umgesetzt.\n",
      "AI: Python ist eine großartige Sprache für KI-Projekte und viele andere Anwendungen.\n",
      "\n",
      "=== Langzeitgedächtnis (extrahierte Fakten) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hierarchisches Gedächtnis testen\n",
    "hierarchical_memory = HierarchicalMemory(llm)\n",
    "\n",
    "# Konversation simulieren\n",
    "conversation = [\n",
    "    (\"Mein Name ist Klaus Schmidt.\", \"Hallo Klaus, schön dich kennenzulernen!\"),\n",
    "    (\"Ich komme aus Hamburg.\", \"Hamburg ist eine schöne Stadt am Wasser.\"),\n",
    "    (\"Ich arbeite als Softwareentwickler.\", \"Softwareentwicklung ist ein spannender Beruf mit vielen Möglichkeiten.\"),\n",
    "    (\"Ich interessiere mich für künstliche Intelligenz.\", \"Das Gebiet der KI entwickelt sich rasant und bietet viele interessante Anwendungsmöglichkeiten.\"),\n",
    "    (\"Ich habe letztes Jahr ein Projekt mit Python umgesetzt.\", \"Python ist eine großartige Sprache für KI-Projekte und viele andere Anwendungen.\"),\n",
    "]\n",
    "\n",
    "for input_text, output_text in conversation:\n",
    "    hierarchical_memory.save_context(input_text, output_text)\n",
    "    print(f\"User: {input_text}\")\n",
    "    print(f\"AI: {output_text}\\n\")\n",
    "\n",
    "# Relevanten Kontext für eine Anfrage abrufen\n",
    "query = \"Wie heißt du und was machst du beruflich?\"\n",
    "context = hierarchical_memory.get_relevant_context(query)\n",
    "print(\"=== Hierarchisches Gedächtnis ===\\n\")\n",
    "print(context)\n",
    "\n",
    "# Langzeitgedächtnis überprüfen\n",
    "print(\"\\n=== Langzeitgedächtnis (extrahierte Fakten) ===\\n\")\n",
    "for key, value in hierarchical_memory.long_term_facts.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Praktische Anwendung: Chat mit effizientem Kontextmanagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Hallo! Ich bin Julia und komme aus München.\n",
      "AI: Hallo Julia! Wie kann ich dir heute helfen?\n",
      "\n",
      "User: Ich bin 32 Jahre alt und arbeite als Grafikdesignerin.\n",
      "AI: Das klingt nach einem spannenden Beruf! Wenn du Fragen zu Grafikdesign hast oder Unterstützung bei einem bestimmten Projekt benötigst, lass es mich gerne wissen. Wie kann ich dir weiterhelfen?\n",
      "\n",
      "User: Ich habe einen Hund namens Bello und eine Katze namens Mimi.\n",
      "AI: Das klingt nach zwei süßen Haustieren! Hast du vielleicht Lust, ein Grafikdesignprojekt rund um deine Haustiere zu gestalten? Zum Beispiel ein Poster, eine Illustration oder ein Logo mit ihren Namen und Bildern? Lass mich wissen, wenn du dabei Unterstützung brauchst oder weitere Ideen hast.\n",
      "\n",
      "User: In meiner Freizeit gehe ich gerne wandern und fotografieren.\n",
      "AI: Das klingt nach tollen Hobbys! Wenn du möchtest, könnte man auch ein Grafikdesignprojekt gestalten, das deine Leidenschaft fürs Wandern und Fotografieren widerspiegelt. Zum Beispiel könntest du ein Naturposter entwerfen, das Landschaften oder Wanderwege zeigt, kombiniert mit Fotografie-Elementen. Oder vielleicht ein Logo für deine persönliche Fotografie-Marke. Wenn du Interesse hast, stehe ich gerne zur Verfügung, um dich bei der Umsetzung zu unterstützen. Hast du konkrete Ideen, die wir gemeinsam umsetzen könnten?\n",
      "\n",
      "User: Kannst du mir zusammenfassen, was du über mich weißt?\n",
      "AI: Natürlich! Du bist Julia, 32 Jahre alt und arbeitest als Grafikdesignerin in München. Du hast einen Hund namens Bello und eine Katze namens Mimi. In deiner Freizeit gehst du gerne wandern und fotografieren. Ich habe vorgeschlagen, Grafikdesignprojekte zu erstellen, die deine Haustiere oder deine Leidenschaft fürs Wandern und Fotografieren reflektieren, wie zum Beispiel ein Poster, eine Illustration, ein Logo oder ein Naturposter mit Fotografie-Elementen. Wenn du konkrete Ideen hast oder Unterstützung bei einem Projekt benötigst, stehe ich gerne zur Verfügung.\n"
     ]
    }
   ],
   "source": [
    "def create_optimized_chat():\n",
    "    # Für einen effizienten Chat kombinieren wir verschiedene Memory-Typen\n",
    "    \n",
    "    # System-Prompt definieren\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent. Verwende die folgenden Informationen, um Fragen zu beantworten:\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    # Hierarchisches Gedächtnis für effizienten Kontext\n",
    "    memory = HierarchicalMemory(llm)\n",
    "    \n",
    "    # Chat-Funktion erstellen\n",
    "    def chat(message):\n",
    "        context = memory.get_relevant_context(message)\n",
    "        \n",
    "        # Konversationshistorie aus dem Kurzzeitgedächtnis laden\n",
    "        history = memory.short_term.load_memory_variables({})[\"history\"]\n",
    "        \n",
    "        # LLM-Anfrage stellen\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"Du bist ein hilfreicher Assistent. Verwende die folgenden Informationen, um Fragen zu beantworten:\\n{context}\"),\n",
    "            HumanMessage(content=message)\n",
    "        ]\n",
    "        response = llm.invoke(messages).content\n",
    "        \n",
    "        # Kontext aktualisieren\n",
    "        memory.save_context(message, response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    return chat\n",
    "\n",
    "# Chat-Funktion erstellen\n",
    "chat = create_optimized_chat()\n",
    "\n",
    "# Beispiel-Konversation durchführen\n",
    "messages = [\n",
    "    \"Hallo! Ich bin Julia und komme aus München.\",\n",
    "    \"Ich bin 32 Jahre alt und arbeite als Grafikdesignerin.\",\n",
    "    \"Ich habe einen Hund namens Bello und eine Katze namens Mimi.\",\n",
    "    \"In meiner Freizeit gehe ich gerne wandern und fotografieren.\",\n",
    "    \"Kannst du mir zusammenfassen, was du über mich weißt?\"\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    response = chat(message)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices für Kontextmanagement\n",
    "\n",
    "1. **Relevanz priorisieren**: Nicht jede Information ist gleich wichtig. Priorisieren Sie kritische Informationen wie Nutzerdaten, Präferenzen und aktuelle Anfragen.\n",
    "\n",
    "2. **Hierarchischen Ansatz verwenden**: Kombinieren Sie verschiedene Memory-Typen für verschiedene Informationsebenen:\n",
    "   - **Kurzzeitgedächtnis**: Aktuelle Konversation (letzte 3-5 Nachrichten)\n",
    "   - **Mittelfristiges Gedächtnis**: Zusammenfassung der bisherigen Konversation\n",
    "   - **Langzeitgedächtnis**: Wichtige Fakten und Präferenzen\n",
    "\n",
    "3. **Kontext regelmäßig komprimieren**: Verwenden Sie Summary Memory oder LLM-basierte Zusammenfassungen, um lange Konversationen zu komprimieren.\n",
    "\n",
    "4. **Dynamische Kontexterweiterung**: Laden Sie nur dann externe Informationen, wenn sie für die aktuelle Anfrage relevant sind.\n",
    "\n",
    "5. **Token-Überwachung**: Überwachen Sie die Anzahl der verwendeten Tokens, um Kosten zu kontrollieren und das Kontextfenster optimal zu nutzen.\n",
    "\n",
    "6. **Metadaten nutzen**: Speichern Sie Zeitstempel und Wichtigkeitsbewertungen für jede Information, um später besser filtern zu können.\n",
    "\n",
    "7. **Vergessen implementieren**: Entfernen Sie weniger relevante oder veraltete Informationen aus dem Kontext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fazit\n",
    "\n",
    "Effektives Kontextmanagement ist entscheidend für die Leistung von LLM-Anwendungen. Die richtige Strategie hängt vom konkreten Anwendungsfall ab:\n",
    "\n",
    "- **Buffer Memory**: Gut für kurze Konversationen mit vollständigem Kontext\n",
    "- **Summary Memory**: Sinnvoll für längere Konversationen, bei denen Details weniger wichtig sind\n",
    "- **Window Memory**: Effizient für aufgabenorientierte Dialoge, wo nur der aktuelle Kontext wichtig ist\n",
    "- **Vector Memory**: Ideal für sehr lange Konversationen oder große Wissensdatenbanken\n",
    "- **Hierarchischer Ansatz**: Die beste Lösung für komplexe Anwendungen mit unterschiedlichen Kontextanforderungen\n",
    "\n",
    "Durch die richtige Kombination dieser Techniken können Sie die Effektivität Ihrer LLM-basierten Anwendungen maximieren und gleichzeitig Kosten und Latenz minimieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
