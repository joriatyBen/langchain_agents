{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluationsmethoden für LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook lernen Sie verschiedene Methoden zur Evaluation von LLM-Anwendungen kennen. Wir werden folgende Ansätze untersuchen:\n",
    "\n",
    "- LLM-As-A-Judge: Bewertung von LLM-Antworten durch andere LLMs\n",
    "- NLP-basierte Testkriterien (ROUGE, BLEU)\n",
    "- PI Scrubbing und Datenschutz mit Microsoft Presidio\n",
    "- RAG-spezifische Evaluierungsmethoden\n",
    "\n",
    "Diese Techniken helfen Ihnen, die Qualität Ihrer KI-Anwendungen systematisch zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation und Konfiguration der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der erforderlichen Pakete\n",
    "!pip install langchain langchain_openai langchain-core rouge-score nltk presidio-analyzer presidio-anonymizer -q\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Grundlegende Importe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LangChain Importe\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvaluator\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisieren des LLMs\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Für Bewertungen verwenden wir ein separates LLM-Objekt\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM-As-A-Judge: Grundlagen und Implementierung\n",
    "\n",
    "LLM-As-A-Judge ist ein Ansatz, bei dem ein LLM zur Bewertung der Ausgaben eines anderen LLMs verwendet wird. Dies ist besonders nützlich, wenn keine Referenzantworten verfügbar sind oder die Bewertung menschlicher Präferenzen simuliert werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielantworten für die Bewertung\n",
    "example_question = \"Was sind die wichtigsten Anwendungsfälle für Vektordatenbanken im Kontext von RAG-Systemen?\"\n",
    "\n",
    "response_a = \"\"\"Vektordatenbanken werden in RAG-Systemen hauptsächlich für die effiziente Speicherung und den Abruf von Embeddings verwendet. \n",
    "Sie ermöglichen semantische Suche basierend auf Ähnlichkeit statt einfache Keyword-Suche. \n",
    "Weitere Anwendungsfälle sind Clusteranalyse und die Reduzierung der Dimensionalität großer Datensätze.\"\"\"\n",
    "\n",
    "response_b = \"\"\"Vektordatenbanken speichern Daten. Sie werden mit RAG verwendet, um Texte zu durchsuchen. \n",
    "Man kann darin Daten speichern und wieder abrufen, wenn man sie braucht. \n",
    "Sie sind besser als normale Datenbanken, weil sie schneller sind.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluierung mit einem vordefinierten Evaluator\n",
    "qa_evaluator = load_evaluator(\"labeled_criteria\", criteria={\n",
    "    \"relevanz\": \"Ist die Antwort relevant für die Frage?\",\n",
    "    \"korrektheit\": \"Enthält die Antwort korrekte und aktuelle Informationen?\",\n",
    "    \"vollständigkeit\": \"Beantwortet die Antwort die Frage vollständig?\",\n",
    "    \"klarheit\": \"Ist die Antwort klar, präzise und gut strukturiert?\"\n",
    "}, llm=evaluator_llm)\n",
    "\n",
    "# Bewertung der Antworten\n",
    "eval_result_a = qa_evaluator.evaluate_strings(\n",
    "    prediction=response_a,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "eval_result_b = qa_evaluator.evaluate_strings(\n",
    "    prediction=response_b,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(\"Bewertung Antwort A:\")\n",
    "for criterion, rating in eval_result_a[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")\n",
    "    \n",
    "print(\"\\nBewertung Antwort B:\")\n",
    "for criterion, rating in eval_result_b[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenen LLM-As-A-Judge Evaluator implementieren\n",
    "\n",
    "LangChain bietet vordefinierte Evaluatoren, aber manchmal benötigen wir spezifischere Bewertungskriterien. Hier erstellen wir einen eigenen Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenen Evaluator mit spezifischen Kriterien erstellen\n",
    "custom_criteria = {\n",
    "    \"technische_genauigkeit\": \"Enthält die Antwort technisch korrekte Informationen über Vektordatenbanken und RAG?\",\n",
    "    \"praxisbezug\": \"Gibt die Antwort praktische Beispiele oder Anwendungsfälle?\",\n",
    "    \"zielgruppenorientierung\": \"Ist die Antwort für Entwickler verständlich und hilfreich?\",\n",
    "    \"vollständigkeit\": \"Deckt die Antwort alle wichtigen Aspekte von Vektordatenbanken im RAG-Kontext ab?\"\n",
    "}\n",
    "\n",
    "custom_evaluator = LabeledCriteriaEvaluator(criteria=custom_criteria, llm=evaluator_llm)\n",
    "\n",
    "# Bewertung mit dem benutzerdefinierten Evaluator\n",
    "custom_eval_a = custom_evaluator.evaluate_strings(\n",
    "    prediction=response_a,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(\"Benutzerdefinierte Bewertung für Antwort A:\")\n",
    "for criterion, rating in custom_eval_a[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleichende Bewertung\n",
    "\n",
    "Manchmal ist es sinnvoller, Antworten direkt miteinander zu vergleichen, anstatt sie einzeln zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichenden Evaluator laden\n",
    "pairwise_evaluator = load_evaluator(\"pairwise_string\")\n",
    "\n",
    "# Direkte Gegenüberstellung der Antworten\n",
    "comparison = pairwise_evaluator.evaluate_string_pairs(\n",
    "    prediction_a=response_a,\n",
    "    prediction_b=response_b,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(f\"Bevorzugte Antwort: {comparison['preferred']}\")\n",
    "print(f\"Begründung: {comparison['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP-basierte Testkriterien (ROUGE, BLEU, etc.)\n",
    "\n",
    "Neben LLM-basierten Evaluierungsmethoden können wir auch traditionelle NLP-Metriken verwenden, die auf Textähnlichkeit basieren. Diese sind besonders nützlich, wenn wir Referenzantworten haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-Metrik importieren\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Referenzantwort (Gold-Standard)\n",
    "reference_answer = \"\"\"Vektordatenbanken spielen eine entscheidende Rolle in RAG-Systemen und bieten folgende Hauptanwendungsfälle:\n",
    "1. Effiziente Speicherung und Indexierung von Embeddings für schnellen Abruf\n",
    "2. Semantische Suche basierend auf Vektorähnlichkeit statt nur Schlüsselwörtern\n",
    "3. Filterung und Abfrage mit Metadaten zusätzlich zur Vektorsuche\n",
    "4. Skalierbare Verwaltung großer Dokumentenkorpora für RAG-Anwendungen\n",
    "5. Unterstützung für komplexe Abfragen mit Hybrid-Retrieval-Strategien\"\"\"\n",
    "\n",
    "# ROUGE-Scorer initialisieren\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# ROUGE-Scores berechnen\n",
    "rouge_scores_a = scorer.score(reference_answer, response_a)\n",
    "rouge_scores_b = scorer.score(reference_answer, response_b)\n",
    "\n",
    "print(\"ROUGE-Scores für Antwort A:\")\n",
    "for metric, score in rouge_scores_a.items():\n",
    "    print(f\"{metric}: Precision = {score.precision:.4f}, Recall = {score.recall:.4f}, F1 = {score.fmeasure:.4f}\")\n",
    "\n",
    "print(\"\\nROUGE-Scores für Antwort B:\")\n",
    "for metric, score in rouge_scores_b.items():\n",
    "    print(f\"{metric}: Precision = {score.precision:.4f}, Recall = {score.recall:.4f}, F1 = {score.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU-Score berechnen\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# NLTK-Daten herunterladen, falls noch nicht erfolgt\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Tokenisieren der Texte\n",
    "reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "response_a_tokens = nltk.word_tokenize(response_a.lower())\n",
    "response_b_tokens = nltk.word_tokenize(response_b.lower())\n",
    "\n",
    "# BLEU-Scores berechnen\n",
    "bleu_a = sentence_bleu([reference_tokens], response_a_tokens)\n",
    "bleu_b = sentence_bleu([reference_tokens], response_b_tokens)\n",
    "\n",
    "print(f\"BLEU-Score für Antwort A: {bleu_a:.4f}\")\n",
    "print(f\"BLEU-Score für Antwort B: {bleu_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse visualisieren\n",
    "metrics = ['ROUGE-1 F1', 'ROUGE-2 F1', 'ROUGE-L F1', 'BLEU']\n",
    "scores_a = [rouge_scores_a['rouge1'].fmeasure, rouge_scores_a['rouge2'].fmeasure, \n",
    "            rouge_scores_a['rougeL'].fmeasure, bleu_a]\n",
    "scores_b = [rouge_scores_b['rouge1'].fmeasure, rouge_scores_b['rouge2'].fmeasure, \n",
    "            rouge_scores_b['rougeL'].fmeasure, bleu_b]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, scores_a, width, label='Antwort A')\n",
    "rects2 = ax.bar(x + width/2, scores_b, width, label='Antwort B')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Vergleich der Evaluationsmetriken')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PI Scrubbing und Datenschutz mit Microsoft Presidio\n",
    "\n",
    "Bei der Evaluierung von LLM-Anwendungen ist Datenschutz ein wichtiger Aspekt. Microsoft Presidio ist ein Open-Source-Projekt für die Erkennung und Anonymisierung personenbezogener Daten (PII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presidio-Komponenten importieren\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "# Analyzer und Anonymizer initialisieren\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispieltext mit personenbezogenen Daten\n",
    "text_mit_pii = \"\"\"\n",
    "Sehr geehrter Herr Schmidt,\n",
    "\n",
    "vielen Dank für Ihre Anfrage. Bitte kontaktieren Sie mich unter meiner E-Mail max.mustermann@example.com oder telefonisch unter +49 176 12345678.\n",
    "\n",
    "Ihr Kundenkonto mit der IBAN DE89 3704 0044 0532 0130 00 wurde aktualisiert.\n",
    "\n",
    "Mit freundlichen Grüßen,\n",
    "Dr. Anna Weber\n",
    "\"\"\"\n",
    "\n",
    "# PII erkennen (deutsche Sprache)\n",
    "analyzer_results = analyzer.analyze(text=text_mit_pii, language=\"de\")\n",
    "\n",
    "print(\"Erkannte personenbezogene Daten:\")\n",
    "for result in analyzer_results:\n",
    "    print(f\"- {result.entity_type}: '{text_mit_pii[result.start:result.end]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text anonymisieren\n",
    "anonymized_text = anonymizer.anonymize(\n",
    "    text=text_mit_pii,\n",
    "    analyzer_results=analyzer_results\n",
    ")\n",
    "\n",
    "print(\"Anonymisierter Text:\")\n",
    "print(anonymized_text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Anonymisieren von Evaluationsdaten\n",
    "def prepare_safe_evaluation_data(raw_text):\n",
    "    # PII erkennen\n",
    "    results = analyzer.analyze(text=raw_text, language=\"de\")\n",
    "    \n",
    "    # Wenn PII gefunden wurde, anonymisieren\n",
    "    if results:\n",
    "        anonymized = anonymizer.anonymize(\n",
    "            text=raw_text,\n",
    "            analyzer_results=results\n",
    "        )\n",
    "        return anonymized.text, True\n",
    "    else:\n",
    "        return raw_text, False\n",
    "\n",
    "# Beispiel für eine sichere Evaluierung\n",
    "evaluation_text = \"\"\"Herr Peter Müller (geboren am 15.04.1975) hat eine Anfrage zu seinem Konto 98765432 geschickt.\n",
    "Seine Telefonnummer lautet 030 12345678. Bitte evaluieren Sie die Qualität der Antwort.\"\"\"\n",
    "\n",
    "safe_text, was_anonymized = prepare_safe_evaluation_data(evaluation_text)\n",
    "print(f\"Text wurde anonymisiert: {was_anonymized}\")\n",
    "print(f\"Sicherer Text für Evaluierung:\\n{safe_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG-spezifische Evaluierungsmethoden\n",
    "\n",
    "Für RAG-Systeme (Retrieval Augmented Generation) gibt es spezifische Evaluierungsmethoden, die sowohl die Retrieval-Komponente als auch die Generierungskomponente bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für RAG-Evaluierung\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Einfacher RAG-Prozess simulieren\n",
    "def simulate_rag(query, documents):\n",
    "    # Dokumente als Kontext zusammenfügen\n",
    "    context = \"\\n\\n\".join([f\"Dokument {i+1}: {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    # Prompt für die Generierung\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"Du bist ein hilfreicher Assistent. Verwende den folgenden Kontext, um die Frage zu beantworten.\n",
    "        \n",
    "        Kontext:\n",
    "        {context}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \n",
    "        Deine Antwort:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Chain erstellen und ausführen\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    result = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return result[\"text\"], context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Dokumente\n",
    "rag_documents = [\n",
    "    \"Vektordatenbanken sind spezielle Datenbanken, die für die Speicherung und Abfrage von Vektoren (Embeddings) optimiert sind. Sie ermöglichen schnelle Ähnlichkeitssuchen in hochdimensionalen Räumen.\",\n",
    "    \"RAG (Retrieval Augmented Generation) ist ein Ansatz, bei dem ein LLM mit externen Informationen angereichert wird. Dadurch wird die Faktengenauigkeit verbessert und Halluzinationen werden reduziert.\",\n",
    "    \"Die Hauptvorteile von Vektordatenbanken in RAG-Systemen sind: schnelle Ähnlichkeitssuche, skalierbare Speicherung von Embeddings und die Möglichkeit zur Metadatenfilterung.\",\n",
    "    \"Chroma und Qdrant sind beliebte Vektordatenbanken für RAG-Anwendungen. Chroma ist einfach zu nutzen und ideal für Prototyping, während Qdrant für Produktionsanwendungen mit hohen Anforderungen geeignet ist.\"\n",
    "]\n",
    "\n",
    "# RAG-Anfrage und Antwort generieren\n",
    "rag_query = \"Welche Vektordatenbanken sind für RAG-Systeme am besten geeignet und warum?\"\n",
    "rag_answer, rag_context = simulate_rag(rag_query, rag_documents)\n",
    "\n",
    "print(\"RAG-Antwort:\")\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-spezifische Evaluierung\n",
    "\n",
    "# 1. Kontext-Relevanz prüfen\n",
    "context_relevance_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte für die Bewertung von Retrieval-Systemen. Deine Aufgabe ist es, die Relevanz der zurückgegebenen Dokumente für eine Anfrage zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte bewerte die Relevanz der folgenden Dokumente für die gegebene Anfrage. \n",
    "    Gib jedem Dokument eine Bewertung von 1-5 (1 = irrelevant, 5 = hochrelevant) und eine kurze Begründung.\n",
    "    \n",
    "    Anfrage: {query}\n",
    "    \n",
    "    Zurückgegebene Dokumente:\n",
    "    {context}\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "context_eval_chain = LLMChain(llm=evaluator_llm, prompt=context_relevance_prompt)\n",
    "context_eval = context_eval_chain.invoke({\"query\": rag_query, \"context\": rag_context})\n",
    "\n",
    "print(\"Bewertung der Kontext-Relevanz:\")\n",
    "print(context_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Treue zum Kontext (Faithfulness) prüfen\n",
    "faithfulness_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte für die Bewertung von KI-generierten Antworten. Deine Aufgabe ist es, zu prüfen, ob eine Antwort durch den gegebenen Kontext unterstützt wird.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte analysiere, ob die folgende Antwort vollständig durch den gegebenen Kontext unterstützt wird. \n",
    "    Identifiziere alle Behauptungen in der Antwort und bewerte, ob sie im Kontext enthalten sind oder Halluzinationen darstellen.\n",
    "    \n",
    "    Kontext:\n",
    "    {context}\n",
    "    \n",
    "    Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib eine Gesamtbewertung der Treue zum Kontext (1-5, wobei 5 die höchste Treue bedeutet) und liste alle Halluzinationen oder nicht unterstützten Aussagen auf.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "faithfulness_chain = LLMChain(llm=evaluator_llm, prompt=faithfulness_prompt)\n",
    "faithfulness_eval = faithfulness_chain.invoke({\"context\": rag_context, \"answer\": rag_answer})\n",
    "\n",
    "print(\"Bewertung der Treue zum Kontext:\")\n",
    "print(faithfulness_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Antwortqualität bewerten\n",
    "quality_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte für die Bewertung von KI-generierten Antworten. Deine Aufgabe ist es, die Qualität einer Antwort zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte bewerte die Qualität der folgenden Antwort auf die gegebene Frage nach diesen Kriterien:\n",
    "    1. Vollständigkeit (1-5): Beantwortet die Antwort alle Aspekte der Frage?\n",
    "    2. Korrektheit (1-5): Enthält die Antwort korrekte Informationen?\n",
    "    3. Klarheit (1-5): Ist die Antwort klar und verständlich formuliert?\n",
    "    4. Prägnanz (1-5): Ist die Antwort präzise und auf den Punkt, ohne unnötige Informationen?\n",
    "    \n",
    "    Frage: {query}\n",
    "    \n",
    "    Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib für jedes Kriterium eine Bewertung und eine kurze Begründung. Berechne auch einen Gesamtscore als Durchschnitt aller Kriterien.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "quality_chain = LLMChain(llm=evaluator_llm, prompt=quality_prompt)\n",
    "quality_eval = quality_chain.invoke({\"query\": rag_query, \"answer\": rag_answer})\n",
    "\n",
    "print(\"Bewertung der Antwortqualität:\")\n",
    "print(quality_eval[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ganzheitliche RAG-Evaluierung: RAGAS\n",
    "\n",
    "RAGAS ist ein Framework zur Evaluierung von RAG-Anwendungen, das verschiedene Aspekte eines RAG-Systems bewertet. Hier zeigen wir eine vereinfachte Version der RAGAS-Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vereinfachte RAGAS-Evaluierung\n",
    "ragas_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte für die Evaluierung von RAG-Systemen (Retrieval Augmented Generation). Deine Aufgabe ist es, ein RAG-System nach den RAGAS-Metriken zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte evaluiere das folgende RAG-System nach diesen RAGAS-Metriken:\n",
    "    \n",
    "    1. Kontext-Relevanz: Sind die abgerufenen Dokumente relevant für die Anfrage?\n",
    "    2. Treue (Faithfulness): Wird die Antwort durch den Kontext unterstützt?\n",
    "    3. Antwort-Relevanz: Beantwortet die generierte Antwort die ursprüngliche Frage?\n",
    "    4. Kontext-Nutzung: Wie gut nutzt die Antwort den bereitgestellten Kontext?\n",
    "    \n",
    "    Anfrage: {query}\n",
    "    \n",
    "    Abgerufene Dokumente:\n",
    "    {context}\n",
    "    \n",
    "    Generierte Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib für jede Metrik eine Bewertung von 0-1 (0 = schlecht, 1 = hervorragend) und berechne einen Gesamtscore als gewichteten Durchschnitt.\n",
    "    Gib außerdem konkrete Verbesserungsvorschläge für das RAG-System.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "ragas_chain = LLMChain(llm=evaluator_llm, prompt=ragas_prompt)\n",
    "ragas_eval = ragas_chain.invoke({\"query\": rag_query, \"context\": rag_context, \"answer\": rag_answer})\n",
    "\n",
    "print(\"RAGAS-Evaluierung:\")\n",
    "print(ragas_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Systematische Evaluierung mit einem Testdatensatz\n",
    "\n",
    "Für eine gründliche Evaluierung sollten wir mehrere Anfragen testen und die Ergebnisse aggregieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdatensatz mit Fragen und erwarteten Antworten\n",
    "test_dataset = [\n",
    "    {\n",
    "        \"query\": \"Welche Vektordatenbanken sind für RAG-Systeme am besten geeignet?\",\n",
    "        \"expected_answer\": \"Chroma eignet sich gut für Prototyping, während Qdrant für Produktionsanwendungen empfohlen wird.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Was ist der Hauptvorteil von Vektordatenbanken in RAG?\",\n",
    "        \"expected_answer\": \"Schnelle Ähnlichkeitssuche und skalierbare Speicherung von Embeddings.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Erkläre den Unterschied zwischen RAG und reinen LLM-Anwendungen.\",\n",
    "        \"expected_answer\": \"RAG ergänzt LLMs mit externen Informationen zur Verbesserung der Faktengenauigkeit.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Funktion zur Durchführung einer systematischen Evaluierung\n",
    "def evaluate_rag_system(test_cases, documents):\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        query = test_case[\"query\"]\n",
    "        expected = test_case[\"expected_answer\"]\n",
    "        \n",
    "        # RAG-Antwort generieren\n",
    "        answer, context = simulate_rag(query, documents)\n",
    "        \n",
    "        # Grundlegende Metriken berechnen\n",
    "        # 1. Rouge-Score für Ähnlichkeit zur erwarteten Antwort\n",
    "        rouge_scores = scorer.score(expected, answer)\n",
    "        rouge_f1 = rouge_scores[\"rougeL\"].fmeasure\n",
    "        \n",
    "        # 2. LLM-basierte Bewertung\n",
    "        eval_prompt = f\"\"\"Bewerte die folgende Antwort auf die Frage: \\\"{query}\\\" \n",
    "        auf einer Skala von 1-10 für Genauigkeit, Vollständigkeit und Klarheit. \n",
    "        Erwartete Antwort: {expected}\n",
    "        Tatsächliche Antwort: {answer}\"\"\"\n",
    "        \n",
    "        evaluation = evaluator_llm.invoke(eval_prompt)\n",
    "        \n",
    "        # Ergebnisse sammeln\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"expected\": expected,\n",
    "            \"answer\": answer,\n",
    "            \"rouge_f1\": rouge_f1,\n",
    "            \"llm_evaluation\": evaluation.content\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluierung durchführen\n",
    "evaluation_results = evaluate_rag_system(test_dataset, rag_documents)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    print(f\"\\nTest {i+1}: {result['query']}\")\n",
    "    print(f\"Rouge-L F1: {result['rouge_f1']:.4f}\")\n",
    "    print(f\"LLM-Bewertung: {result['llm_evaluation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zusammenfassung: Evaluierungsworkflow\n",
    "\n",
    "Basierend auf den vorgestellten Methoden können wir einen umfassenden Evaluierungsworkflow für LLM- und RAG-Anwendungen erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umfassender Evaluierungsworkflow für eine LLM-Anwendung\n",
    "def comprehensive_evaluation_workflow(query, documents, reference_answer=None):\n",
    "    print(f\"Evaluierung für Anfrage: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Datenschutz-Check und Anonymisierung\n",
    "    safe_query, was_anonymized = prepare_safe_evaluation_data(query)\n",
    "    if was_anonymized:\n",
    "        print(\"⚠️ Anfrage enthielt personenbezogene Daten und wurde anonymisiert.\")\n",
    "        print(f\"Anonymisierte Anfrage: {safe_query}\")\n",
    "    \n",
    "    # 2. RAG-Antwort generieren\n",
    "    answer, context = simulate_rag(safe_query, documents)\n",
    "    print(\"\\n📝 Generierte Antwort:\")\n",
    "    print(answer)\n",
    "    \n",
    "    # 3. Metriken berechnen\n",
    "    metrics = {}\n",
    "    \n",
    "    # 3.1 Wenn eine Referenzantwort vorhanden ist, klassische NLP-Metriken verwenden\n",
    "    if reference_answer:\n",
    "        rouge_scores = scorer.score(reference_answer, answer)\n",
    "        metrics[\"ROUGE-L F1\"] = rouge_scores[\"rougeL\"].fmeasure\n",
    "        \n",
    "        reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "        answer_tokens = nltk.word_tokenize(answer.lower())\n",
    "        metrics[\"BLEU\"] = sentence_bleu([reference_tokens], answer_tokens)\n",
    "    \n",
    "    # 3.2 LLM-basierte Evaluierung für Qualität\n",
    "    quality_eval = quality_chain.invoke({\"query\": safe_query, \"answer\": answer})\n",
    "    \n",
    "    # 3.3 RAG-spezifische Evaluierung\n",
    "    faith_eval = faithfulness_chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    context_eval = context_eval_chain.invoke({\"query\": safe_query, \"context\": context})\n",
    "    \n",
    "    # 4. Ergebnisse zusammenfassen\n",
    "    print(\"\\n📊 Evaluierungsergebnisse:\")\n",
    "    if reference_answer:\n",
    "        print(f\"ROUGE-L F1: {metrics['ROUGE-L F1']:.4f}\")\n",
    "        print(f\"BLEU Score: {metrics['BLEU']:.4f}\")\n",
    "    \n",
    "    print(\"\\n🔍 Qualitätsbewertung:\")\n",
    "    print(quality_eval[\"text\"])\n",
    "    \n",
    "    print(\"\\n✓ Treue zum Kontext:\")\n",
    "    print(faith_eval[\"text\"])\n",
    "    \n",
    "    print(\"\\n📚 Relevanz der abgerufenen Dokumente:\")\n",
    "    print(context_eval[\"text\"])\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"context\": context,\n",
    "        \"metrics\": metrics,\n",
    "        \"quality_evaluation\": quality_eval[\"text\"],\n",
    "        \"faithfulness_evaluation\": faith_eval[\"text\"],\n",
    "        \"context_evaluation\": context_eval[\"text\"],\n",
    "    }\n",
    "\n",
    "# Beispiel für den umfassenden Evaluierungsworkflow\n",
    "reference = \"Für RAG-Systeme sind Chroma und Qdrant gut geeignete Vektordatenbanken. Chroma ist einfach zu nutzen und ideal für Prototyping, während Qdrant für Produktionsanwendungen mit hohen Anforderungen geeignet ist.\"\n",
    "eval_results = comprehensive_evaluation_workflow(rag_query, rag_documents, reference_answer=reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Übungen\n",
    "\n",
    "Hier sind einige Übungen, um Ihr Wissen über Evaluationsmethoden zu vertiefen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 1: Eigenen Evaluator implementieren\n",
    "\n",
    "Erstellen Sie einen benutzerdefinierten Evaluator für einen spezifischen Anwendungsfall (z.B. technische Dokumentation, medizinische Beratung, oder Kundensupport).\n",
    "\n",
    "1. Definieren Sie 3-5 domänenspezifische Kriterien für Ihre Bewertung\n",
    "2. Implementieren Sie diese mit dem `LabeledCriteriaEvaluator`\n",
    "3. Testen Sie Ihren Evaluator mit mindestens 2 verschiedenen Antworten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 2: Vergleichende RAG-Evaluation\n",
    "\n",
    "Vergleichen Sie zwei verschiedene RAG-Setups und evaluieren Sie ihre Leistung:\n",
    "\n",
    "1. Setup A: Standard-RAG mit allen Dokumenten\n",
    "2. Setup B: RAG mit Vorfilterung (z.B. nur die 2 relevantesten Dokumente verwenden)\n",
    "\n",
    "Verwenden Sie die gleichen 3 Testanfragen für beide Setups und analysieren Sie die Unterschiede in den Ergebnissen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 3: Dashboard für Evaluierungsergebnisse\n",
    "\n",
    "Erstellen Sie ein einfaches Dashboard zur Visualisierung von Evaluierungsergebnissen:\n",
    "\n",
    "1. Führen Sie mindestens 5 Tests mit verschiedenen Anfragen durch\n",
    "2. Speichern Sie die Ergebnisse in einem DataFrame\n",
    "3. Erstellen Sie Visualisierungen für verschiedene Metriken\n",
    "4. Berechnen Sie Durchschnittswerte und identifizieren Sie Verbesserungspotenzial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Schlussfolgerungen und Best Practices\n",
    "\n",
    "- **Kombinieren Sie mehrere Evaluierungsmethoden**: Nutzen Sie sowohl automatische Metriken als auch LLM-basierte Bewertungen für ein umfassenderes Bild.\n",
    "\n",
    "- **Datenschutz beachten**: Stellen Sie sicher, dass Ihre Evaluierungsdaten keine personenbezogenen Informationen enthalten oder diese angemessen anonymisiert sind.\n",
    "\n",
    "- **Kontinuierlich evaluieren**: Bauen Sie Evaluierung in Ihren Entwicklungsprozess ein, nicht nur als einmalige Aktivität am Ende.\n",
    "\n",
    "- **Repräsentative Testdaten**: Stellen Sie sicher, dass Ihre Testfälle die tatsächlichen Nutzungsszenarien repräsentieren.\n",
    "\n",
    "- **Menschliche Überprüfung**: Automatische Evaluierung sollte durch menschliche Überprüfung ergänzt werden, besonders bei wichtigen Anwendungsfällen.\n",
    "\n",
    "- **Ergebnisse dokumentieren**: Halten Sie Ihre Evaluierungsergebnisse fest, um Verbesserungen im Laufe der Zeit zu verfolgen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
