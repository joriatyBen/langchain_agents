{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architekturkonzepte in LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook werden wir verschiedene Architekturkonzepte für LLM-Anwendungen untersuchen:\n",
    "\n",
    "1. Agentenbasierte Ansätze: Grundlagen und Vorteile\n",
    "2. ReAct Pattern: Reasoning and Acting in LLMs\n",
    "3. Graph-RAG und Hierarchical RAG: Fortgeschrittene RAG-Architekturen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benötigte Bibliotheken installieren\n",
    "\n",
    "Zunächst müssen wir sicherstellen, dass alle notwendigen Bibliotheken installiert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /opt/tljh/user/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain-community in /opt/tljh/user/lib/python3.10/site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain-openai in /opt/tljh/user/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: langgraph in /opt/tljh/user/lib/python3.10/site-packages (0.2.44)\n",
      "Requirement already satisfied: python-dotenv in /opt/tljh/user/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (0.1.139)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-openai) (1.53.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from langgraph) (2.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in /opt/tljh/user/lib/python3.10/site-packages (from langgraph) (0.1.35)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/tljh/user/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/tljh/user/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/tljh/user/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.11.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /opt/tljh/user/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/tljh/user/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/tljh/user/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/tljh/user/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/tljh/user/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/tljh/user/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/tljh/user/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/tljh/user/lib/python3.10/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.66.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/tljh/user/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/tljh/user/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/tljh/user/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.2.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/tljh/user/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/tljh/user/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/tljh/user/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-openai langgraph python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API-Schlüssel ist konfiguriert.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Laden der Umgebungsvariablen aus der .env-Datei\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "\n",
    "\n",
    "# Überprüfen, ob der OpenAI API-Schlüssel gesetzt ist\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API-Schlüssel ist konfiguriert.\")\n",
    "else:\n",
    "    print(\"WARNUNG: OpenAI API-Schlüssel fehlt! Bitte in der .env-Datei konfigurieren.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agentenbasierte Ansätze\n",
    "\n",
    "### Grundlagen von Agenten\n",
    "\n",
    "Agenten sind KI-Systeme, die selbstständig Entscheidungen treffen und Aktionen ausführen können. Sie kombinieren LLMs mit Tools und einem Entscheidungsprozess, um komplexe Aufgaben zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytz in /opt/tljh/user/lib/python3.10/site-packages (2024.1)\n",
      "Requirement already satisfied: tzdata in /opt/tljh/user/lib/python3.10/site-packages (2024.1)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytz tzdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I should first get the current time in Berlin and then use a calculator to calculate 15 to the power of 0.5.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Berlin'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Berlin'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Paris'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Paris'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/London'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/London'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Moscow'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Moscow'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Rome'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Rome'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Berlin'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Berlin'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Amsterdam'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Amsterdam'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Brussels'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Brussels'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Berlin'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Berlin'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Paris'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Paris'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/London'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/London'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Moscow'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Moscow'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Rome'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Rome'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Berlin'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Berlin'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I should try using a different timezone.\n",
      "Action: aktuelle_zeit\n",
      "Action Input: timezone = 'Europe/Amsterdam'\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mFehler: \"No time zone found with key timezone = 'Europe/Amsterdam'\"\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Wie lautet die aktuelle Uhrzeit in Berlin und berechne dann 15 hoch 0.5?',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# Einfachen LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Einige einfache Tools definieren\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Eigenes Tool erstellen\n",
    "@tool\n",
    "def aktuelle_zeit(timezone: str = \"Europe/Berlin\") -> str:\n",
    "    \"\"\"Gibt die aktuelle Uhrzeit in der angegebenen Zeitzone zurück.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from zoneinfo import ZoneInfo\n",
    "    try:\n",
    "        return datetime.now(ZoneInfo(timezone)).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    except Exception as e:\n",
    "        return f\"Fehler: {str(e)}\"\n",
    "\n",
    "# Tool zur Tool-Liste hinzufügen\n",
    "tools.append(aktuelle_zeit)\n",
    "\n",
    "# Einfachen Agenten erstellen\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agenten ausführen\n",
    "agent.invoke(\"Wie lautet die aktuelle Uhrzeit in Berlin und berechne dann 15 hoch 0.5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile agentenbasierter Ansätze\n",
    "\n",
    "- **Autonomie**: Agenten können eigenständig Entscheidungen treffen und Aktionen ausführen\n",
    "- **Flexibilität**: Können mit vielen verschiedenen Tools arbeiten\n",
    "- **Komplexität**: Können mehrstufige Probleme in Teilschritte zerlegen\n",
    "- **Erweiterbarkeit**: Leichte Integration neuer Tools und Funktionen\n",
    "\n",
    "![Agent Architektur](https://upload.wikimedia.org/wikipedia/commons/2/21/Agent_based_modelling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ReAct Pattern: Reasoning and Acting in LLMs\n",
    "\n",
    "Das ReAct-Muster kombiniert Reasoning (Denken) und Acting (Handeln) in einem iterativen Prozess.\n",
    "\n",
    "### Komponenten des ReAct-Musters:\n",
    "\n",
    "1. **Thought (Gedanke)**: Das LLM überlegt, wie es vorgehen soll\n",
    "2. **Action (Aktion)**: Das LLM wählt eine Aktion/Tool aus und führt sie aus\n",
    "3. **Observation (Beobachtung)**: Das LLM erhält die Ergebnisse der Aktion\n",
    "4. **Wiederholen**: Bis die Aufgabe gelöst ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I should use the search engine to find information about Angela Merkel's age.\n",
      "Action: Search\n",
      "Action Input: \"Angela Merkel age\"\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3m70 years\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m Now I can use the calculator to square this number.\n",
      "Action: Calculator\n",
      "Action Input: 70\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 70\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer.\n",
      "Final Answer: 4900\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Wie alt ist Angela Merkel und was ist diese Zahl quadriert?',\n",
       " 'output': '4900'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementierung des ReAct-Musters mit LangChain\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Tools laden (wir nutzen serpapi für Web-Suche und llm-math für Berechnungen)\n",
    "# Hinweis: Sie benötigen einen SERPAPI-Schlüssel in der .env-Datei\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# ReAct-Agent initialisieren\n",
    "react_agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Dies ist der ReAct-Agent\n",
    "    verbose=True  # Ausführliche Ausgabe, um den Denkprozess zu sehen\n",
    ")\n",
    "\n",
    "# Komplexe Frage stellen, die mehrere Schritte erfordert\n",
    "react_agent.invoke(\n",
    "    \"Wie alt ist Angela Merkel und was ist diese Zahl quadriert?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise des ReAct-Musters\n",
    "\n",
    "1. Das LLM wird mit einer **promt template** angewiesen, seine Gedanken zu verbalisieren\n",
    "2. Der Agent formuliert einen **Gedanken** darüber, wie er die Aufgabe angehen sollte\n",
    "3. Der Agent wählt eine **Aktion** und ein Tool aus der verfügbaren Tool-Liste\n",
    "4. Das Tool wird ausgeführt und liefert eine **Beobachtung** zurück\n",
    "5. Der Agent überlegt, basierend auf der Beobachtung, was als nächstes zu tun ist\n",
    "6. Dieser Prozess wiederholt sich, bis der Agent glaubt, die Aufgabe gelöst zu haben\n",
    "\n",
    "### Vorteile des ReAct-Musters:\n",
    "\n",
    "- **Transparenz**: Der Denkprozess des Agenten ist sichtbar\n",
    "- **Bessere Entscheidungen**: Durch explizites Nachdenken werden bessere Entscheidungen getroffen\n",
    "- **Selbstkorrektur**: Der Agent kann Fehler erkennen und korrigieren\n",
    "- **Systematisches Vorgehen**: Komplexe Probleme werden strukturiert gelöst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph-RAG und Hierarchical RAG\n",
    "\n",
    "RAG (Retrieval Augmented Generation) verbessert LLM-Antworten durch die Einbindung externer Informationen. Graph-RAG und Hierarchical RAG sind fortgeschrittene Architekturen, die die Leistung von RAG-Systemen verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-RAG\n",
    "\n",
    "Graph-RAG nutzt Graphstrukturen, um Beziehungen zwischen Dokumenten und Konzepten zu modellieren.\n",
    "\n",
    "**Schlüsselkonzepte:**\n",
    "- Dokumente und Informationen werden als Knoten in einem Graphen dargestellt\n",
    "- Beziehungen zwischen Informationen werden als Kanten modelliert\n",
    "- Der Graph ermöglicht kontextreicheres Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: Die Hauptstadt von Deutschland ist Berlin.\n"
     ]
    }
   ],
   "source": [
    "# Einfaches Beispiel für Graph-RAG mit LangGraph\n",
    "from typing import Annotated, TypedDict, List, Dict, Any\n",
    "from langgraph.graph import StateGraph\n",
    "import operator\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Definieren eines einfachen Zustands für unseren Graphen\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "\n",
    "# Embeddings initialisieren\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Beispieldaten für die Vektordatenbank\n",
    "sample_texts = [\n",
    "    \"Berlin ist die Hauptstadt von Deutschland.\",\n",
    "    \"München ist die Hauptstadt von Bayern.\",\n",
    "    \"Hamburg ist die zweitgrößte Stadt Deutschlands.\",\n",
    "    \"Frankfurt ist ein wichtiges Finanzzentrum in Europa.\",\n",
    "    \"Köln ist bekannt für seinen Dom.\"\n",
    "]\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_texts(sample_texts, embeddings)\n",
    "\n",
    "# Knoten-Funktionen definieren\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"Dokumente aus der Vektordatenbank abrufen\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    # Top 2 relevante Dokumente abrufen\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    return {\"context\": [doc.page_content for doc in docs]}\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Antwort basierend auf dem Kontext generieren\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Du bist ein hilfreicher Assistent. \n",
    "        Verwende den folgenden Kontext, um die Frage zu beantworten.\n",
    "        \n",
    "        Kontext: {context}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": \"\\n\".join(context), \"query\": query})\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Graph erstellen\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Knoten hinzufügen\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# Kanten definieren\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.set_finish_point(\"generate\")\n",
    "\n",
    "# Graph kompilieren\n",
    "chain = graph.compile()\n",
    "\n",
    "# Graph testen\n",
    "result = chain.invoke({\"query\": \"Was ist die Hauptstadt von Deutschland?\", \"context\": [], \"answer\": \"\"})\n",
    "print(\"Antwort:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical RAG\n",
    "\n",
    "Hierarchical RAG organisiert Informationen in Hierarchien, um den Suchraum effizient einzugrenzen.\n",
    "\n",
    "**Schlüsselkonzepte:**\n",
    "- Informationen werden in verschiedenen Ebenen organisiert\n",
    "- Die Suche beginnt auf einer hohen Ebene und verfeinert sich schrittweise\n",
    "- Ermöglicht effizientere Informationsextraktion bei großen Dokumentenmengen\n",
    "\n",
    "#### Beispielarchitektur für Hierarchical RAG:\n",
    "\n",
    "1. **Oberste Ebene**: Dokumenttitel und Zusammenfassungen\n",
    "2. **Mittlere Ebene**: Abschnitte und Kapitel\n",
    "3. **Unterste Ebene**: Detaillierte Textpassagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgewähltes Dokument Level 1: doc1\n",
      "Ausgewählter Abschnitt Level 2: doc1_section1\n",
      "\n",
      "Antwort: Berlin hat rund 3,7 Millionen Einwohner.\n"
     ]
    }
   ],
   "source": [
    "# Konzeptionelles Beispiel für Hierarchical RAG (vereinfacht)\n",
    "from typing import List, Dict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Vereinfachte Dokument-Hierarchie\n",
    "document_hierarchy = {\n",
    "    \"level1\": [\n",
    "        {\"id\": \"doc1\", \"summary\": \"Deutschlands Großstädte und ihre Bedeutung\"},\n",
    "        {\"id\": \"doc2\", \"summary\": \"Europäische Hauptstädte im Vergleich\"}\n",
    "    ],\n",
    "    \"level2\": {\n",
    "        \"doc1\": [\n",
    "            {\"id\": \"doc1_section1\", \"title\": \"Berlin als Hauptstadt\"},\n",
    "            {\"id\": \"doc1_section2\", \"title\": \"Hamburg als Handelsmetropole\"}\n",
    "        ],\n",
    "        \"doc2\": [\n",
    "            {\"id\": \"doc2_section1\", \"title\": \"Berlin im europäischen Kontext\"},\n",
    "            {\"id\": \"doc2_section2\", \"title\": \"Paris als Kulturzentrum\"}\n",
    "        ]\n",
    "    },\n",
    "    \"level3\": {\n",
    "        \"doc1_section1\": \"Berlin ist die Hauptstadt und bevölkerungsreichste Stadt Deutschlands. Mit rund 3,7 Millionen Einwohnern ist Berlin auch die größte Stadt der Europäischen Union.\",\n",
    "        \"doc1_section2\": \"Hamburg ist mit 1,8 Millionen Einwohnern die zweitgrößte Stadt Deutschlands und ein bedeutendes Wirtschafts- und Handelszentrum in Nordeuropa.\",\n",
    "        \"doc2_section1\": \"Im Vergleich zu anderen europäischen Hauptstädten hat Berlin eine besondere Geschichte aufgrund der deutschen Teilung im 20. Jahrhundert.\",\n",
    "        \"doc2_section2\": \"Paris, die Hauptstadt Frankreichs, gilt als eines der wichtigsten Kulturzentren Europas mit berühmten Museen wie dem Louvre und dem Centre Pompidou.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def hierarchical_search(query: str) -> str:\n",
    "    \"\"\"Führt eine hierarchische Suche durch\"\"\"\n",
    "    \n",
    "    # 1. Schritt: Auswahl relevanter Dokumente auf Level 1\n",
    "    level1_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Dokumentzusammenfassungen:\n",
    "        {summaries}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Dokuments zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Zusammenfassungen zusammenstellen\n",
    "    summaries = \"\\n\".join([f\"ID: {doc['id']}, Zusammenfassung: {doc['summary']}\" \n",
    "                           for doc in document_hierarchy[\"level1\"]])\n",
    "    \n",
    "    # Level 1 Auswahl treffen\n",
    "    level1_chain = level1_prompt | llm | StrOutputParser()\n",
    "    selected_doc = level1_chain.invoke({\"summaries\": summaries, \"query\": query})\n",
    "    print(f\"Ausgewähltes Dokument Level 1: {selected_doc}\")\n",
    "    \n",
    "    # 2. Schritt: Auswahl relevanter Abschnitte auf Level 2\n",
    "    level2_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Abschnitte aus dem Dokument {doc_id}:\n",
    "        {sections}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Abschnitts zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Abschnitte zusammenstellen\n",
    "    sections = \"\\n\".join([f\"ID: {section['id']}, Titel: {section['title']}\" \n",
    "                          for section in document_hierarchy[\"level2\"].get(selected_doc.strip(), [])])\n",
    "    \n",
    "    # Level 2 Auswahl treffen\n",
    "    level2_chain = level2_prompt | llm | StrOutputParser()\n",
    "    selected_section = level2_chain.invoke({\"doc_id\": selected_doc, \"sections\": sections, \"query\": query})\n",
    "    print(f\"Ausgewählter Abschnitt Level 2: {selected_section}\")\n",
    "    \n",
    "    # 3. Schritt: Detailtext abrufen und Antwort generieren\n",
    "    detail_text = document_hierarchy[\"level3\"].get(selected_section.strip(), \"Keine Informationen gefunden.\")\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Basierend auf dem folgenden Text, beantworte die Frage.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \n",
    "        Antwort:\"\"\"\n",
    "    )\n",
    "    \n",
    "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\"text\": detail_text, \"query\": query})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Testen der hierarchischen Suche\n",
    "result = hierarchical_search(\"Wie viele Einwohner hat Berlin?\")\n",
    "print(\"\\nAntwort:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir drei wichtige Architekturkonzepte für LLM-Anwendungen kennengelernt:\n",
    "\n",
    "1. **Agentenbasierte Ansätze**:\n",
    "   - Agenten kombinieren LLMs mit Tools für komplexe Aufgaben\n",
    "   - Sie bieten Autonomie, Flexibilität und einfache Erweiterbarkeit\n",
    "\n",
    "2. **ReAct Pattern**:\n",
    "   - Kombiniert Reasoning und Acting in einem iterativen Prozess\n",
    "   - Verbessert die Transparenz und Qualität von Entscheidungen\n",
    "   - Ermöglicht systematisches Problemlösen\n",
    "\n",
    "3. **Graph-RAG und Hierarchical RAG**:\n",
    "   - Graph-RAG nutzt Beziehungen zwischen Informationen\n",
    "   - Hierarchical RAG organisiert Wissen in verschiedenen Abstraktionsebenen\n",
    "   - Beide verbessern die Effektivität von RAG-Systemen\n",
    "\n",
    "Diese Konzepte können kombiniert werden, um leistungsfähige KI-Systeme zu entwickeln, die komplexe Aufgaben effizient lösen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übungen\n",
    "\n",
    "1. Erweitern Sie den ReAct-Agenten um ein eigenes Tool (z.B. Wetter-API oder Währungsumrechnung)\n",
    "2. Modifizieren Sie den Graph-RAG-Ansatz, um einen Feedback-Knoten einzubauen, der die Antwort verbessert\n",
    "3. Entwerfen Sie eine hierarchische RAG-Struktur für ein spezielles Fachgebiet (z.B. Medizin oder Rechtswesen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
