{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "Ein Dataset mit QA Pairs erstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"QA Jungle Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"Was ist ein Jaguar?\"},\n",
    "        {\"question\": \"Was ist ein Tukan?\"},\n",
    "        {\"question\": \"Was ist ein Faultier?\"},\n",
    "        {\"question\": \"Was ist ein Anakonda?\"},\n",
    "        {\"question\": \"Was ist ein Ozelot?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"Ein großes Raubtier, das in den tropischen Wäldern Amerikas lebt\"},\n",
    "        {\"answer\": \"Ein bunter Vogel mit einem großen Schnabel, der im Regenwald lebt\"},\n",
    "        {\"answer\": \"Ein langsames Säugetier, das die meiste Zeit an Bäumen hängt\"},\n",
    "        {\"answer\": \"Eine riesige Schlange, die im Amazonas-Regenwald lebt\"},\n",
    "        {\n",
    "            \"answer\": \"Ein Axolotl ist ein neotenes Amphibium, das seine larvalen Merkmale wie Kiemen auch im erwachsenen Stadium behält und in Mexiko beheimatet ist.\"\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes erstellen wir einen Evaluator der generierte Antworten mit den Antworten aus dem Dataset vergleicht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "eval_llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.0)\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "    \"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt erstellen wir die Funktion, die getestet werden soll. Im Beispiel ein einfacher LLM Aufruf mit einer einzelnen Frage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def my_app(question):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    prompt = SystemMessage(\n",
    "        content=\"Antworte auf die Frage des Nutzer in einer kurzen, präzisen Art (ein Satz).\"\n",
    "    )\n",
    "\n",
    "    result = llm.invoke(\n",
    "        [\n",
    "            prompt,\n",
    "            HumanMessage(content=question),\n",
    "        ]\n",
    "    )\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def langsmith_app(inputs):\n",
    "    output = my_app(inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssen wir das Experiment nur noch ausführen und schon können wir uns die Ergebnisse in Langsmith ansehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'jungle_questions-8ffefffe' at:\n",
      "https://smith.langchain.com/o/eb86f40f-e90c-4eb8-846e-94f09f6daf2e/datasets/820274a3-9cd3-4a56-b956-cf9ee10baa34/compare?selectedSessions=5868391d-9964-4360-9ab6-68d3b3d1a774\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001e943ebcd3492f8a34375870329175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evaluator],\n",
    "    experiment_prefix=\"jungle_questions\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
