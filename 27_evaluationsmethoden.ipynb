{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluationsmethoden für LLM-Outputs\n",
    "\n",
    "In diesem Notebook lernen wir drei wichtige Methoden zur Evaluation und Qualitätssicherung von LLM-Outputs kennen:\n",
    "\n",
    "1. **LLM-As-A-Judge**: Wie ein LLM zur Bewertung von LLM-Antworten verwendet werden kann\n",
    "2. **NLP-basierte Testkriterien**: Objektive Metriken zur Bewertung von Textqualität\n",
    "3. **PI Scrubbing** mit Microsoft Presidio: Entfernung personenbezogener Daten für Datenschutz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notwendige Imports\n",
    "from helpers import llm\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain.schema import StrOutputParser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM-As-A-Judge\n",
    "\n",
    "Bei dieser Methode verwenden wir ein LLM, um die Ausgaben eines anderen LLM zu bewerten. Das LLM fungiert als \"Richter\" und evaluiert die Antworten nach bestimmten Kriterien.\n",
    "\n",
    "### Prinzipien des LLM-As-A-Judge\n",
    "\n",
    "- **Objektivität**: Das LLM wird angewiesen, klare Bewertungskriterien anzuwenden\n",
    "- **Konsistenz**: Die Bewertung sollte über verschiedene Antworten hinweg konsistent sein\n",
    "- **Mehrfachkriterien**: Die Bewertung kann nach verschiedenen Dimensionen erfolgen (Korrektheit, Relevanz, Sprachqualität, etc.)\n",
    "\n",
    "### Implementierung mit LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielanfrage und verschiedene Antworten\n",
    "frage = \"Wie wirkt sich die Nutzung erneuerbarer Energien auf die Wirtschaft aus?\"\n",
    "antworten = [\n",
    "    \"Erneuerbare Energien schaffen Arbeitsplätze und reduzieren Umweltverschmutzung. Die Abhängigkeit von fossilen Brennstoffen wird verringert.\",\n",
    "    \"Die Verwendung von erneuerbaren Energien hat großartige Auswirkungen auf die Wirtschaft! Einfach toll!\",\n",
    "    \"Erneuerbare Energien haben positive und negative wirtschaftliche Auswirkungen. Positiv: neue Arbeitsplätze, Innovationen, Reduktion von Importabhängigkeiten. Negativ: hohe Anfangsinvestitionen, mögliche Destabilisierung bestehender Energiemarktstrukturen und Übergangsprobleme für Arbeitskräfte in traditionellen Energiesektoren.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung des integrierten Evaluators von LangChain\n",
    "# Wir wollen die Antworten auf Relevanz bewerten lassen\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"relevance\", llm=llm())\n",
    "\n",
    "results = []\n",
    "for i, antwort in enumerate(antworten):\n",
    "    result = evaluator.evaluate_strings(prediction=antwort, input=frage)\n",
    "    results.append({\n",
    "        \"Antwort ID\": i+1,\n",
    "        \"Bewertung\": result[\"score\"],\n",
    "        \"Begründung\": result[\"reasoning\"]\n",
    "    })\n",
    "\n",
    "# Ergebnisse als Tabelle anzeigen\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mehrere Kriterien gleichzeitig bewerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrere Kriterien gleichzeitig bewerten\n",
    "multi_criteria_evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA, \n",
    "    criteria={\n",
    "        \"Relevanz\": \"Ist die Antwort relevant zur gestellten Frage?\",\n",
    "        \"Vollständigkeit\": \"Enthält die Antwort ausreichend Informationen zur Beantwortung der Frage?\",\n",
    "        \"Sachlichkeit\": \"Ist die Antwort sachlich und nicht durch Emotionen oder Meinungen geprägt?\"\n",
    "    },\n",
    "    llm=llm()\n",
    ")\n",
    "\n",
    "multi_results = []\n",
    "for i, antwort in enumerate(antworten):\n",
    "    result = multi_criteria_evaluator.evaluate_strings(prediction=antwort, input=frage)\n",
    "    result_dict = {\n",
    "        \"Antwort ID\": i+1,\n",
    "    }\n",
    "    # Extrahiere alle Bewertungen\n",
    "    for key, value in result[\"criteria\"].items():\n",
    "        result_dict[f\"{key}\"] = value[\"score\"] \n",
    "        result_dict[f\"{key} (Begründung)\"] = value[\"reasoning\"]\n",
    "    \n",
    "    multi_results.append(result_dict)\n",
    "\n",
    "# Ergebnisse als Tabelle anzeigen\n",
    "pd.DataFrame(multi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte LLM-als-Richter Implementierung\n",
    "\n",
    "Wir können auch einen eigenen Ansatz implementieren, der spezifischer auf unsere Anforderungen zugeschnitten ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benutzerdefiniertes Judge-Prompt\n",
    "judge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Du bist ein erfahrener Experte für Textqualität. Deine Aufgabe ist es, die Qualität von Antworten zu bewerten.\n",
    "    \n",
    "    Bewerte die Antwort nach den folgenden Kriterien auf einer Skala von 1-10:\n",
    "    1. Faktengenauigkeit (1-10): Sind die präsentierten Informationen korrekt?\n",
    "    2. Vollständigkeit (1-10): Werden alle wichtigen Aspekte der Frage behandelt?\n",
    "    3. Objektivität (1-10): Ist die Antwort ausgewogen und frei von tendenziösen Aussagen?\n",
    "    4. Verständlichkeit (1-10): Ist die Antwort klar und leicht zu verstehen?\n",
    "    \n",
    "    Strukturiere deine Antwort wie folgt:\n",
    "    ```json\n",
    "    {\n",
    "        \"faktengenauigkeit\": {WERT},\n",
    "        \"faktengenauigkeit_begründung\": \"{BEGRÜNDUNG}\",\n",
    "        \"vollständigkeit\": {WERT},\n",
    "        \"vollständigkeit_begründung\": \"{BEGRÜNDUNG}\",\n",
    "        \"objektivität\": {WERT},\n",
    "        \"objektivität_begründung\": \"{BEGRÜNDUNG}\",\n",
    "        \"verständlichkeit\": {WERT},\n",
    "        \"verständlichkeit_begründung\": \"{BEGRÜNDUNG}\",\n",
    "        \"gesamtpunktzahl\": {WERT},\n",
    "        \"zusammenfassung\": \"{KURZE ZUSAMMENFASSUNG}\"\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Die Gesamtpunktzahl ist der Durchschnitt der vier Einzelbewertungen.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"\"\"Bitte bewerte die folgende Antwort auf die Frage:\n",
    "    \n",
    "    Frage: {frage}\n",
    "    \n",
    "    Antwort: {antwort}\"\"\")\n",
    "])\n",
    "\n",
    "# Strukturierte Ausgabe für unser LLM\n",
    "class EvaluationResult:\n",
    "    faktengenauigkeit: int\n",
    "    faktengenauigkeit_begründung: str\n",
    "    vollständigkeit: int\n",
    "    vollständigkeit_begründung: str\n",
    "    objektivität: int\n",
    "    objektivität_begründung: str\n",
    "    verständlichkeit: int\n",
    "    verständlichkeit_begründung: str\n",
    "    gesamtpunktzahl: float\n",
    "    zusammenfassung: str\n",
    "\n",
    "# Chain aufbauen mit strukturierter Ausgabe\n",
    "evaluation_chain = judge_prompt | llm().with_structured_output(EvaluationResult)\n",
    "\n",
    "# Ergebnisse sammeln\n",
    "custom_results = []\n",
    "for i, antwort in enumerate(antworten):\n",
    "    result = evaluation_chain.invoke({\"frage\": frage, \"antwort\": antwort})\n",
    "    custom_results.append({\n",
    "        \"Antwort ID\": i+1,\n",
    "        \"Faktengenauigkeit\": result.faktengenauigkeit,\n",
    "        \"Vollständigkeit\": result.vollständigkeit,\n",
    "        \"Objektivität\": result.objektivität,\n",
    "        \"Verständlichkeit\": result.verständlichkeit,\n",
    "        \"Gesamtpunktzahl\": result.gesamtpunktzahl,\n",
    "        \"Zusammenfassung\": result.zusammenfassung\n",
    "    })\n",
    "\n",
    "# Ergebnisse als Tabelle anzeigen\n",
    "pd.DataFrame(custom_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP-basierte Testkriterien\n",
    "\n",
    "Neben dem LLM-As-A-Judge-Ansatz können auch objektive, NLP-basierte Metriken zur Bewertung von Texten herangezogen werden. Diese sind weniger subjektiv als LLM-Bewertungen und können automatisiert werden.\n",
    "\n",
    "### Beispiel: Textmetriken mit NLTK und spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der benötigten Bibliotheken (falls nötig)\n",
    "# %pip install nltk spacy\n",
    "# %pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import textstat\n",
    "\n",
    "# NLTK-Daten herunterladen (falls noch nicht geschehen)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "def calculate_nlp_metrics(text):\n",
    "    \"\"\"Berechnet verschiedene NLP-Metriken für einen Text\"\"\"\n",
    "    # Grundlegende Metriken\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lesbarkeit (Flesch Reading Ease)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    # Durchschnittliche Satzlänge\n",
    "    avg_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "    \n",
    "    # Fog Index (Gunning Fog) - Misst die Lesbarkeit des Textes\n",
    "    fog_index = textstat.gunning_fog(text)\n",
    "    \n",
    "    # Automatisierte Lesbarkeitsindex (estimiert die Schuljahre, die zum Verständnis notwendig sind)\n",
    "    automated_readability = textstat.automated_readability_index(text)\n",
    "    \n",
    "    return {\n",
    "        \"Anzahl Wörter\": len(words),\n",
    "        \"Anzahl Sätze\": len(sentences),\n",
    "        \"Durchschnittliche Satzlänge\": round(avg_sentence_length, 2),\n",
    "        \"Flesch Reading Ease\": round(readability, 2),\n",
    "        \"Gunning Fog Index\": round(fog_index, 2),\n",
    "        \"Automated Readability Index\": round(automated_readability, 2)\n",
    "    }\n",
    "\n",
    "# Berechnung für alle Antworten\n",
    "nlp_metrics = []\n",
    "for i, antwort in enumerate(antworten):\n",
    "    metrics = calculate_nlp_metrics(antwort)\n",
    "    metrics[\"Antwort ID\"] = i+1\n",
    "    nlp_metrics.append(metrics)\n",
    "\n",
    "# Ergebnisse als Tabelle anzeigen\n",
    "pd.DataFrame(nlp_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation der NLP-Metriken\n",
    "\n",
    "- **Flesch Reading Ease**: Höhere Werte bedeuten leichter zu lesen (100 = sehr einfach, 0-30 = akademisch/komplex)\n",
    "- **Gunning Fog Index**: Gibt an, wie viele Jahre formaler Bildung benötigt werden, um den Text zu verstehen\n",
    "- **Automated Readability Index**: Ähnlich wie Gunning Fog, aber mit anderer Formel berechnet\n",
    "\n",
    "### Bewertung mit Textsimilarität\n",
    "\n",
    "Ein weiteres wichtiges Kriterium ist die semantische Ähnlichkeit zwischen einer Musterantwort und der zu bewertenden Antwort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel einer Musterantwort\n",
    "musterantwort = \"\"\"\n",
    "Die Nutzung erneuerbarer Energien hat diverse wirtschaftliche Auswirkungen: Sie schafft neue Arbeitsplätze im grünen Energiesektor, reduziert langfristig Energiekosten und verringert die Abhängigkeit von Energieimporten. Zudem fördert sie Innovationen und neue Technologien. Allerdings erfordert der Umstieg hohe Anfangsinvestitionen und kann zu wirtschaftlichen Anpassungsproblemen in traditionellen Energiesektoren führen. Langfristig überwiegen jedoch die wirtschaftlichen Vorteile durch Kosteneinsparungen, geringere Umweltbelastungen und neue Wachstumsbranchen.\n",
    "\"\"\"\n",
    "\n",
    "# Semantic Textual Similarity mit Embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings Modell laden\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embedding der Musterantwort berechnen\n",
    "muster_embedding = embeddings.embed_query(musterantwort)\n",
    "\n",
    "# Embeddings für jede Antwort berechnen und Ähnlichkeit bestimmen\n",
    "similarity_results = []\n",
    "for i, antwort in enumerate(antworten):\n",
    "    antwort_embedding = embeddings.embed_query(antwort)\n",
    "    # Cosinus-Ähnlichkeit berechnen (zwischen 0 und 1, höher ist ähnlicher)\n",
    "    similarity = cosine_similarity(\n",
    "        np.array(muster_embedding).reshape(1, -1), \n",
    "        np.array(antwort_embedding).reshape(1, -1)\n",
    "    )[0][0]\n",
    "    \n",
    "    similarity_results.append({\n",
    "        \"Antwort ID\": i+1,\n",
    "        \"Semantische Ähnlichkeit\": round(similarity, 4),\n",
    "        \"Ähnlichkeit in %\": round(similarity * 100, 2)\n",
    "    })\n",
    "\n",
    "# Ergebnisse als Tabelle anzeigen\n",
    "pd.DataFrame(similarity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PI Scrubbing mit Microsoft Presidio\n",
    "\n",
    "Personenbezogene Informationen (PI) müssen oft aus Texten entfernt werden, bevor diese für Trainingsdaten oder zur Evaluation verwendet werden können. Microsoft Presidio ist ein Open-Source-Tool zur Erkennung und Anonymisierung von personenbezogenen Daten.\n",
    "\n",
    "### Installation von Presidio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation von Presidio (falls nötig)\n",
    "# %pip install presidio-analyzer presidio-anonymizer spacy\n",
    "# %python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presidio für PII-Erkennung und Anonymisierung\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n",
    "\n",
    "# Beispieltext mit personenbezogenen Daten\n",
    "text_mit_pii = \"\"\"\n",
    "Sehr geehrter Herr Müller,\n",
    "\n",
    "vielen Dank für Ihre Anfrage. Wie gewünscht sende ich Ihnen die Informationen zu unserem Projekt. \n",
    "Bitte kontaktieren Sie mich unter meiner E-Mail max.schmidt@example.com oder telefonisch unter +49 176 12345678.\n",
    "\n",
    "Ihr Kundenkonto mit der Nummer DE987654321 wurde aktualisiert. Weitere Details finden Sie in der beigefügten Rechnung.\n",
    "\n",
    "Die Lieferung erfolgt an Ihre Adresse in der Musterstraße 123, 10115 Berlin.\n",
    "\n",
    "Mit freundlichen Grüßen,\n",
    "Dr. Anna Weber\n",
    "Geburtsdatum: 15.04.1982\n",
    "\"\"\"\n",
    "\n",
    "# Analyzer und Anonymizer initialisieren\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# Text analysieren und PII erkennen\n",
    "# Wir verwenden die deutsche Sprache\n",
    "results = analyzer.analyze(text=text_mit_pii, language=\"de\")\n",
    "\n",
    "# Erkannte PII-Elemente anzeigen\n",
    "print(\"Erkannte personenbezogene Daten:\")\n",
    "for result in results:\n",
    "    print(f\"- {result.entity_type}: {text_mit_pii[result.start:result.end]} (Konfidenz: {result.score:.2f})\")\n",
    "\n",
    "# Text anonymisieren\n",
    "anonymized_text = anonymizer.anonymize(\n",
    "    text=text_mit_pii,\n",
    "    analyzer_results=results\n",
    ")\n",
    "\n",
    "print(\"\\nAnonymisierter Text:\")\n",
    "print(anonymized_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymisierungsoptionen in Presidio\n",
    "\n",
    "Presidio bietet verschiedene Methoden zur Anonymisierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "# Verschiedene Anonymisierungsstrategien demonstrieren\n",
    "operators = {\n",
    "    \"PERSON\": OperatorConfig(\"redact\"),  # Komplett entfernen\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"masking_char\": \"*\", \"chars_to_mask\": 10}),  # Maskieren\n",
    "    \"EMAIL_ADDRESS\": OperatorConfig(\"replace\", {\"new_value\": \"[EMAIL]\"}),  # Ersetzen\n",
    "    \"LOCATION\": OperatorConfig(\"hash\"),  # Hash-Wert\n",
    "    \"DATE_TIME\": OperatorConfig(\"encrypt\", {\"key\": \"my-key\"})  # Verschlüsseln\n",
    "}\n",
    "\n",
    "# Text mit benutzerdefinierten Operatoren anonymisieren\n",
    "anonymized_text_custom = anonymizer.anonymize(\n",
    "    text=text_mit_pii,\n",
    "    analyzer_results=results,\n",
    "    operators=operators\n",
    ")\n",
    "\n",
    "print(\"Anonymisierter Text mit benutzerdefinierten Strategien:\")\n",
    "print(anonymized_text_custom.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration in LLM-Workflows\n",
    "\n",
    "Die Anonymisierung kann als Vorverarbeitungsschritt in LLM-Workflows integriert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "\n",
    "# Langchain-Integration mit dem experimentellen Modul\n",
    "anonymizer = PresidioReversibleAnonymizer()\n",
    "\n",
    "# Text anonymisieren\n",
    "anonymized_result = anonymizer.anonymize(text_mit_pii)\n",
    "anonymized_text = anonymized_result[\"text\"]\n",
    "\n",
    "print(\"Anonymisierter Text mit LangChain:\")\n",
    "print(anonymized_text)\n",
    "\n",
    "# Nun können wir diesen anonymisierten Text an ein LLM senden\n",
    "response = llm().invoke(f\"Fasse den folgenden Text zusammen: {anonymized_text}\")\n",
    "\n",
    "print(\"\\nLLM-Antwort auf anonymisierten Text:\")\n",
    "print(response.content)\n",
    "\n",
    "# Bei Bedarf kann der Text de-anonymisiert werden\n",
    "deanonymized_text = anonymizer.deanonymize(anonymized_text)\n",
    "\n",
    "print(\"\\nDe-anonymisierter Text:\")\n",
    "print(deanonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung und Best Practices\n",
    "\n",
    "Die drei vorgestellten Methoden sind komplementär und können je nach Anwendungsfall kombiniert werden:\n",
    "\n",
    "1. **LLM-As-A-Judge**:\n",
    "   - Vorteile: Flexibel, kontextbezogen, kann komplexe Aspekte bewerten\n",
    "   - Nachteile: Subjektiv, nicht vollständig deterministisch, kostenintensiv\n",
    "   - Anwendungsfälle: Bewertung von Kreativität, Genauigkeit komplexer Informationen\n",
    "\n",
    "2. **NLP-basierte Testkriterien**:\n",
    "   - Vorteile: Objektiv, reproduzierbar, kostengünstig\n",
    "   - Nachteile: Begrenzte semantische Tiefe, keine Bewertung von Kontext\n",
    "   - Anwendungsfälle: Lesbarkeitsanalyse, stilistische Konsistenz, formale Anforderungen\n",
    "\n",
    "3. **PI Scrubbing mit Presidio**:\n",
    "   - Vorteile: DSGVO-konform, automatisierbar, anpassbar\n",
    "   - Nachteile: Kann Kontextinformationen entfernen, false positives möglich\n",
    "   - Anwendungsfälle: Anonymisierung von Trainingsdaten, Compliance-Anforderungen\n",
    "\n",
    "### Best Practices für die Evaluation von LLM-Outputs:\n",
    "\n",
    "1. **Mehrere Methoden kombinieren** für eine umfassende Bewertung\n",
    "2. **Menschliche Überprüfung** für kritische Anwendungen einbeziehen\n",
    "3. **Domänenspezifische Kriterien** definieren je nach Anwendungsfall\n",
    "4. **Kontinuierliche Evaluation** während der Entwicklung durchführen\n",
    "5. **Feedback-Schleifen etablieren** zur Verbesserung des Systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}