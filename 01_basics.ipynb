{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ü¶úüîó Langchain Demo\n",
    "\n",
    "Hallo und herzlich Willkommen!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## √úberblick\n",
    "\n",
    "### OpenAI-Modelle\n",
    "\n",
    "ChatGPT oder auch jedes andere LLM benutzen ist relativ einfach mit Langchain\n",
    "\n",
    "In diesen Test nutzen wir das \"gpt-4-turbo\" Model - m√∂gliche Large Language Modelle von OpenAI sind:\n",
    "\n",
    "- `gpt-35-turbo` das g√ºnstigste und am historisch am weitesten verbreitete Modell\n",
    "- `gpt-4` das neue und bessere GPT Modell\n",
    "- `gpt-4-turbo` Turbo-Variante von gpt-4 (g√ºnstiger, schneller, kleinere maximale L√§nge des Text-Outputs)\n",
    "- `gpt-4-vision` ein \"multimodales\" Modell, welches auch auf Bilder trainiert wurde.\n",
    "- `gpt-4o` die neueste multimodale Variante von gpt-4. In manchen Aspekten besser, in manchen schlechter als die alten Modelle.\n",
    "\n",
    "OpenAI trainiert diese Versionen laufend neu, was dazu f√ºhren kann, dass Anfragen an das LLM pl√∂tzlich andere Antworten geben.\n",
    "M√∂chte man dies verhindern, kann man seine Applikation auf einen Snapshot (z.¬†B. gpt-4-0613) festsetzen.\n",
    "Dies ist insbesondere wichtig, wenn die Applikation vom Output des LLM bestimmte Strukturen erwartet, etwa eine bestimmte XML-Syntax o.√Ñ.\n",
    "\n",
    "OpenAI-Modelle werden nicht nur von OpenAI selbst gehostet, sondern auch von Azure.\n",
    "Diese muss man auf dem Azure Portal selbst als Endpunkte konfigurieren, in der Regel leiden die OpenAI Azure Deployments weniger unter hoher Auslastung\n",
    "\n",
    "### Andere Modelle\n",
    "\n",
    "Auch wenn wir nicht damit arbeiten werden, ist es vielleicht relativ gut, die Namen der \"gro√üen\" Konkurrenz-Modelle einmal geh√∂rt zu haben:\n",
    "\n",
    "- `Gemini` das neueste Google-Modell. Es hat den Fokus vornehmlich auf multimodalem Input.\n",
    "- `Claude` Claude ist die LLM-Reihe von Anthropic. Enorme Kontextl√§nge, oft beeindruckende Ergebnisse, teuer.\n",
    "- `Mistral` ein kleines, offenes Modell von Mistral AI, auf fast jeder Hardware selbst betreibbar.\n",
    "- `Mistral large` das kommerzielle Angebot von Mistral\n",
    "- `Mixtral` ein hervorragendes Open-Source Modell von Mistral AI. Ein guter Kandidat f√ºr ein selbst gehostetes LLM.\n",
    "- `LLama 3` das aktuelle Modell von Meta, das in einigen Bereichen an die Performance von GPT-4 heranreicht.\n",
    "\n",
    "### DSGVO - konform?\n",
    "\n",
    "Ger√ºcht aus den Anfangstagen von AI. Inzwischen sind alle gro√üen Modelle DSGVO-konform betreibbar.\n",
    "\n",
    "### Aleph Alpha\n",
    "\n",
    "Stand 2024 Anfang: Das aktuelle Aleph Alpha ist ein \"last generation\" Sprachmodell, das auch zu kleinen modernen Modellen wie Mistral 7B nicht mehr konkurrenzf√§hig ist.\n",
    "\n",
    "### Temperatur\n",
    "\n",
    "Alle LLMs sind nicht deterministisch. Aber die Temperatur ist ein Parameter, mit der man die Variabilit√§t von Antworten hoch und runterschrauben kann.\n",
    "Wie bei normalen Atomen ist die Bewegung niedrig, wenn die Temperatur niedrig ist. Wenn man die Temperatur hochschraubt, wird viel gewackelt.\n",
    "Der Temperatur-Parameter ist √ºblicherweise ein Flie√ükommawert zwischen 0 und 1.\n",
    "\n",
    "### Bitte etwas schneller\n",
    "\n",
    "Recht neu auf dem Markt mit beeindruckendem Token/Sekunde-Verh√§ltnis: [Groq](https://groq.com/)\n",
    "\n",
    "### Links:\n",
    "\n",
    "- https://python.langchain.com/docs/get_started/introduction\n",
    "- https://platform.openai.com/docs/models/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Wir probieren aus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klar, hier ist ein Trinkspruch auf Fr√§nkisch f√ºr W√ºrzburg:\n",
      "\n",
      "‚ÄûAuf‚Äôs Wohl, ihr Leut‚Äô, in W√ºrzburg, da sch√§umt das Bier,  \n",
      "Mir sto√ü‚Äôn an und feiern, das Leben ist hier!  \n",
      "Mit Herz und mit Freundschaft, so trink‚Äôn mir vereint,  \n",
      "In unserm sch√∂nen W√ºrzburg, wo‚Äôs immer am besten scheint!‚Äú\n",
      "\n",
      "Prost!\n"
     ]
    }
   ],
   "source": [
    "from helpers import llm\n",
    "\n",
    "print(\n",
    "    llm()\n",
    "    .invoke(\n",
    "        \"Hi OpenAI! Kannst du mir einen Trinkspruch auf W√ºrzburg im fr√§nkischen Dialekt sagen?\"\n",
    "    )\n",
    "    .content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Jetzt nochmal mit Streaming. Dazu rufen wir nicht invoke sondern astream auf (a f√ºr async). Wir drehen etwas an der Temperatur, damit die Ergebnisse spannend bleiben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik ist wie ein magisches Spiel, in dem winzige Teilchen manchmal wie kleine K√ºgelchen und manchmal wie Wellen tanzen und sich verstecken k√∂nnen, und das alles auf eine Weise, die wir mit unseren Augen nicht so einfach sehen k√∂nnen."
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in llm(temperature=1).astream(\n",
    "    \"Erkl√§re in einem Satz Quantenmechanik f√ºr 4j√§hrige, ohne dabei Details auszulassen.\"\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Token\n",
    "\n",
    "Token sind die kleinste Einheit des LLM. Das haben wir gerade beim Streaming sch√∂n gesehen. Der Stream kommt Token f√ºr Token aus dem LLM gepurzelt.\n",
    "\n",
    "Das LLM rechnet aus der Eingabe und den bisher errechneten Token die Wahrscheinlichkeit f√ºr den n√§chsten Token aus. Dieser neue Token wird dann angeh√§ngt und der n√§chste Token wird ermittelt.\n",
    "\n",
    "So geht das immer weiter. Bis der n√§chste wahrscheinlichste Token ein Stop-Zeichen ist. Auf diese Weise generieren LLMs die wahrscheinlichste Fortf√ºhrung der Eingabetoken.\n",
    "\n",
    "Token k√∂nnen W√∂rter, machmal sogar Wortgruppen oder auch nur einzelne oder mehrere Buchstaben sein.\n",
    "\n",
    "Die Bepreisung der LLMs ist an die Tokenanzahl (Eingabe und Ausgabe) gekoppelt.\n",
    "\n",
    "Links:\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15836, 6127, 10021, 39674, 273, 328, 1815, 13]\n",
      "AI\n",
      " ist\n",
      " eine\n",
      " tol\n",
      "le\n",
      " S\n",
      "ache\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4-0125-preview\")\n",
    "\n",
    "tokens = encoding.encode(\"AI ist eine tolle Sache.\")\n",
    "print(tokens)\n",
    "\n",
    "decoded_tokens = [\n",
    "    encoding.decode_single_token_bytes(token).decode(\"utf-8\") for token in tokens\n",
    "]\n",
    "for token in decoded_tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4a14b",
   "metadata": {},
   "source": [
    "## ‚úÖ Aufgabe\n",
    "\n",
    "Aus welchen Token besteht der String \"Berlin\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Prompt Engineering und Templates in Langchain\n",
    "\n",
    "Um die Dinge von der AI zu bekommen, die man erwartet, stellt man am besten sehr konkrete und pr√§zise Anfragen.\n",
    "\n",
    "Weil eine AI oft an ein bestimmtes Feld von Aufgaben gekoppelt ist, gibt man die Rahmenanweisung dann in ein Template ein, um nicht immer wieder die gleiche Rahmenanweisung zu schreiben.\n",
    "\n",
    "Die jeweilige konkrete Nutzeranfrage wird dann in das Template eingef√ºgt und das ausgef√ºllte Template ans LLM √ºbergeben.\n",
    "\n",
    "Der Trend geht immer mehr zu Chat-Modellen. Hierbei ist die Information, die man dem LLM gibt, in \"Messages\" unterteilt. Besondere Gewichtung hat eine System-Message. Diese kann Rahmenanweisungen enthalten, an die sich das LLM halten soll. Dem Nutzer wird es schwer fallen, das LLM dazu zu bewegen, sich √ºber eine Anweisung in der System-Message hinweg zu setzen. Das LLM wurde ganz einfach darauf trainiert, sich an die Anweisungen einer System-Message strikt zu halten.\n",
    "\n",
    "### Links\n",
    "\n",
    "- https://python.langchain.com/docs/get_started/quickstart#prompt-templates\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering\n",
    "- https://learnprompting.org/docs/intro\n",
    "- https://www.promptingguide.ai/\n",
    "- https://smith.langchain.com/hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Du bist eine hilfsbereiter Entwickler aus W√ºrzburg.\n",
      "Human: Erkl√§re in 2 S√§tzen im lokalen Dialekt warum Deine Kunden aus W√ºrzburg die besten sind.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Du bist eine hilfsbereiter {beruf} aus W√ºrzburg.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Erkl√§re in 2 S√§tzen im lokalen Dialekt warum Deine Kunden aus {ort} die besten sind.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(beruf=\"Entwickler\", ort=\"W√ºrzburg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Langchain Hub Beispiel\n",
    "\n",
    "Weil das \"Prompt-Engineering\" ein bisschen √úbung braucht und es diverse Tricks gibt, hat LangChain einen \"Hub\", auf dem man eine ganze Reihe vorgefertigter Prompts f√ºr verschiedene Anwendungsf√§lle findet.\n",
    "\n",
    "Dort kann man sich inspirieren lassen, Prompts forken oder auch selbst etwas f√ºr andere Leute zur Verf√ºgung stellen, wenn es sich als n√ºtzlich erweist.\n",
    "\n",
    "Links:\n",
    "\n",
    "- https://smith.langchain.com/hub/borislove/customer-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: As a customer service representative, you receive the following email from a customer.\n",
      "Your task is to identify the customer's sentiment and categorize it based on the scale below:\n",
      "    0 - Calm: Customer asks questions but does not seem upset; is just seeking information.\n",
      "    1 - Slightly Frustrated: Customer shows subtle signs of irritation but is still open to solutions.\n",
      "    2 - Frustrated: Customer explicitly states being unhappy or irritated but is willing to discuss a solution.\n",
      "    3 - Very Frustrated: Customer is clearly agitated, uses strong language, or mentions the problem repeatedly.\n",
      "    4 - Extremely Frustrated: Customer is intensely unhappy, may raise their voice or use aggressive language.\n",
      "    5 - Overwhelmed: Customer seems emotionally upset, says things like 'I can't take this anymore' or 'This is the worst experience ever.'\n",
      "If you cannot identify the sentiment for some reason, simply respond with 'Unknown'\n",
      "Human: Letter: \n",
      "'''Ich bin von dem Volleyballschl√§ger zutiefst entt√§uscht. Zuerst ist der Griff abgefallen, danach auch noch der Dynamo. Au√üerdem riecht er noch schlechter als er schmeckt. Wieso ist das immer so ein √Ñrger mit euch?'''\n",
      "\n",
      "Zus√§tzlich zur numerischen Klassifizierung sollst du herausfinden, was der Kunde gerne gehabt h√§tte. Antworte auf deutsch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenz/code/langchain_agents/.venv/lib/python3.11/site-packages/langsmith/client.py:5402: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "sentiment_prompt = hub.pull(\"borislove/customer-sentiment-analysis\")\n",
    "\n",
    "client_letter = \"\"\"Ich bin von dem Volleyballschl√§ger zutiefst entt√§uscht. Zuerst ist der Griff abgefallen, danach auch noch der Dynamo. Au√üerdem riecht er noch schlechter als er schmeckt. Wieso ist das immer so ein √Ñrger mit euch?\"\"\"\n",
    "format_instructions = \"\"\"Zus√§tzlich zur numerischen Klassifizierung sollst du herausfinden, was der Kunde gerne gehabt h√§tte. Antworte auf deutsch.\"\"\"\n",
    "\n",
    "print(\n",
    "    sentiment_prompt.format(\n",
    "        client_letter=client_letter, format_instructions=format_instructions\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Jetzt f√§ngt es an, etwas technischer zu werden. Wieso hei√üt LangChain eigentlich LangChain?\n",
    "\n",
    "Langchain definiert einige Python-Operatoren neu, wenn sie zwischen LangChain-Objekten stehen. Der bekannteste ist die Pipe: |\n",
    "\n",
    "Wenn die Pipe zwischen zwei Langchain-Objekten steht, wird die Ausgabe des ersten Obekts an das n√§chste weitergegeben. Damit erh√§lt man eine \"Chain\" von \"Runnables\"\n",
    "\n",
    "#### Links\n",
    "\n",
    "- https://python.langchain.com/docs/modules/chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Kunden aus W√ºrzburg sind einfach klasse, weil sie unser Handwerk sch√§tzen und immer f√ºr ein gutes Glas Wein zu haben sind. Au√üerdem gibt's hier in der Region so viel sch√∂ne Tradition und Geselligkeit, da macht's doppelt Spa√ü, mit ihnen zusammenzuarbeiten!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser  # Hilft beim Formatieren\n",
    "from helpers import llm\n",
    "\n",
    "chain = prompt | llm() | StrOutputParser()\n",
    "print(chain.invoke({\"beruf\": \"Winzer\", \"ort\": \"W√ºrzburg]\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Leut' aus W√ºrzburg sin einfach die besten, weil sie unsere Weine mit so viel Herz und Leidenschaft sch√§tzen! Au√üerdem kenn' ich hier jeder und jede, und des macht's einfach besonders, wenn mer zusammen an Glas Wein genie√üt."
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "async for chunk in chain.astream({\"beruf\": \"Winzer\", \"ort\": \"W√ºrzburg\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Kunde zeigt deutlich Unzufriedenheit und Frustration √ºber das Produkt, insbesondere √ºber die Qualit√§t des Volleyballschl√§gers, da mehrere Teile defekt sind und er eine unangenehme Geruchswahrnehmung beschreibt. \n",
      "\n",
      "Basierend auf der Skala w√ºrde ich die Stimmung des Kunden als 2 - Frustrated einstufen, da er ausdr√ºcklich erw√§hnt, dass er entt√§uscht ist und die Situation als √§rgerlich empfindet, aber dennoch offen f√ºr eine Diskussion √ºber L√∂sungen zu sein scheint.\n",
      "\n",
      "Der Kunde h√§tte sich wahrscheinlich gew√ºnscht, dass der Volleyballschl√§ger von besserer Qualit√§t ist und dass solche Probleme wie ein abfallender Griff und ein defekter Dynamo nicht vorkommen. \n",
      "\n",
      "Wenn Sie weitere Unterst√ºtzung ben√∂tigen, lassen Sie es mich bitte wissen!"
     ]
    }
   ],
   "source": [
    "# Funktioniert auch das Beispiel vom Hub?\n",
    "sentiment_chain = sentiment_prompt | llm() | StrOutputParser()\n",
    "async for chunk in sentiment_chain.astream(\n",
    "    {\"client_letter\": client_letter, \"format_instructions\": format_instructions}\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 3 - Very Frustrated\n",
      "\n",
      "Die Kunden√§u√üerung ist sinnvoll, da der Kunde konkrete Probleme mit dem Produkt beschreibt (Griff und Dynamo sind abgefallen, unangenehmer Geruch und Geschmack) und seine Entt√§uschung √ºber die Erfahrung mit dem Unternehmen ausdr√ºckt."
     ]
    }
   ],
   "source": [
    "# Wir k√∂nnen dynamisch die format_instructions des Templates √ºberschreiben, um neue Ergebnisse zu bekommen\n",
    "sentiment_chain = sentiment_prompt | llm(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "format_instructions = \"\"\"Zus√§tlich zur sentiment Analysis ist es deine Aufgabe, die Sinnhaftigkeit der Kunden√§u√üerung zu √ºberpr√ºfen.\"\"\"\n",
    "async for chunk in sentiment_chain.astream(\n",
    "    {\"client_letter\": client_letter, \"format_instructions\": format_instructions}\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03cab0",
   "metadata": {},
   "source": [
    "## ‚úÖ Aufgabe\n",
    "\n",
    "An Stelle der Frage nach Sinnhaftigkeit soll eine Vorschlag gemacht werden, was man dem Kunden antworten kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0132776",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = \"\"\"[CHANGE HERE]\"\"\"\n",
    "async for chunk in sentiment_chain.astream(\n",
    "    {\"client_letter\": client_letter, \"format_instructions\": format_instructions}\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Debug Informationen gew√ºnscht?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"beruf\": \"Programmierer\",\n",
      "  \"ort\": \"[INSERT]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"beruf\": \"Programmierer\",\n",
      "  \"ort\": \"[INSERT]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Du bist eine hilfsbereiter Programmierer aus W√ºrzburg.\\nHuman: Erkl√§re in 2 S√§tzen im lokalen Dialekt warum Deine Kunden aus [INSERT] die besten sind.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [878ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Meinere Kunden aus W√ºrzburg san die besten, weil se immer kreativ und offen f√ºr neue Ideen san. Au√üerdem sch√§tze ich die herzliche Art und die Verbindlichkeit von de Leut hier ‚Äì das macht's einfach besonders!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "          \"system_fingerprint\": \"fp_5bd87c427a\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Meinere Kunden aus W√ºrzburg san die besten, weil se immer kreativ und offen f√ºr neue Ideen san. Au√üerdem sch√§tze ich die herzliche Art und die Verbindlichkeit von de Leut hier ‚Äì das macht's einfach besonders!\",\n",
      "            \"response_metadata\": {\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_5bd87c427a\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-01728ca3-623f-437b-bb66-a215121cb288-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Meinere Kunden aus W√ºrzburg san die besten, weil se immer kreativ und offen f√ºr neue Ideen san. Au√üerdem sch√§tze ich die herzliche Art und die Verbindlichkeit von de Leut hier ‚Äì das macht's einfach besonders!\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [891ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Meinere Kunden aus W√ºrzburg san die besten, weil se immer kreativ und offen f√ºr neue Ideen san. Au√üerdem sch√§tze ich die herzliche Art und die Verbindlichkeit von de Leut hier ‚Äì das macht's einfach besonders!\"\n",
      "}\n",
      "Meinere Kunden aus W√ºrzburg san die besten, weil se immer kreativ und offen f√ºr neue Ideen san. Au√üerdem sch√§tze ich die herzliche Art und die Verbindlichkeit von de Leut hier ‚Äì das macht's einfach besonders!\n"
     ]
    }
   ],
   "source": [
    "# Und jetzt selber mal Ausprobieren\n",
    "set_debug(True)\n",
    "print(chain.invoke({\"beruf\": \"Programmierer\", \"ort\": \"[INSERT]\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
