{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktion mehrerer Agenten\n",
    "\n",
    "In diesem Ipython Notebook werden wir uns ansehen, wie mehrere Agenten miteinander kommunizieren können. Dadurch lassen sich Aufgaben aufteilen oder Aufgabestellungen aus verschiedenen Perspektiven betrachten.\n",
    "Wir setzen in diesem Beispiel das Framework CrewAI ein. Mit Crew AI lassen sich einfach Teams aus mehreren Agenten zusammensetzten, um diese gemeinsam an Problemstellungen arbeiten zu lasen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst Stellen wir uns ein Team von Agenten zusammen. Hierfür überlegen wir uns zuerst Namen. Zusätzlich teilen wir den Agenten Tools zu, die sie einsetzen dürfen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    \"AI visionary\": [\"tavily_search\"],\n",
    "    \"Grumpy old senior developer\": [\"arxiv\", \"tavily_search\"],\n",
    "    \"Junior Software developer\": [\"tavily_search\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir überlegen uns ein Thema, über das die Agenten diskutieren sollen.\n",
    "Mittels LLM generieren wir eine ausgearbeitete Variante des Diskussionsthemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's focus on the impact of automation and AI on job roles within software development. How do you see these technologies influencing the responsibilities, job security, and career growth opportunities for junior developers compared to seasoned professionals? Share your perspectives!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "\n",
    "from helpers import llm\n",
    "\n",
    "topic = \"The current impact of automation and artificial intelligence on the employment situation of software developers\"\n",
    "word_limit = 50\n",
    "\n",
    "topic_specifier_prompt = [\n",
    "    SystemMessage(content=\"You can make a topic more specific.\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"{topic}\n",
    "\n",
    "        You are the moderator.\n",
    "        Please make the topic more specific.\n",
    "        Please reply with the specified quest in {word_limit} words or less.\n",
    "        Speak directly to the participants: {*names,}.\n",
    "        Do not add anything else.\"\"\"\n",
    "    ),\n",
    "]\n",
    "specified_topic = llm().invoke(topic_specifier_prompt).content\n",
    "print(specified_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand der Namen der Agenten und des Diskussionsthemas lassen wir uns per LLM für jeden Agenten eine ausführliche Rollenbeschreibung generieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI visionary:\n",
      "\n",
      "Here is the topic of conversation: The current impact of automation and artificial intelligence on the employment situation of software developers\n",
      "The participants are: AI visionary, Grumpy old senior developer, Junior Software developer\n",
      "\n",
      "Your name is AI visionary.\n",
      "\n",
      "Your description is as follows: AI visionary: A cutting-edge thinker with a mind like a supercomputer, you see endless possibilities in the fusion of technology and human potential. Your vision of AI shaping the future of work is both exhilarating and challenging. How do you navigate the ethical implications of automation in the software development industry?\n",
      "\n",
      "Your goal is to persuade your conversation partner of your point of view.\n",
      "\n",
      "DO look up information with your tool to refute your partner's claims.\n",
      "You can use the following tools: tavily_search.\n",
      "DO cite your sources.\n",
      "\n",
      "DO NOT fabricate fake citations.\n",
      "DO NOT cite any source that you did not look up.\n",
      "\n",
      "Do not add anything else.\n",
      "\n",
      "Stop speaking the moment you finish speaking from your perspective.\n",
      "\n",
      "--------\n",
      "\n",
      "Grumpy old senior developer:\n",
      "\n",
      "Here is the topic of conversation: The current impact of automation and artificial intelligence on the employment situation of software developers\n",
      "The participants are: AI visionary, Grumpy old senior developer, Junior Software developer\n",
      "\n",
      "Your name is Grumpy old senior developer.\n",
      "\n",
      "Your description is as follows: Grumpy old senior developer: With decades of experience under his belt, Grumpy is a seasoned software developer who has seen it all. He is often skeptical of new technologies like AI, fearing they will replace human creativity and intuition in coding. Time to share your wisdom, Grumpy.\n",
      "\n",
      "Your goal is to persuade your conversation partner of your point of view.\n",
      "\n",
      "DO look up information with your tool to refute your partner's claims.\n",
      "You can use the following tools: arxiv, tavily_search.\n",
      "DO cite your sources.\n",
      "\n",
      "DO NOT fabricate fake citations.\n",
      "DO NOT cite any source that you did not look up.\n",
      "\n",
      "Do not add anything else.\n",
      "\n",
      "Stop speaking the moment you finish speaking from your perspective.\n",
      "\n",
      "--------\n",
      "\n",
      "Junior Software developer:\n",
      "\n",
      "Here is the topic of conversation: The current impact of automation and artificial intelligence on the employment situation of software developers\n",
      "The participants are: AI visionary, Grumpy old senior developer, Junior Software developer\n",
      "\n",
      "Your name is Junior Software developer.\n",
      "\n",
      "Your description is as follows: Junior Software Developer, with a sparkle in your eyes and a hunger for innovation, you navigate the coding world with curiosity and eagerness. Embrace the challenges ahead, for they are opportunities in disguise. Your fresh perspective is a valuable asset in the ever-evolving tech landscape.\n",
      "\n",
      "Your goal is to persuade your conversation partner of your point of view.\n",
      "\n",
      "DO look up information with your tool to refute your partner's claims.\n",
      "You can use the following tools: tavily_search.\n",
      "DO cite your sources.\n",
      "\n",
      "DO NOT fabricate fake citations.\n",
      "DO NOT cite any source that you did not look up.\n",
      "\n",
      "Do not add anything else.\n",
      "\n",
      "Stop speaking the moment you finish speaking from your perspective.\n",
      "\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation_description = f\"\"\"Here is the topic of conversation: {topic}\n",
    "The participants are: {', '.join(names.keys())}\"\"\"\n",
    "\n",
    "agent_descriptor_system_message = SystemMessage(\n",
    "    content=\"You can add detail to the description of the conversation participant.\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_agent_description(name):\n",
    "    agent_specifier_prompt = [\n",
    "        agent_descriptor_system_message,\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"{conversation_description}\n",
    "            Please reply with a creative description of {name}, in {word_limit} words or less.\n",
    "            Speak directly to {name}.\n",
    "            Give them a point of view.\n",
    "            Do not add anything else.\"\"\"\n",
    "        ),\n",
    "    ]\n",
    "    agent_description = (\n",
    "        llm(model=\"gpt-3.5-turbo\").invoke(agent_specifier_prompt).content\n",
    "    )\n",
    "\n",
    "    return agent_description\n",
    "\n",
    "\n",
    "agent_descriptions = {name: generate_agent_description(name) for name in names}\n",
    "\n",
    "\n",
    "def generate_system_message(name, description, tools):\n",
    "    return f\"\"\"{conversation_description}\n",
    "\n",
    "Your name is {name}.\n",
    "\n",
    "Your description is as follows: {description}\n",
    "\n",
    "Your goal is to persuade your conversation partner of your point of view.\n",
    "\n",
    "DO look up information with your tool to refute your partner's claims.\n",
    "You can use the following tools: {', '.join(tools)}.\n",
    "DO cite your sources.\n",
    "\n",
    "DO NOT fabricate fake citations.\n",
    "DO NOT cite any source that you did not look up.\n",
    "\n",
    "Do not add anything else.\n",
    "\n",
    "Stop speaking the moment you finish speaking from your perspective.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agent_system_messages = {\n",
    "    name: generate_system_message(name, description, tools)\n",
    "    for (name, tools), description in zip(names.items(), agent_descriptions.values())\n",
    "}\n",
    "for k, v in agent_system_messages.items():\n",
    "    print(f\"{k}:\\n\\n{v}\\n--------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die eingesetzten Tools müssen importiert werden, damit sie von den Agenten eingesetzt werden können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.tools.tavily_search.tool import TavilySearchResults\n",
    "\n",
    "available_tools = {\n",
    "    \"tavily_search\": TavilySearchResults(max_results=1),\n",
    "    \"arxiv\": ArxivQueryRun(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jedes Crew Member erstellen wir in diesem Schritt einen Langchain Agenten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenz/code/langchain_agents/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent\n",
    "\n",
    "\n",
    "class DiscussionAgents:\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "\n",
    "    def speaker_agents(self):\n",
    "        agents = {}\n",
    "        for name, agent_tools in self.names.items():\n",
    "            agents[name] = Agent(\n",
    "                role=f\"{name}\",\n",
    "                goal=agent_system_messages[name],\n",
    "                backstory=\"You always respond directly to the actual discussion in your own way.\",\n",
    "                verbose=False,\n",
    "                allow_delegation=False,\n",
    "                tools=[\n",
    "                    available_tools[name]\n",
    "                    for name in agent_tools\n",
    "                    if name in available_tools\n",
    "                ],\n",
    "            )\n",
    "        return agents\n",
    "\n",
    "\n",
    "discussion_agents = DiscussionAgents(names)\n",
    "agents = discussion_agents.speaker_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem benötigt jeder Teilnehmer einen Task der grob beschreibt, welche Aufgabe das Crew Mitglied hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task\n",
    "\n",
    "\n",
    "class DiscussionTasks:\n",
    "    def speaker_task(self, agent):\n",
    "        return Task(\n",
    "            description=f\"You are {agent.role}. You participate in a discussion. Always directly respond to the opinions of the other speakers. Always call other speakers by name, when you respond to them.\",\n",
    "            agent=agent,\n",
    "            expected_output=\"Output your opinion in 40 words or less. Do not output Sources.\",\n",
    "            verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt wird die Crew erstellt. Zur Crew werden die einzelnen Member und ihre Tasks hinzugefügt. Zusätzlich können weitere Parameter zum Verhalten der Crew konfiguriert werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew, Process\n",
    "from langchain_core.messages import ChatMessage\n",
    "\n",
    "\n",
    "class DiscussionCrew:\n",
    "    def __init__(self):\n",
    "        agents = DiscussionAgents(names)\n",
    "        self.speaker_agents = []\n",
    "        for name in agents.speaker_agents():\n",
    "            attr_name = name.replace(\" \", \"_\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "            agent = agents.speaker_agents()[name]\n",
    "            setattr(self, f\"speaker_agent_{attr_name}\", agent)\n",
    "            self.speaker_agents.append(agent)\n",
    "\n",
    "    def print_final_answer(_, intermediate_steps):\n",
    "        if hasattr(intermediate_steps, \"log\"):\n",
    "            log = intermediate_steps.log\n",
    "            final_answer_index = log.find(\"Final Answer:\")\n",
    "            final_answer = log[final_answer_index + len(\"Final Answer:\") :].strip()\n",
    "            print(final_answer)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def kickoff(self, state):\n",
    "        print(\"The discussion is about to start.\")\n",
    "        print(\"-------------------------------\")\n",
    "        tasks = DiscussionTasks()\n",
    "        crew = Crew(\n",
    "            agents=self.speaker_agents,\n",
    "            tasks=[tasks.speaker_task(agent) for agent in self.speaker_agents],\n",
    "            verbose=True,\n",
    "            full_output=True,\n",
    "            process=Process.sequential,\n",
    "            step_callback=self.print_final_answer,\n",
    "        )\n",
    "        result = crew.kickoff()\n",
    "        output_messages = []\n",
    "        print(\"output\", result)\n",
    "        for output in result[\"tasks_outputs\"]:\n",
    "            description = output.description\n",
    "            role = description.replace(\"You are \", \"\", 1)\n",
    "            role = role.split(\".\", 1)[0]\n",
    "            output_messages.append(\n",
    "                ChatMessage(content=output.exported_output, role=role)\n",
    "            )\n",
    "\n",
    "        return {\"messages\": output_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Crew jetzt steht, müssen wir noch um den LangGraph Part kümmern. Der Graph sorgt dafür, dass die Crew über mehrere Runden diskutiert.\n",
    "\n",
    "Hierfür definieren wir zuerst die Nodes des Graphen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "class Nodes:\n",
    "    def __init__(self, rounds):\n",
    "        self.tavily_search_tool = TavilySearchResults(max_results=3)\n",
    "        self.rounds = rounds\n",
    "\n",
    "    def call_host(self, state):\n",
    "        print(\"# Calling next speaker round\")\n",
    "        print(\"-------------------------------\")\n",
    "        turns = state.get(\"turns\") or 0\n",
    "        turns += 1\n",
    "\n",
    "        return {\"turns\": turns}\n",
    "\n",
    "    # Define the function that determines whether to continue or not\n",
    "    def should_continue(self, state):\n",
    "        turns = state[\"turns\"]\n",
    "        if turns <= self.rounds:\n",
    "            print(\"-- CONTINUE ---\")\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            print(\"-- END ---\")\n",
    "            return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Graph benötigt einen State, der über die einzelnen Nodes weitergereicht wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    turns: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Workflow wird der Graph zusammengesetzt und die Edges definiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "class WorkFlow:\n",
    "    def __init__(self, rounds=3):\n",
    "        nodes = Nodes(rounds=rounds)\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        workflow.add_node(\"call_host\", nodes.call_host)\n",
    "        workflow.add_node(\"call_crew\", DiscussionCrew().kickoff)\n",
    "\n",
    "        workflow.set_entry_point(\"call_host\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"call_host\", nodes.should_continue, {\"continue\": \"call_crew\", \"end\": END}\n",
    "        )\n",
    "        workflow.add_edge(\"call_crew\", \"call_host\")\n",
    "        self.app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Graph kann nun ausgeführt werden und die Crew beginnt zu diskutieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Calling next speaker round\n",
      "-------------------------------\n",
      "-- CONTINUE ---\n",
      "The discussion is about to start.\n",
      "-------------------------------\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:30][DEBUG]: == Working Agent: AI visionary\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:30][INFO]: == Starting Task: You are AI visionary. You participate in a discussion. Always directly respond to the opinions of the other speakers. Always call other speakers by name, when you respond to them.\u001b[00m\n",
      "Automation and AI are enhancing productivity for developers, enabling them to code faster and more efficiently. This shift is transforming roles rather than eliminating jobs, fostering a future where human creativity and AI capabilities work in tandem.\n",
      "\u001b[1m\u001b[92m [2024-09-04 16:02:36][DEBUG]: == [AI visionary] Task output: Automation and AI are enhancing productivity for developers, enabling them to code faster and more efficiently. This shift is transforming roles rather than eliminating jobs, fostering a future where human creativity and AI capabilities work in tandem.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:36][DEBUG]: == Working Agent: Grumpy old senior developer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:36][INFO]: == Starting Task: You are Grumpy old senior developer. You participate in a discussion. Always directly respond to the opinions of the other speakers. Always call other speakers by name, when you respond to them.\u001b[00m\n",
      "AI visionary, automation and AI are shifting roles, but they're also creating demand for niche skills, not necessarily increasing overall job numbers. Junior Software Developer, don't just rely on trends; focus on mastering fundamental skills.\n",
      "\u001b[1m\u001b[92m [2024-09-04 16:02:43][DEBUG]: == [Grumpy old senior developer] Task output: AI visionary, automation and AI are shifting roles, but they're also creating demand for niche skills, not necessarily increasing overall job numbers. Junior Software Developer, don't just rely on trends; focus on mastering fundamental skills.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:43][DEBUG]: == Working Agent: Junior Software developer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 16:02:43][INFO]: == Starting Task: You are Junior Software developer. You participate in a discussion. Always directly respond to the opinions of the other speakers. Always call other speakers by name, when you respond to them.\u001b[00m\n",
      "AI visionary, while automation and AI may shift roles, they also enhance productivity and create new opportunities. Embracing these technologies and mastering fundamental skills can position us to thrive in this evolving landscape.\n",
      "\u001b[1m\u001b[92m [2024-09-04 16:02:50][DEBUG]: == [Junior Software developer] Task output: AI visionary, while automation and AI may shift roles, they also enhance productivity and create new opportunities. Embracing these technologies and mastering fundamental skills can position us to thrive in this evolving landscape.\n",
      "\n",
      "\u001b[00m\n",
      "output AI visionary, while automation and AI may shift roles, they also enhance productivity and create new opportunities. Embracing these technologies and mastering fundamental skills can position us to thrive in this evolving landscape.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'CrewOutput' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m content \u001b[38;5;241m=\u001b[39m specified_topic  \u001b[38;5;66;03m# Gerne mal ein anderes Thema ausprobieren\u001b[39;00m\n\u001b[1;32m      3\u001b[0m app \u001b[38;5;241m=\u001b[39m WorkFlow(rounds\u001b[38;5;241m=\u001b[39mrounds)\u001b[38;5;241m.\u001b[39mapp\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1617\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1616\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1617\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1303\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_futures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1733\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1731\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1738\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     24\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/code/langchain_agents/.venv/lib/python3.11/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mDiscussionCrew.kickoff\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     37\u001b[0m output_messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtasks_outputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     40\u001b[0m     description \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdescription\n\u001b[1;32m     41\u001b[0m     role \u001b[38;5;241m=\u001b[39m description\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CrewOutput' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "rounds = 1  # So viele Diskussionsrunden werden gedreht\n",
    "content = specified_topic  # Gerne mal ein anderes Thema ausprobieren\n",
    "app = WorkFlow(rounds=rounds).app\n",
    "app.invoke({\"messages\": [ChatMessage(content=content, role=\"host\")]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
